{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f69531",
   "metadata": {},
   "source": [
    "<h1> Pre-Processing 2 </h1>\n",
    "\n",
    "\n",
    "Currently, there are 21,548 reports with a total of 20,361,959 words. \n",
    "\n",
    "Pre-processing 2 involves:\n",
    "- Tokenization \n",
    "- Lemmatization\n",
    "- Removing different types of words (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f0fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data after pre-processing 1\n",
    "df = pd.read_excel(\"complete_processed_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0acdf",
   "metadata": {},
   "source": [
    "<h3> Tokenization </h3>\n",
    "\n",
    "Tokenize words and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0945f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_words(string):\n",
    "    data = re.sub(r'[,!?;-]+', '.', string)\n",
    "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch == '.' \n",
    "           ]\n",
    "    return data\n",
    "\n",
    "df.text = df.text.loc[:].apply(tokenize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0acdf",
   "metadata": {},
   "source": [
    "<h3> Lemmatization </h3>\n",
    "\n",
    "For example: 'months' -> 'month' or 'running' -> 'run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "bbed89e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "for text in df.text:\n",
    "    corpus_list += text\n",
    "\n",
    "corpus_tagged = dict(nltk.pos_tag(set(corpus_list)))\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for j, text in enumerate(df.text):\n",
    "    for i, word in enumerate(text):\n",
    "        if corpus_tagged[word].startswith(('J', 'V', 'N', 'R')):\n",
    "            df.loc[j, \"text\"][i] = lemmatizer.lemmatize(word, pos = get_wordnet_pos(corpus_tagged[word]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0acdf",
   "metadata": {},
   "source": [
    "<h3> Remove specific 'time' phrases </h3>\n",
    "\n",
    "Phrases such as 'first time', 'each time' or 'point in time' describe a specific moment in time. They are unlikely to refer to the perception of a time duration, therefore are excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "399c68a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_word_expressions = [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eigth\", \"nineth\", \"tenth\", \"whole\", \"this\", \"every\", \"each\", \"that\", \"next\", \"one\", \"good\", \"same\", \"last\", \"hard\", \"great\", \"entire\"]\n",
    "\n",
    "three_word_expressions1 = [\"in\", \"the\", \"such\"]\n",
    "three_word_expressions2 = [\"point\", \"by\", \"around\", \"at\", \"for\", \"until\", \"at\"] \n",
    "\n",
    "for text in df.text:\n",
    "    for i, word in enumerate(text):\n",
    "        if word == \"time\" and text[i-1] in two_word_expressions or text[i-1] in three_word_expressions1 and text[i-2] in three_word_expressions2:\n",
    "            text.pop(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fff6dd0",
   "metadata": {},
   "source": [
    "<h3> Remove various types of words </h3>\n",
    "\n",
    "The following will remove:\n",
    "- NLTK stop words (see: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)\n",
    "- Numbers 1-100 as words\n",
    "- Names of substances\n",
    "- Words containing only 1 or 2 letters\n",
    "- Rare words (occuring less than 10 times in the corpus)\n",
    "- Unimportant common words in the Erowid identified by Ballentine (2022)\n",
    "- Unimportant common words identified by myself\n",
    "\n",
    "<h6> Ballentine, G., Friedman, S. F., & Bzdok, D. (2022). Trips and neurotransmitters: Discovering principled patterns across 6850 hallucinogenic experiences. In Sci Adv (Vol. 8, Issue 11, p. eabl6989). https://doi.org/10.1126/sciadv.abl6989 </h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "0aa0b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words with 1-2letters, nltk stopwords, words occuring less than 7 times in the entire corpus, words recommended by Ballentine (2022), and common words subjectively identified as not relevant \n",
    "\n",
    "\n",
    "#stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#rare words\n",
    "counter = Counter(corpus_list)\n",
    "rare_words = list(Counter({k: c for k, c in counter.items() if c < 10}))\n",
    "\n",
    "#numbers as words\n",
    "numbers_as_words = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"twenty-one\", \"twenty-two\", \"twenty-three\", \"twenty-four\", \"twenty-five\", \"twenty-six\", \"twenty-seven\", \"twenty-eight\", \"twenty-nine\", \"thirty\", \"thirty-one\", \"thirty-two\", \"thirty-three\", \"thirty-four\", \"thirty-five\", \"thirty-six\", \"thirty-seven\", \"thirty-eight\", \"thirty-nine\", \"forty\", \"forty-one\", \"forty-two\", \"forty-three\", \"forty-four\", \"forty-five\", \"forty-six\", \"forty-seven\", \"forty-eight\", \"forty-nine\", \"fourty\", \"fourty-one\", \"forty-two\", \"fourty-three\", \"fourty-four\", \"fourty-five\", \"fourty-six\", \"fourty-seven\", \"fourty-eight\", \"fourty-nine\", \"fifty\", \"fifty-one\", \"fifty-two\", \"fifty-three\", \"fifty-four\", \"fifty-five\", \"fifty-six\", \"fifty-seven\", \"fifty-eight\", \"fifty-nine\", \"sixty\", \"sixty-one\", \"sixty-two\", \"sixty-three\", \"sixty-four\", \"sixty-five\", \"sixty-six\", \"sixty-seven\", \"sixty-eight\", \"sixty-nine\", \"seventy\", \"seventy-one\", \"seventy-two\", \"seventy-three\", \"seventy-four\", \"seventy-five\", \"seventy-six\", \"seventy-seven\", \"seventy-eight\", \"seventy-nine\", \"eighty\", \"eighty-one\", \"eighty-two\", \"eighty-three\", \"eighty-four\", \"eighty-five\", \"eighty-six\", \"eighty-seven\", \"eighty-eight\", \"eighty-nine\", \"ninety\", \"ninety-one\", \"ninety-two\", \"ninety-three\", \"ninety-four\", \"ninety-five\", \"ninety-six\", \"ninety-seven\", \"ninety-eight\", \"ninety-nine\", \"one hundred\", \"hundred\"]\n",
    "\n",
    "#substances\n",
    "substances = [ \"lsd\",\"psilocybin mushrooms\",\"morning glory\",\"2c-i\",\"5-meo-dmt\",\"dmt\",\"25i-nbome\",\"argyreia nervosa\",\"2c-e\",\"5-meo-dipt\",\"amt\",\"2c-t-7\",\"2c-b\",\"echinopsis pachanoi\",\"dpt\",\"5-meo-dalt\",\"5-meo-mipt\",\"2c-c\",\"4-aco-dmt\",\"psychotria viridis\",\"dom\",\"tma-2\",\"doc\",\"2c-p\",\"2c-t-21\",\"4-ho-mipt\",\"2c-d\",\"4-ho-dipt\",\"1p-lsd\",\"dob-dragonfly\",\"4-acetoxy-dipt\",\"dob\",\"4-aco-det\",\"doi\",\"iboga\",\"ibogaine\",\"al-lad\",\"anadenanthera peregrina\",\"2c-t-2\",\"ayahuasca\",\"5-meo-amt\",\"mimosahuasca\",\"echinopsis peruviana\",\"anadenanthera colubrina\",\"lophophora williamsii\",\"4-acetoxy-mipt\",\"mescaline\",\"dipt\",\"2c-t-4\",\"dxm\",\"amanita muscaria\",\"methoxetamine\",\"3-meo-pcp\",\"ketamine\",\"nitrous oxide\",\"pcp\",\"mdma\",\"mda\",\"6-apb\",\"butylone\",\"ethylone\",\"mephedrone\",\"methylone\",\"mdai\",\"mbdb\",\"iap\",\"datura spp.\",\"brugmansia spp.\",\"scopolamine\",\"dimenhydrinate\",\"atropa belladona\",\"cannabis spp.\",\"diphenhydramine\",\"heroine\",\"zolpidem\",\"piper methysticum\",\"jwh-018\",\"hydrocodone\",\"fentanyl\",\"alprazolam\",\"melatonin\",\"alcohol (hard)\",\"sceletium tortuosum\",\"leonotis leonurus\",\"turnera diffusa\",\"morphine\",\"1,4-butaneidol\",\"cyclobenzaprine\",\"clonazepam\",\"opium\",\"piracetam\",\"lorazepam\",\"passiflora spp.\",\"alcohol (beer-wine)\",\"triazolam\",\"mitragyna speciosa\",\"tramadol\",\"synthetic cannabis\",\"oxycodone\",\"gabapentin\",\"papaver somniferum\",\"methadone\",\"alcohol\",\"codeine\",\"cannabinoid receptor agonists\",\"buprenorphine\",\"ether\",\"valeriana officinalis\",\"nymphaea caerulea\",\"gbl\",\"hash\",\"lactuca virosa\",\"hydromorphone\",\"carisoprodol\",\"zopiclone\",\"diazepam\",\"etizolam\",\"barbiturates\",\"meperidine\",\"cocaine\",\"myristica spp.\",\"caffeine\",\"mdpv\",\"methylphenidate\",\"ethylphenidate\",\"ilex paraguariensis\",\"ephedrine\",\"dmae\",\"coffea spp.\",\"betel nut\",\"2-aminoindan\",\"amphetamine\",\"metamphetamine\",\"modafinil\",\"nicotiana tabacum\",\"crack\",\"4-fluoroamphetamine\",\"adrafinil\",\"substituted piperazines\",\"tfmpp\",\"benzylpiperazine\",\"atomoxetine\",\"propylhexedrine\",\"calea zacatechichi\",\"silene undulata\",\"paroxetine\",\"bupropion\",\"trazadone\",\"sertraline\",\"olanzapine\",\"venlafaxine\",\"quetiapine\",\"mirtazapine\",\"amitriptyline\",\"hypericum perforatum\",\"salvia divinorum\",\"yohimbe\",\"acorus calamus\",\"nepeta cataria\",\"heimia salicifolia\",\"5-htp\",\n",
    "               'lsd', 'psilocybin', 'mushroom', 'mushrooms', 'mushroom - p. cubensis', 'magic mushrooms', 'magic mushrooms (sclerotia)', \"'mushrooms'\", 'mushrooms  - p. cubensis', 'mushrooms - c. cyanescens', 'mushrooms - p cubensis', 'mushrooms - p. arcana', 'mushrooms - p. atlantis (sclerotia)', 'mushrooms - p. azurescens', 'mushrooms - p. azurescens?', 'mushrooms - p. baeocystis', 'mushrooms - p. cubenesis', 'mushrooms - p. cubenesis (amazonian strain)', 'mushrooms - p. cubensis', 'mushrooms - p. cubensis (albino penis envy)', 'mushrooms - p. cubensis (amazon strain)', 'mushrooms - p. cubensis (cambodian)', \"mushrooms - p. cubensis ('cambodian')\", 'mushrooms - p. cubensis (dried)', 'mushrooms - p. cubensis (ecuadorean)', 'mushrooms - p. cubensis (extract)', 'mushrooms - p. cubensis (fresh)', 'mushrooms - p. cubensis (golden teacher)', 'mushrooms - p. cubensis (golden teachers)', 'mushrooms - p. cubensis (in chocolate)', 'mushrooms - p. cubensis (mexican)', 'mushrooms - p. cubensis (mycelium)', 'mushrooms - p. cubensis (penis envy)', 'mushrooms - p. cubensis (pes amazonia)', 'mushrooms - p. cubensis (sclerotia)', 'mushrooms - p. cubensis (smoked)', 'mushrooms - p. cubensis (tasmanian strain)', 'mushrooms - p. cyanescens', 'mushrooms - p. cyanescens (smoked)', 'mushrooms - p. galindoi', 'mushrooms - p. mexicana', 'mushrooms - p. mexicana (fresh)', 'mushrooms - p. mexicana (sclerotia)', 'mushrooms - p. mexicana (truffles)', 'mushrooms - p. ovoideocystidiata', 'mushrooms - p. semilanceata', 'mushrooms - p. subaeruginosa', 'mushrooms - p. subbalteatus or p. papilionaceus', 'mushrooms - p. tampanensis', 'mushrooms - p. tampanensis (sclerotia)', 'mushrooms - p. tampanensis (truffles)', 'mushrooms - p. weilii', 'mushrooms - p. zapotecorum', 'mushrooms - panaeolus cyanescens', 'mushrooms - panaeolus subbalteatus', 'mushrooms - psilocybe cyanescens', 'mushrooms (blue vein)', 'mushrooms (dried)', 'mushrooms (edible)', 'mushrooms (extract)', 'mushrooms (fresh)', \"mushrooms ('gold caps')\", 'mushrooms (golden caps)', 'mushrooms (hawaiian)', 'mushrooms (in chocolate)', 'mushrooms (magic mushrooms)', 'mushrooms (magic)', 'mushrooms (mexican)', \"mushrooms ('mexican')\", 'mushrooms (p. cubensis - mexican strain)', 'mushrooms (p. cubensis smoked)', 'mushrooms (p. cubensis)', 'mushrooms (p. cyanescens)', 'mushrooms (p. pelliculosa)', 'mushrooms (sclerotia)', 'mushrooms (smoked)', 'mushrooms (thai)', 'mushrooms- p. cubensis', 'mushroooms', 'cannabis', 'morning glorys', ' morning glory', 'mdma (ecstasy)', 'methamphetamine', 'ecstasy', 'meth', 'kratom', 'nutmeg', 'datura', 'amphetamines', 'cacti - t. pachanoi', 'kava', 'kava kava', 'absinthe', 'alcohol - hard', 'alcohol (rum)', 'alcohol (whiskey)', 'alcohol (mead)', 'alcohol (vodka)', 'alcohol - (liquor)', 'alcohol - (wine)', ' alcohol - hard', 'absinthe  (homemade)', 'absinthe (czech)', 'absinthe (homemade)', 'beer', 'wine', 'alcohol - beer', 'alcohol - wine', 'alcohol (mead)', 'alcohol - (wine)', 'alcohol - (wine)', 'alcohol - (wine)', '4-methylmethcathinone', '4-methylmethcathinone (mephedrone)', 'ambien', 'zolpidem (ambien)', ' zolpidem (ambien)', ' h.b. woodrose', 'h.b. woodrose seeds', 'woodrose', 'h.b. woodrose', 'h. b. woodrose', 'hawaiian baby woodrose seeds', 'hawaiian baby woodrose seeds', 'hawaiian baby woodrose seeds', 'h.b. woodrose (hbw)', 'catnip', 'cacti - t. peruvianus', '4-methylmethcathinone', 'brugmansia', 'brugmansia (tree datura)', 'brugmansia sanguinea', 'brugmansia suaveolens', 'damiana', 'tobacco', 'tobacco - cigarettes', 'tramadol (ultram)', 'blue lotus', 'dimenhydrinate (dramamine)', 'dramamine (dimenhydrinate)', 'dramamine (dimenhydrinate)', 'poppies - opium', 'poppies', 'poppies', 'oxycodone (oxycontin)', 'oxycontin', 'roxicodone', 'peyote', 'cannabis - hash', 'calamus', 'yerba mate', \"st. john's wort\", \"st. john's wort\", 'bromo-dragonfly', 'coffee', 'caffeine (coffee)', 'caffeine (coffee)', 'caffeine (coffee)', 'caffeine (coffee)', 'caffeine (coffee)', 'quetiapine (seroquel)', 'seroquel', 'gabapentin (neurontin)', 'neurontin', 'modafinil (provigil)', 'provigil', ' venlafaxine', 'venlafaxine (effexor)', 'effexor', 'paroxetine (paxil)', 'paxil', 'bupropion (wellbutrin)', 'wellbutrin', '4-Acetoxy-DET', 'diphenhydramine (benadryl)', 'benadryl', 'triazolam (halcion)', 'halcion', 'coca', 'valerian', 'piperazines - bzp', 'piperazines', 'piperazines - mcpp', 'piperazines - mcpp', 'piperazines - mcpp', 'silene capensis', 'passion flower', 'mimosa tenuiflora', 'dmt (extracted from m. tenuiflora)', 'dmt (extracted from m. tenuiflora)', 'belladonna', '1-4-butanediol', '1,4 butanediol', 'mipt', 'det'\n",
    "             ]\n",
    "\n",
    "#words from Ballentine (2022)\n",
    "ballentine_common_words = ['the', 'this', 'that', 'not', 'and', 'have', 'there', 'all', 'then', 'what', 'but', 'would', 'for', 'with', 'will', 'was', 'thing',\n",
    "                'get', 'could', 'from', 'more', 'etc', 'who', 'out', 'another', 'like', 'too', 'while', 'about', 'more', 'less', 'way', 'on',\n",
    "                'she', 'her', 'him', 'his', 'our', \"i'm\", 'i’m', 'are', 'can’t', \"i'd\", 'i’d',  'ich', 'der', 'das',\n",
    "                \"didn't\", \"don't\", \"dont\", \"i've\", \"it's\", \"wasn’t\", \"can't\", \"wouldn't\", \"couldn't\", \"couldn´t\", \"won't\", \"i'll\",\n",
    "                'them', 'were', 'they', 'through', 'back', 'being', 'only', 'also',\n",
    "                'went', 'some', 'again', 'into', 'after', 'around', 'down', 'just', 'very', 'things', 'when', 'over', 'other', 'before',\n",
    "                'because', 'which', 'took', 'than', 'before', 'still', 'didn’t',\n",
    "                'it’s', 'i’ve', 'didnt', 'didn´t', 'couldnt', 'couldn’t', 'their',\n",
    "                'don’t', \"that's\", 'won’t', 'und', 'che', 'que',\n",
    "                'μg/kg', 'mgs', \"mg's\", 'hcl', 'indole',\n",
    "                'pill', 'pills', 'pipe', 'smoke', 'smokes', 'smoked', 'smoking', 'blotter', 'tab', 'tabs', 'line', 'lines', 'dose', 'doses', 'dosage', 'hit', 'hits', 'bowl',\n",
    "                'trip', 'trips', 'tripping', 'tripped', 'trippy', 'k.hole', 'k-hole', 'khole',\n",
    "                'roll', 'rolls', 'rolling', 'rolled',\n",
    "                'das', '1999', '1/2', 'ten', 'substance', 'load', 'cherek', '5:00', '2001', '300', 'you', 'josh',\n",
    "            #    'you', 'seconds', 'months', 'days', 'weeks', 'years', 'second', 'month', 'day', 'week', 'year', 'hour', 'hours',\n",
    "\n",
    "                'powder', 'crystals', 'vaporized', 'vaporize',  'roll', 'rolling', 'rolled', 'nasal', 'bong', 'foil', 'root', 'bark', 'cannabis', 'toke', 'heroin'\n",
    "                'inject', 'injection', 'trip', 'pill', 'pills', 'injecting', 'insufflation', 'trips', 'tripping', 'tripped', 'trippy', 'pipe', 'rectal',\n",
    "                'snort', 'smoked', 'snorting', 'snorted', 'insufflated', 'injected', 'blotter', 'tab', 'oral', 'orally', 'weed', 'exstasy',\n",
    "                # 'body', 'experience', 'time', 'felt', 'feel', 'life', 'been', 'feeling', 'first', 'really', 'load', 'compound', 'effects',\n",
    "                'hole', 'bump', 'bumps', 'drunk', 'clubbing', 'boyfriend', 'husband', 'wife',\n",
    "                'syringe', 'needle', 'hospital',\n",
    "                'vial', 'bag',\n",
    "                'inject', 'drugs',\n",
    "                'vials', 'caps', 'bottle', 'robo', 'robitussin', 'syrup', 'vicks', 'coricidin', 'cough', 'freebase', 'compound', 'bottles', 'brand', 'tussin', 'cpm', 'maleate',\n",
    "                'chlorpheniramine', 'delsym', 'robotussin', 'joe', 'dex', 'dxm', 'prozac', '8oz', 'joint', 'pot',\n",
    "\n",
    "                'rave', 'raves', 'club', 'night', 'party', 'friend', 'car', '2000', 'boyfriends', 'girlfriend', 'girlfriends', 'rollin',\n",
    "\n",
    "                'die', 'nicht', 'mit', 'mir', 'darla', 'sich', 'mich', 'ist', 'ein', 'war', 'den',\n",
    "                'noch', 'een', 'auch', 'dass', 'hatte', 'auf', 'von', 'meine', 'als', 'eine',\n",
    "                'einen', 'alal', 'sie', 'het', 'dem', 'aus', 'mark', 'aber', 'nach', 'marijuana',\n",
    "                'des', 'approx', 'wavy', 'john', 'burnt', 'wie', 'chris'\n",
    "                \n",
    "                \n",
    "               ]\n",
    "#my common words\n",
    "my_common_words = [#adverbs\n",
    "                   'now', 'well', 'first', \"second\", \"third\", \"whole\", \"this\", \"every\", \"next\", \"one\", \"same\", \"however\", \"even\", \"something\", \"got\", \"probably\", \"without\", \"thereafter\", \"within\", \"aprox\", \"actually\", \"certain\", \"somehow\", \"rather\", \"least\", \"whatever\", \"whatsoever\", 'wenn', 'whoever', 'whose', 'whenever', 'whichever', 'really', 'never', 'bit', 'ever', 'told', 'later', 'almost', 'away', 'quite', 'pretty', 'completely', 'always', 'finally', 'extremely', 'yet', 'maybe', 'soon', 'far', 'side', 'else', 'definitely', 'close', 'slightly', 'eventually', 'usually', 'already', 'together', 'ago', 'along', 'alone', 'sometimes', 'immediately', 'simply', 'somewhat', 'totally', 'instead', 'perhaps', 'exactly', 'anyway', 'often', 'especially', 'normally', 'absolutely', 'mostly', 'truly', 'anymore', 'nearly', 'fully', 'fairly', 'incredibly', 'easily', 'barely', \n",
    "                   'recently', 'somewhere', 'basically', 'literally', 'constantly', 'certainly', 'forward', 'twice', 'store', 'clearly', \n",
    "                   'possibly', 'particularly', 'everywhere', 'generally', 'apparently', 'entirely', 'corner', 'instantly', 'approximately', 'alot', 'previously', 'perfectly', 'apart', 'obviously', 'relatively', 'occasionally', 'honestly', 'highly', 'roughly', 'directly', 'becomes', 'gradually', 'shortly', 'seriously', 'rapidly', 'indeed', 'hangover', 'ahead', 'otherwise', 'hardly', 'thus', 'aside', 'strongly', 'luckily', 'anywhere', 'nowhere', 'properly', 'smooth', 'intensely', 'greatly', 'chose', 'heavily', 'mainly', 'potentially', 'seemingly', 'loop', 'naturally', 'currently', 'whatsoever', 'therefore', 'merely', 'mildly', 'hopefully', 'initially', 'regularly', 'sometime', 'uncontrollably', 'frequently', 'surprisingly', 'carefully', 'silly', 'strangely', 'significantly', 'thoroughly', 'hallway', 'simultaneously', 'necessarily', 'increasingly', \n",
    "                   'rarely', 'definately', 'ignore', 'sore', 'lovely', 'lightly', 'violently', 'weather', 'thankfully', 'repeatedly', 'halfway', 'surely', 'specifically', 'differently', 'vaguely', 'amazingly', 'tingly', 'ether', 'desperately', 'settle', 'typically', 'fortunately', 'ultimately', 'essentially', 'practically', 'heavenly', 'distinctly', 'oddly', 'lately', 'suicide', 'quietly', 'terribly', 'strongest', 'nicely', 'partially', 'originally', 'infinitely', 'safely', 'nonetheless', 'overly', 'afterward', 'randomly', 'ugly', 'farther', 'driver', 'altogether', 'purely', 'equally', 'immensely', 'horribly', 'recreationally',  'subside', 'severely', 'supposedly', 'closely', 'noticeably', 'pleasantly', 'vividly', 'lonely', 'correctly', 'consciously', 'trippin', 'promptly', 'comfortably', 'virtually', 'considerably', 'loudly', 'partly', \n",
    "                   'unusually', 'largely', 'accurately', 'anytime', 'plate', 'profoundly', 'accidentally', 'dramatically', 'profusely', 'meanwhile', 'favourite', 'primarily', 'grind', 'importantly', 'frantically', 'successfully', 'wildly', 'freely', 'insanely', 'moderately', 'interestingly', 'commonly', 'readily', 'smoothly', 'downward', 'unbelievably', 'upside', 'whoever', 'tightly', 'effectively', 'softly', 'overwhelmingly', 'ridiculously', 'silently', 'november', 'drastically', 'reasonably', 'genuinely', 'peanut', 'consistently', 'elsewhere', 'brightly', 'precisely', 'firmly', 'tore', 'ally', 'subtly','remotely', 'sang', 'remarkably', 'similarly', 'upward', 'weekly', 'completly', 'finely', 'smack', 'intently', 'jelly', 'newly', 'belong', 'tremendously', 'responsibly', 'temple', 'exceptionally', 'automatically', 'solely', \n",
    "                   'uncertain', 'wonderfully', 'poorly', 'actively', 'aimlessly', 'presumably', 'avid', 'extensively', 'truely', 'furthermore', 'anything', 'nothing', 'half', 'part', 'sure', 'done', 'enough', 'sort', 'including', \"couple\", \"most\"\n",
    "                    #modal verbs\n",
    "                    'might', 'may', 'must', 'shall', 'ought',\n",
    "                    #cordinating conjunction\n",
    "                    'either', 'neither', 'plus', 'minus',\n",
    "                    #Preposition or subordinating conjunction\n",
    "                    'though', 'since', 'outside', 'love', 'inside', 'although', 'begin', 'behind', 'across', 'except', 'upon', 'throughout', 'onto', 'beyond', 'near', 'whether', 'despite', 'background', 'laugh', 'unlike', 'per', 'awhile', 'super', 'toward',\n",
    "                    #Adjective, comparative\n",
    "                    'higher', 'lower', 'smaller', 'larger', 'bigger', 'greater', 'lot'\n",
    "                    #Neutral adjectives\n",
    "                    'little', 'last', 'many', 'large', 'small', 'able', 'light', 'hard', 'right', 'left', 'several', 'great', 'lot', \"high\", \n",
    "                    #Neutral verbs\n",
    "                    \"let\", \"decide\", 'begin', 'seem', 'seems', 'come', 'want', 'know', 'become', 'sit', 'keep', 'happen', 'turn', 'close', 'give', 'call', 'end', 'go', 'get', 'take', 'try', 'come', 'talk', 'sit', 'walk', 'move', 'use', 'start', 'say', 'star', 'follow', 'following', 'stand', 'know', 'become', 'hold', 'leave', 'hear', 'bring', 'understand', 'giving', 'turn', 'know', 'make', 'find', 'put', 'tell', 'notice', 'do', 'show', \"let\", \"stay\"   \n",
    "                    #Neutral nouns\n",
    "                    \"amount\", \"point\" \n",
    "                    ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create hashmap\n",
    "remove_words_list = rare_words + stop_words + ballentine_common_words +  my_common_words + substances + numbers_as_words\n",
    "\n",
    "remove_words_dict = dict() \n",
    "for value, index in enumerate(remove_words_list):\n",
    "    remove_words_dict[index] = value\n",
    "\n",
    "def remove_words(text):\n",
    "    text = [w for w in text if w not in remove_words_dict] \n",
    "    #remove 1-2 letter words\n",
    "    text = [' '.join([w for w in i.split(' ') if len(w) >= 3]) for i in text]\n",
    "    while(\"\" in text):\n",
    "        text.remove(\"\")\n",
    "    return text\n",
    "    \n",
    "df.text = df.text.loc[:].apply(remove_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d91ea",
   "metadata": {},
   "source": [
    "Get length of corpus after removal of types of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "eeff471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5204968\n"
     ]
    }
   ],
   "source": [
    "y=0\n",
    "for i in range(len(df)):\n",
    "    x = len(df.text[i])\n",
    "    y += x\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0acdf",
   "metadata": {},
   "source": [
    "<h3> Save data </h3>\n",
    "\n",
    "The dataframe structure will look very similar to at the end of pre-processing 1. The only difference is the content of the 'text' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988076b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"clean_processed_data.pkl)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ac408516564915e59f6571e8840a617524b2c5af7c094526d6726d37b65d83a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
