{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get Time corpus 1 (Co-occurence networks only) </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter, OrderedDict\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Data after Pre-Processing 1 and 2 </h4>\n",
    "\n",
    "Variables: \n",
    "- df = Dataframe with information about title, substance, class, url, text of each report\n",
    "- corpus_list = List of all the report texts concatenated "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"processed_data_3.pkl\")\n",
    "\n",
    "\n",
    "#corpus list\n",
    "corpus_list = []\n",
    "for text in df.text:\n",
    "    corpus_list += text\n",
    "\n",
    "corpus_list = list(filter(lambda a: a not in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\", \"DATE\", \"PLACEHOLDER\"], corpus_list))\n",
    "\n",
    "\n",
    "counter_ = Counter(corpus_list)\n",
    "corpus_dict = dict(Counter({k: c for k, c in counter_.items()}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Seed time perception words </h4>\n",
    "\n",
    "- These words act as 'seeds' in the Erowid corpus. Words surrounding the seed might describe information about time perception. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntp_words = ['time', 'period', 'periods', 'duration', 'clock', 'temporal', 'spacetime', 'timespan', 'timespans', 'timeline', 'timelines', 'elapse', 'elapsed', 'length', 'timewise', 'velocity', 'pace', 'rate', 'tempo', 'pass', 'passing', 'passed']\n",
    "ftp_words = ['quick','quicker', 'quickly', 'quickest', 'fast', 'faster', 'fastest', 'fastened', 'rapid','rapidly', 'short', 'shorter', 'shortly', 'shortest','speedy', 'speedy','speeded', 'speedier', 'hurry', 'hurried', 'swift', 'swifter', 'swiftly', 'haste', 'hasty', 'brisk', 'turbo', 'accelerate', 'acceleration', 'accelerated', 'accelerating']\n",
    "stp_words = ['slow', 'slower', 'slowly', 'slows', 'slowed', 'slowest', 'slowing', 'slowdown', 'long', 'looong', 'longer', 'longer', 'longest', 'steady', 'deceleration', 'decelerate', 'decelerating', 'decelerated', 'dilatory', 'dilation', 'infinity', 'eternity', 'lengthy', 'prolonged', 'protracted', 'extended', 'unending', 'endless']\n",
    "time_words = sorted(ntp_words + ftp_words + stp_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Get context window function </h4>\n",
    "\n",
    "- Used to iterate through a list and yield C words before and after each seed word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context window function for time_words\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        if center_word not in time_words:\n",
    "            i += 1\n",
    "            pass\n",
    "        else:\n",
    "            context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "            yield context_words, center_word\n",
    "            i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Context window examples </h4>\n",
    "\n",
    "- Get context window and report url for specific time seed words. \n",
    "\n",
    "- Line 7 can be adjusted to get the context windows for specific seed or context words. Very useful for understanding nodes in the co-occurence network graph (later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#co-occurences in context window examples\n",
    "\n",
    "listx = []\n",
    "\n",
    "\n",
    "def context_words_edges_vertices(C, listx):\n",
    "    for i, text in enumerate(df.text):\n",
    "        url = df.loc[i, \"url\"]\n",
    "        substance = df.loc[i, \"substance\"]\n",
    "        for context_words, center_word in get_windows(text, C):\n",
    "            #can look for specific words\n",
    "            if \"symphony\" in context_words and \"silence\" in context_words: # line 7\n",
    "                print(substance)\n",
    "                print(context_words[0:C])\n",
    "                print(center_word)\n",
    "                print(context_words[C:])\n",
    "                print(url)\n",
    "                print(\"\\n\")\n",
    "                listx += context_words\n",
    "\n",
    "context_words_edges_vertices(C=5, listx=listx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Get all co-occurences in Time corpus  </h4>\n",
    "\n",
    "- Time corpus = All context windows seeded by all time words.\n",
    "\n",
    "- Get all pairs of word-occurence in each context window in the Time corpus. \n",
    "\n",
    "- Pairs stored in dataframe (df2) in the following format:\n",
    "\n",
    "    |substance| classes | seed   | source    | tagert   | weight | colourbias | \n",
    "    |----------------| ------------------------- | ------ | --------- | -------- | ------ | ---------- |\n",
    "    |LSD| Serotonergic psychedelics | slowly | periphery | abstract | 1      | -0.1       |\n",
    "\n",
    "<br>\n",
    "\n",
    "- The colourbias quantifies whether the seed word is a ntp, ftp, or stp. This score is used for visulization later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#co-occurence network - edges\n",
    "def context_words_edges_vertices(C):\n",
    "    #splitting the dataframe into parts and saving locally reduces memory load on juypter notebook and speeds up code significantly\n",
    "    for k in range(216): #adjust 216 - in my case reports = 21,548, so 100*216 = 21,600\n",
    "        print(k)\n",
    "        df2  = pd.DataFrame(columns = [\"substance\", \"classes\", \"seed\", \"source\", \"target\", \"colourbias\"])\n",
    "        for i, text in enumerate(df.text[k*100:(k+1)*100]): \n",
    "            \n",
    "            substance = df.loc[i+(k*100), \"substance\"]\n",
    "            class_ = df.loc[i+(k*100), \"classes\"]\n",
    "            \n",
    "\n",
    "            for context_words, center_word in get_windows(text, C):\n",
    "                if center_word in ntp_words:\n",
    "                    all_pairs = [(a, b) for idx, a in enumerate(context_words) for b in context_words[idx + 1:]]\n",
    "                    for a, b in all_pairs:\n",
    "                        df2.loc[len(df2.index)] = [substance, class_, center_word, a, b, 0]   \n",
    "                elif center_word in ftp_words:\n",
    "                    all_pairs = [(a, b) for idx, a in enumerate(context_words) for b in context_words[idx + 1:]]\n",
    "                    for a, b in all_pairs:\n",
    "                        df2.loc[len(df2.index)] = [substance, class_, center_word, a, b, 1]\n",
    "                elif center_word in stp_words:\n",
    "                    all_pairs = [(a, b) for idx, a in enumerate(context_words) for b in context_words[idx + 1:]]\n",
    "                    for a, b in all_pairs:\n",
    "                        df2.loc[len(df2.index)] = [substance, class_, center_word, a, b, -1]    \n",
    "        df2.to_pickle(\"timecorpus\" + str(k) + \".pkl\")\n",
    "\n",
    "    \n",
    "\n",
    "#adjust context window C\n",
    "context_words_edges_vertices(C=4)\n",
    "\n",
    "\n",
    "#concatenate dataframe parts\n",
    "df2  = pd.DataFrame(columns = [\"classes\", \"seed\", \"source\", \"target\", \"colourbias\"])\n",
    "for k in range(216):\n",
    "    temp = pd.read_pickle(\"timecorpus\" + str(k) + \".pkl\")\n",
    "    df2 = pd.concat([df2, temp], axis=0)\n",
    "\n",
    "\n",
    "#Format and save file\n",
    "#keep \"timecorpus.pkl\", delete timecorpus0-215.pkl\n",
    "df2.reset_index(inplace=True, drop=True)\n",
    "df2.to_pickle(\"timecorpus_C=4.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> EXTRA: Get all co-occurrences from random seed words (null model) </h4>\n",
    "\n",
    " This is seperate from generating the Time corpus pipeline. Compute the co-occurrences for a network from random seed words. \n",
    " \n",
    " Steps:\n",
    "\n",
    " - Calculate distribution characteristics of time perception seed words.\n",
    " - Create random list of unique words (randomly selected from Erowid corpus) that match the distribution characteristics of time perception seed words. \n",
    " - For reach random list, compute the co-occurrences for set context window and save as file.\n",
    " - For me, each random network ~ 1h compute time, therefore for high number of random networks, this cell may run a couple of days. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Beforehand: \n",
    "- Create local folders 'Temp Random corpus' and 'Random corpus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#frequency of timewords in Erowid corpus\n",
    "time_words_count_list = []\n",
    "for word in time_words:\n",
    "    time_words_count_list.append(corpus_list.count(word))\n",
    "\n",
    "\n",
    "# Calculate the mean\n",
    "time_mean = sum(time_words_count_list) / len(time_words_count_list)\n",
    "\n",
    "# Calculate the variance\n",
    "time_variance = sum((x - time_mean) ** 2 for x in time_words_count_list) / len(time_words_count_list)\n",
    "\n",
    "# Calculate the standard deviation\n",
    "time_std_deviation = math.sqrt(time_variance)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for j in range(30): #Adjust N, to determine how many random (null) networks you want. (Compuate time: 1h each)\n",
    "    print(j)\n",
    "    random_list = []\n",
    "    random_words_count_list = []\n",
    "    keys = list(set(corpus_list))\n",
    "    condition = False\n",
    "    num = 0\n",
    "    # Check that the number of words to select is less than or equal to the total number of keys\n",
    "    while not condition: \n",
    "        num += 1\n",
    "        random_list = random.sample(keys, k=81)\n",
    "        random_count_list = [corpus_dict[key] for key in random_list if key in corpus_dict]\n",
    "        random_mean = sum(random_count_list) / len(random_count_list)\n",
    "        random_variance = sum((x - random_mean) ** 2 for x in random_count_list) / len(random_count_list)\n",
    "        random_std_deviation = math.sqrt(random_variance)\n",
    "        #if a random word list shares the same number of words (81) and a +-5% mean, standard deviation and cummulative frequency with the time words, it's chosen\n",
    "        if (0.95 * sum(time_words_count_list)) < sum(random_count_list) < (1.05 * sum(time_words_count_list)) and (0.95*time_mean) < random_mean < (1.05*time_mean) and (0.95*time_std_deviation) < random_std_deviation < (1.05*time_std_deviation):\n",
    "            condition = True\n",
    "            print(random_list)\n",
    "\n",
    "\n",
    "            # Get context window function for random_words\n",
    "            def modified_get_windows(words, C):\n",
    "                i = C\n",
    "                while i < len(words) - C:\n",
    "                    center_word = words[i]\n",
    "                    if center_word not in random_list:\n",
    "                        i += 1\n",
    "                        pass\n",
    "                    else:\n",
    "                        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "                        yield context_words, center_word\n",
    "                        i += 1\n",
    "\n",
    "\n",
    "            #co-occurence network - edges\n",
    "            def context_words_edges_vertices(C):\n",
    "                #splitting the dataframe into parts and saving locally reduces memory load on juypter notebook and speeds up code significantly\n",
    "                for k in range(216): #adjust 216 - in my case reports = 21,548, so 100*216 = 21,600\n",
    "                    print(k)\n",
    "                    df2  = pd.DataFrame(columns = [\"substance\", \"classes\", \"seed\", \"source\", \"target\"])\n",
    "                    for i, text in enumerate(df.text[k*100:(k+1)*100]): \n",
    "                        \n",
    "                        substance = df.loc[i+(k*100), \"substance\"]\n",
    "                        class_ = df.loc[i+(k*100), \"classes\"]\n",
    "                        \n",
    "                        for context_words, center_word in modified_get_windows(text, C):\n",
    "                            all_pairs = [(a, b) for idx, a in enumerate(context_words) for b in context_words[idx + 1:]]\n",
    "                            for a, b in all_pairs:\n",
    "                                df2.loc[len(df2.index)] = [substance, class_, center_word, a, b]\n",
    "                    df2.to_pickle(\"Temp Random corpus/timecorpus\" + str(k) + \".pkl\")\n",
    "\n",
    "        \n",
    "\n",
    "            #adjust context window \n",
    "            context_words_edges_vertices(C=4)\n",
    "\n",
    "\n",
    "            #concatenate dataframe parts\n",
    "            df2  = pd.DataFrame(columns = [\"classes\", \"seed\", \"source\", \"target\"])\n",
    "            for k in range(216):    \n",
    "                temp = pd.read_pickle(\"Temp Random corpus/timecorpus\" + str(k) + \".pkl\")\n",
    "                df2 = pd.concat([df2, temp], axis=0)\n",
    "\n",
    "\n",
    "            #Format and save file\n",
    "            #keep \"timecorpus.pkl\", delete timecorpus0-215.pkl\n",
    "            df2.reset_index(inplace=True, drop=True)\n",
    "            df2.to_pickle(f\"Random corpus/_randomcorpus_C=4_j={j}.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
