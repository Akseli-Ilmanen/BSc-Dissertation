{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pre-processing 2 </h1>\n",
    "\n",
    "Involves:\n",
    "- Remove reports not written in English\n",
    "- Lowercasing and punctuation \n",
    "- Remove named entities (for anonymity) and dates\n",
    "- Remove certain 2-3word time phrases\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data after pre-processing 1\n",
    "df = pd.read_excel(\"processed_data_1.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Remove reports not written in English </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify index of non-english reports\n",
    "for i, text in enumerate(df.text):\n",
    "    if detect(text) != \"en\":\n",
    "        #remove those rows\n",
    "        df.drop(i, axis = 0, inplace = True)\n",
    "\n",
    "#reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lowercasing & Punctuation </h3>\n",
    "\n",
    "Lowercasing makes it less likely that word at the beginning of a sentence (e.g. \"Reconstituting and ...\") are identified as named entities. \n",
    "\n",
    "Remove punctuation, except [. , ']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing_punctuation(string):\n",
    "    data = nltk.word_tokenize(string)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch in [\".\", \",\"]\n",
    "           ]\n",
    "    data = \" \".join(data) # convert back to string\n",
    "    return data\n",
    "\n",
    "df.text = df.text.loc[:].apply(lowercasing_punctuation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Anonymize named entitiies </h3>\n",
    "\n",
    "For anonymity, remove the following and replace with their named entity type.\n",
    "\n",
    "- PERSON (Names of people)\n",
    "- ORG (Organisations) \n",
    "- GPE (Geopolitical entity)\n",
    "- LOC (Location)\n",
    "- DATE (Time of date)\n",
    "\n",
    "DATE is included above, as terms such as '5th of February', or 'last week' often occur in sentences with a time seed word. They tend not to focus on issues of time duration perception. To make it easier to identify these sentences and exclude them from BERTopic analyses, they are replaced with the placeholder 'DATE'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list=[]\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "for i, text in enumerate(df.text):\n",
    "    print(i)\n",
    "    doc = nlp(text)\n",
    "    # Replace named entities with their entity type\n",
    "    new_tokens = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\", \"DATE\"] and token.text != \"time\":\n",
    "            remove_list.append(token.text)\n",
    "            new_tokens.append(token.ent_type_)\n",
    "        else:\n",
    "            new_tokens.append(token.text)\n",
    "\n",
    "    # Join the new tokens into a single string\n",
    "    df.loc[i, \"text\"] = \" \".join(new_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Tokenization </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(string):\n",
    "    data = nltk.word_tokenize(string)  # tokenize string to words\n",
    "    return data\n",
    "\n",
    "df.text = df.text.loc[:].apply(tokenize_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Remove specific 2-3 word phrases containg time words </h3>\n",
    "\n",
    "Removes phrases such as 'first time', 'point in time', or 'long story', as these phrases are describing points in time or idioms unrelated to time perception. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: Whether to include sentinment-based idioms of time phrases could be debated ('best time', 'wonderful time')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is an overview of all the removed time words together (some don't make sense because fo the three word combination phrases)\n",
    "['first time', 'second time', 'third time', 'fourth time', 'fifth time', 'sixth time', 'seventh time', 'eigth time', 'nineth time', 'tenth time', 'whole time', 'this time', 'every time', 'each time', 'that time', 'next time', 'one time', 'good time', 'bad time', 'same time', 'last time', 'hard time', 'great time', 'entire time', 'some time', 'current time', 'single time', 'my time', 'winter time', 'summer time', 'spring time', 'dinner time', 'wonderful time', 'night time', 'right time', 'have time', 'had time', 'best time', 'awful time', 'worst time', 'free time', 'quality time', 'another time', 'popular time', 'in short', 'stopped short', 'running short', 'not long', 'to rate', 'I rate', 'would rate', 'any rate', 'week period', 'year period', 'in length', 'time first', 'time second', 'time third', 'time fourth', 'time fifth', 'time sixth', 'time seventh', 'time eigth', 'time nineth', 'time tenth', 'time whole', 'time this', 'time every', 'time each', 'time that', 'time next', 'time one', 'time good', 'time bad', 'time same', 'time last', 'time hard', 'time great', 'time entire', 'time some', 'time current', 'time single', 'time my', 'time winter', 'time summer', 'time spring', 'time dinner', 'time wonderful', 'time night', 'time right', 'time have', 'time had', 'time best', 'time awful', 'time worst', 'time free', 'time quality', 'time another', 'time popular', 'short in', 'short stopped', 'short running', 'long not', 'rate to', 'rate I', 'rate would', 'rate any', 'length in', 'period week', 'period year', 'second', 'seconds', 'minute', 'minutes', 'hour', 'hours', 'day', 'days', 'week', 'weeks', 'weekend', 'weekends', 'month', 'months', 'year', 'years', 'times', 'spent', 'point in time', 'point the time', 'point such time', 'point a time', 'point only time', 'point was time', 'point my time', 'point about time', 'point any time', 'by in time', 'by the time', 'by such time', 'by a time', 'by only time', 'by was time', 'by my time', 'by about time', 'by any time', 'around in time', 'around the time', 'around such time', 'around a time', 'around only time', 'around was time', 'around my time', 'around about time', 'around any time', 'at in time', 'at the time', 'at such time', 'at a time', 'at only time', 'at was time', 'at my time', 'at about time', 'at any time', 'for in time', 'for the time', 'for such time', 'for a time', 'for only time', 'for was time', 'for my time', 'for about time', 'for any time', 'until in time', 'until the time', 'until such time', 'until a time', 'until only time', 'until was time', 'until my time', 'until about time', 'until any time', 'the in time', 'the the time', 'the such time', 'the a time', 'the only time', 'the was time', 'the my time', 'the about time', 'the any time', 'it in time', 'it the time', 'it such time', 'it a time', 'it only time', 'it was time', 'it my time', 'it about time', 'it any time', 'when in time', 'when the time', 'when such time', 'when a time', 'when only time', 'when was time', 'when my time', 'when about time', 'when any time', 'was in time', 'was the time', 'was such time', 'was a time', 'was only time', 'was was time', 'was my time', 'was about time', 'was any time', 'pass in time', 'pass the time', 'pass such time', 'pass a time', 'pass only time', 'pass was time', 'pass my time', 'pass about time', 'pass any time', 'all in time', 'all the time', 'all such time', 'all a time', 'all only time', 'all was time', 'all my time', 'all about time', 'all any time', 'from time to time', 'in a long time', 'time to time', 'one more time', 'waste of time', 'as long as', 'as quickly as', 'fast and furious', 'period of years', 'my period of', 'at this rate', 'length of this', 'feet long', 'as fast as possible']\n",
    "\n",
    "\n",
    "#two word phrases\n",
    "preword_dict = {\"time\":  [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eigth\", \"nineth\", \"tenth\", \"whole\", \"this\", \"every\", \"each\", \"that\", \"next\", \"one\", \"good\", 'bad', \"same\", \"last\", \"hard\", \"great\", \"entire\", \"some\", \"current\", \"single\", \"my\", \"winter\", \"summer\", \"spring\", \"dinner\", \"wonderful\", \"night\", 'right', 'have', 'had', \"best\", 'awful', 'worst', \"free\", \"quality\", 'another', 'popular'],\n",
    "                \"short\": [\"in\", \"stopped\", 'running'],\n",
    "                'long':  [\"not\"],\n",
    "                'rate':  [\"to\", \"I\", \"would\", \"any\"],\n",
    "                'length':[\"in\"],\n",
    "                'period':  [\"week\", \"year\"] \n",
    "                }\n",
    "\n",
    "postword_dict = {\n",
    "                \"time\" : [],\n",
    "                \"short\": [\"story\", \"sentences\", 'phrase', 'version', 'versions'],\n",
    "                'long':  [\"story\", \"pants\", \"after\", \"gone\", \"before\", 'walk', 'walks', 'hair'],\n",
    "                'rate':  [],\n",
    "                'period':  [\"to\", \"I\", \"would\", \"any\"],\n",
    "                'length':  [\"in\"]\n",
    "                }\n",
    "\n",
    "#three_word_phrases combinations pre time\n",
    "one_word_pre_time = [\"in\", \"the\", \"such\", \"a\", \"only\", \"was\", 'my', 'about', 'any']\n",
    "two_words_pre_time = [\"point\", \"by\", \"around\", \"at\", \"for\", \"until\", \"the\", \"it\", \"when\", 'was', 'pass', 'all']\n",
    "\n",
    "\n",
    "#replace with \"PLACEHOLDER\"\n",
    "for key in preword_dict:\n",
    "    for text in df.text:\n",
    "        for i, word in enumerate(text[:-3]):\n",
    "            if key == word and text[i-1] in preword_dict[key]:\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif key == word and text[i+1] in postword_dict[key]:\n",
    "                text[i] = \"PLACEHOLDER\" #removed later - useful for indexing\n",
    "            elif word == \"time\" and text[i-1] in one_word_pre_time and text[i-2] in two_words_pre_time:\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"time\" and text[i-1] == \"from\" and text[i+1] == \"to\" and text[i-3] == \"time\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"time\" and text[i-3] == \"in\" and text[i-2] == \"a\" and text[i-3] == \"long\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"time\" and text[i+1] == \"to\" and text[i+2] == \"time\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"time\" and text[i-2] == \"time\" and text[i-1] == \"to\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"time\" and text[i-2] == \"one\" and text[i-1] == \"more\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"time\" and text[i-2] == \"waste\" and text[i-1] == \"of\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"long\" and text[i-1] == \"as\" and text[i+1] == \"as\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"quickly\" and text[i-1] == \"as\" and text[i+1] == \"as\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"fast\" and text[i+1] == \"and\" and text[i+2] == \"furious\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"period\" and text[i+1] == \"of\" and text[i+2] == \"years\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"period\" and text[i+1] == \"of\" and text[i+2] == \"my\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"rate\" and text[i-2] == \"at\" and text[i-1] == \"this\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"length\" and text[i+1] == \"of\" and text[i+2] == \"this\":\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "            elif word == \"fast\" and text[i+1] == \"as\" and text[i+2] == \"possible\":\n",
    "                text[i] = \"PLACEHOLDER\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "df.to_pickle(\"processed_data_2.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ac408516564915e59f6571e8840a617524b2c5af7c094526d6726d37b65d83a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
