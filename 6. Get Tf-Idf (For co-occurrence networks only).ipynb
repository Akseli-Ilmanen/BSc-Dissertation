{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Get Tf-Idf (For co-occurence networks only) </h1>\n",
    "\n",
    "This is a $tf\\text{-}idf$ implementation where we calculate the $tf$ (term frequency) for the time corpus of each substance class. $idf$ (inverse document frequency) is calculated by cocatenating all of the Erowid corpus subtracted by the time corpus for a particular substance, and splitting them into docs of 50 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from math import log\n",
    "import math\n",
    "\n",
    "#import data\n",
    "df = pd.read_pickle(\"processed_data_3.pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Seed words </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntp_words = ['time', 'period', 'periods', 'duration', 'clock', 'temporal', 'spacetime', 'timespan', 'timespans', 'timeline', 'timelines', 'elapse', 'elapsed', 'length', 'timewise', 'velocity', 'pace', 'rate', 'tempo', 'pass', 'passing', 'passed']\n",
    "ftp_words = ['quick','quicker', 'quickly', 'quickest', 'fast', 'faster', 'fastest', 'fastened', 'rapid','rapidly', 'short', 'shorter', 'shortly', 'shortest','speedy', 'speedy','speeded', 'speedier', 'hurry', 'hurried', 'swift', 'swifter', 'swiftly', 'haste', 'hasty', 'brisk', 'turbo', 'accelerate', 'acceleration', 'accelerated', 'accelerating']\n",
    "stp_words = ['slow', 'slower', 'slowly', 'slows', 'slowed', 'slowest', 'slowing', 'slowdown', 'long', 'looong', 'longer', 'longer', 'longest', 'steady', 'deceleration', 'decelerate', 'decelerating', 'decelerated', 'dilatory', 'dilation', 'infinity', 'eternity', 'lengthy', 'prolonged', 'protracted', 'extended', 'unending', 'endless']\n",
    "time_words = sorted(ntp_words + ftp_words + stp_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Functions </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context window function for time_words\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        if center_word not in time_words:\n",
    "            i += 1\n",
    "            pass\n",
    "        else:\n",
    "            context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "            yield context_words, center_word\n",
    "            i += 1\n",
    "\n",
    "# Get indices for each seed word in corpus list\n",
    "def get_time_indices(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        if words[i] not in time_words:\n",
    "            i += 1\n",
    "            pass\n",
    "        else:\n",
    "            yield i\n",
    "            i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Erowid corpus </h3>\n",
    "\n",
    "- class_sorted_df = df sorted by class alphabetically\n",
    "- corpus_list = Erowid corpus cocatenated sorted by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=4 #change C here\n",
    "\n",
    "#class_sorted_df\n",
    "class_sorted_df = df.copy()\n",
    "class_sorted_df.sort_values(by=[\"classes\"], ascending=True, inplace=True)\n",
    "class_sorted_df.reset_index(drop=True, inplace=True)  \n",
    "\n",
    "#corpus list\n",
    "corpus_list = []\n",
    "for text in class_sorted_df.text:\n",
    "    corpus_list += text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Tf-idf info dictionary for each class </h3> \n",
    "\n",
    "- classes_info_dict = stores tf-idf info for each class separately.\n",
    "\n",
    "Format:\n",
    "\n",
    "```ruby\n",
    "{Stimulants:[[XXX, XXX], #'Stimulants corpus' indexed in corpus_list\n",
    "              [\"word 1\", \"word 2\", \"word 1\"], #tf list\n",
    "              [\"try new stimulants was excited ...\", \"closed my eyes and ...\"] #idf doc list (each doc 50 words)  \n",
    "            ]}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class info dict: 1st list: start and end index, 2nd list:tf list, 3rd list: idf doc list\n",
    "\n",
    "classes_info_dict = {\"all\":[[0, 0], [], []]}\n",
    "for class_ in class_sorted_df.classes.unique():\n",
    "    classes_info_dict[class_] = [[0, 0], [], []]  \n",
    "\n",
    "#add values to #class info dict\n",
    "for class_ in classes_info_dict:\n",
    "    start_index = 0\n",
    "    temp_class_ = class_\n",
    "    for temp_class_ in classes_info_dict:\n",
    "        if temp_class_ == \"all\":\n",
    "            classes_info_dict[\"all\"][0][0] = 0\n",
    "            classes_info_dict[\"all\"][0][1] = len(corpus_list) - 1\n",
    "\n",
    "        else:   \n",
    "            classes_info_dict[temp_class_][0][0] = start_index\n",
    "            last_index = start_index - 1\n",
    "            for i, text in enumerate(class_sorted_df.text):\n",
    "                if temp_class_ == class_sorted_df.loc[i, \"classes\"]:\n",
    "                    last_index += len(text)\n",
    "                    classes_info_dict[temp_class_][0][1] = last_index\n",
    "            start_index = last_index + 1\n",
    "\n",
    "\n",
    "    class_index_start = classes_info_dict[class_][0][0]\n",
    "    class_index_stop = classes_info_dict[class_][0][1]\n",
    "\n",
    "\n",
    "    #tf words list per classs\n",
    "    tf_words_list = []\n",
    "    for context_words, center_word in get_windows(corpus_list[class_index_start:class_index_stop], C=C):\n",
    "        tf_words_list += context_words\n",
    "\n",
    "    #save tf words list in dict\n",
    "    classes_info_dict[class_][1] += tf_words_list\n",
    "\n",
    "\n",
    "    #indices of time words in Stimulants corpus\n",
    "    indices = []\n",
    "    for i in get_time_indices(corpus_list, C=C): \n",
    "        if i in range(class_index_start, class_index_stop):\n",
    "            indices.append(i)\n",
    "\n",
    "    # Sort indices in descending order so that we can delete items without affecting the indices of the remaining items\n",
    "    indices.sort(reverse=True)\n",
    "\n",
    "    # Get indices of entire context windows to delete in a separate list\n",
    "    to_delete = []\n",
    "    for i in indices:\n",
    "        # Calculate the indices to delete\n",
    "        context_window_indices = [index for index in range(i - C, i + C + 1)]\n",
    "        to_delete += context_window_indices\n",
    "\n",
    "\n",
    "    # Only keep text in corpus list that that does not match the indices of context windows\n",
    "    to_delete_set = set(to_delete)\n",
    "    idf_words_list = [word for i, word in enumerate(corpus_list) if i not in to_delete_set]\n",
    "\n",
    "    #Convert idf words into list of string docs with 50 words each\n",
    "    idf_doc_list = []\n",
    "\n",
    "    j=0\n",
    "    for i in range(0,len(idf_words_list), 50):\n",
    "        idf_doc_list.append(\" \".join(idf_words_list[j:i]))\n",
    "        j = i\n",
    "    idf_doc_list.pop(0)\n",
    "\n",
    "    #save to dict\n",
    "    classes_info_dict[class_][2] += idf_doc_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Calculate tf-idf values for word in each class and collect in dataframe </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"all\"] + list(class_sorted_df.classes.unique())\n",
    "\n",
    "tfidf_df = pd.DataFrame(columns=classes)\n",
    " \n",
    "#insert column with all unique words in corpus_list\n",
    "tfidf_df.insert(loc=0, column=\"word\", value=list(set(corpus_list)))\n",
    "\n",
    "#replace \"nan\" with 0\n",
    "tfidf_df.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "for class_ in classes_info_dict:\n",
    "    print(class_)\n",
    "    #get tf words list and idf doc list for every class\n",
    "    tf_words_list = classes_info_dict[class_][1]\n",
    "    idf_doc_list = classes_info_dict[class_][2]\n",
    "\n",
    "    # Count the occurrences of each word in the document\n",
    "    tf_words_counter = Counter(tf_words_list)\n",
    "\n",
    "    # Pre-calculate IDF values for each unique word in the document list\n",
    "    unique_words = set(tf_words_list)\n",
    "    idf_values = {}\n",
    "    for word in unique_words:\n",
    "        num_docs_containing_word = sum(1 for d in idf_doc_list if word in d)\n",
    "        idf_values[word] = log(len(idf_doc_list) / (1 + num_docs_containing_word*5))\n",
    "\n",
    "    tfidf_dict = {}\n",
    "\n",
    "    # Calculate TF-IDF values for each unique word in the document\n",
    "    for i, word in enumerate(unique_words):\n",
    "        tf = log(tf_words_counter[word], 1.01)\n",
    "        idf = idf_values[word]\n",
    "        tfidf_dict[word] = round(tf * idf, 2)\n",
    "\n",
    "\n",
    "    for i, word in enumerate(tfidf_df.word):\n",
    "        if word in tfidf_dict:\n",
    "            tfidf_df.loc[i, class_] = tfidf_dict[word]\n",
    "\n",
    "#save\n",
    "tfidf_df.to_pickle(f\"tfidf_df_C=\"{C}\".pkl\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Below is a similar implemntation but for substances with large number of reports </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C=4 #change C here\n",
    "substances = [\"lsd\", \"psilocybin mushrooms\", \"dmt\", \"mdma\", \"cannabis spp.\", \"salvia divinorum\"]\n",
    "\n",
    "\n",
    "#class_sorted_df - NOTE: I am calling subtances\n",
    "substance_sorted_df = df.copy()\n",
    "substance_sorted_df.sort_values(by=[\"substance\"], ascending=True, inplace=True)\n",
    "substance_sorted_df.reset_index(drop=True, inplace=True)  \n",
    "\n",
    "\n",
    "\n",
    "#corpus list\n",
    "substance_corpus_list = []\n",
    "for text in substance_sorted_df.text:\n",
    "    substance_corpus_list += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substance info dict: 1st list: start and end index, 2nd list:tf list, 3rd list: idf doc list\n",
    "\n",
    "\n",
    "\n",
    "substance_info_dict = {}\n",
    "for substance in substances:\n",
    "    substance_info_dict[substance] = [[0, 0], [], []]  \n",
    "    first_time = True\n",
    "    substance_corpus_list_index = -1\n",
    "    for i, text in enumerate(substance_sorted_df.text):\n",
    "        substance_corpus_list_index += len(text)\n",
    "        if substance == substance_sorted_df.loc[i, \"substance\"] and first_time:\n",
    "            substance_info_dict[substance][0][0] = substance_corpus_list_index - len(text) #start_index\n",
    "            first_time = False\n",
    "        \n",
    "        if substance != substance_sorted_df.loc[i, \"substance\"] and not first_time:\n",
    "            substance_info_dict[substance][0][1] = substance_corpus_list_index - len(text) #last_index  \n",
    "            first_time = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#add values to #substance info dict\n",
    "for substance in substance_info_dict:\n",
    "\n",
    "    substance_index_start = substance_info_dict[substance][0][0]\n",
    "    substance_index_stop = substance_info_dict[substance][0][1]\n",
    "\n",
    "\n",
    "    #tf words list per substance\n",
    "    tf_words_list = []\n",
    "    for context_words, center_word in get_windows(substance_corpus_list[substance_index_start:substance_index_stop], C=C):\n",
    "        tf_words_list += context_words\n",
    "\n",
    "    #save tf words list in dict\n",
    "    substance_info_dict[substance][1] += tf_words_list\n",
    "\n",
    "\n",
    "    #indices of time words in Stimulants corpus\n",
    "    indices = []\n",
    "    for i in get_time_indices(substance_corpus_list, C=C): \n",
    "        if i in range(substance_index_start,substance_index_stop):\n",
    "            indices.append(i)\n",
    "\n",
    "    # Sort indices in descending order so that we can delete items without affecting the indices of the remaining items\n",
    "    indices.sort(reverse=True)\n",
    "\n",
    "    # Get indices of entire context windows to delete in a separate list\n",
    "    to_delete = []\n",
    "    for i in indices:\n",
    "        # Calculate the indices to delete\n",
    "        context_window_indices = [index for index in range(i - C, i + C + 1)]\n",
    "        to_delete += context_window_indices\n",
    "\n",
    "\n",
    "    # Only keep text in corpus list that that does not match the indices of context windows\n",
    "    to_delete_set = set(to_delete)\n",
    "    idf_words_list = [word for i, word in enumerate(substance_corpus_list) if i not in to_delete_set]\n",
    "\n",
    "    #Convert idf words into list of string docs with 50 words each\n",
    "    idf_doc_list = []\n",
    "\n",
    "    j=0\n",
    "    for i in range(0,len(idf_words_list), 50):\n",
    "        idf_doc_list.append(\" \".join(idf_words_list[j:i]))\n",
    "        j = i\n",
    "    idf_doc_list.pop(0)\n",
    "\n",
    "    #save to dict\n",
    "    substance_info_dict[substance][2] += idf_doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tfidf_df = pd.DataFrame(columns=substances)\n",
    " \n",
    "#insert column with all unique words in corpus_list\n",
    "tfidf_df.insert(loc=0, column=\"word\", value=list(set(substance_corpus_list)))\n",
    "\n",
    "#replace \"nan\" with 0\n",
    "tfidf_df.fillna(0,inplace=True)\n",
    "\n",
    "\n",
    "for substance in substance_info_dict:\n",
    "    print(substance)\n",
    "    #get tf words list and idf doc list for every class\n",
    "    tf_words_list = substance_info_dict[substance][1]\n",
    "    idf_doc_list = substance_info_dict[substance][2]\n",
    "\n",
    "    # Count the occurrences of each word in the document\n",
    "    tf_words_counter = Counter(tf_words_list)\n",
    "\n",
    "    # Pre-calculate IDF values for each unique word in the document list\n",
    "    unique_words = set(tf_words_list)\n",
    "    idf_values = {}\n",
    "    for word in unique_words:\n",
    "        num_docs_containing_word = sum(1 for d in idf_doc_list if word in d)\n",
    "        idf_values[word] = log(len(idf_doc_list) / (1 + num_docs_containing_word*5))\n",
    "\n",
    "    tfidf_dict = {}\n",
    "\n",
    "    # Calculate TF-IDF values for each unique word in the document\n",
    "    for i, word in enumerate(unique_words):\n",
    "        tf = log(tf_words_counter[word], 1.01)\n",
    "        idf = idf_values[word]\n",
    "        tfidf_dict[word] = round(tf * idf, 2)\n",
    "\n",
    "\n",
    "    for i, word in enumerate(tfidf_df.word):\n",
    "        if word in tfidf_dict:\n",
    "            tfidf_df.loc[i, substance] = tfidf_dict[word]\n",
    "\n",
    "#save\n",
    "tfidf_df.to_pickle(f\"tfidf_df_C={C}_SUBSTANCES.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
