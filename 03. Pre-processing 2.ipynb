{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Pre-processing 2 </h1>\n",
    "\n",
    "Involves:\n",
    "- Remove reports not written in English\n",
    "- Lowercasing and punctuation \n",
    "- Remove named entities (for anonymity) and dates\n",
    "- Remove certain 2-3word time phrases\n",
    "- Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aksel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data after pre-processing 1\n",
    "df = pd.read_excel(\"processed_data_1.xlsx\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Remove reports not written in English </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify index of non-english reports\n",
    "for i, text in enumerate(df.text):\n",
    "    if detect(text) != \"en\":\n",
    "        #remove those rows\n",
    "        df.drop(i, axis = 0, inplace = True)\n",
    "\n",
    "#reset index\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Lowercasing & Punctuation </h3>\n",
    "\n",
    "Lowercasing makes it less likely that word at the beginning of a sentence (e.g. \"Reconstituting and ...\") are identified as named entities. \n",
    "\n",
    "Remove punctuation, except [. , ']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing_punctuation(string):\n",
    "    data = nltk.word_tokenize(string)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch in [\".\", \",\"]\n",
    "           ]\n",
    "    data = \" \".join(data) # convert back to string\n",
    "    return data\n",
    "\n",
    "df.text = df.text.loc[:].apply(lowercasing_punctuation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Anonymize named entitiies </h3>\n",
    "\n",
    "For anonymity, remove the following and replace with their named entity type.\n",
    "\n",
    "- PERSON (Names of people)\n",
    "- ORG (Organisations) \n",
    "- GPE (Geopolitical entity)\n",
    "- LOC (Location)\n",
    "- DATE (Time of date)\n",
    "\n",
    "DATE is included above, as terms such as '5th of February', or 'last week' often occur in sentences with a time seed word. They tend not to focus on issues of time duration perception. To make it easier to identify these sentences and exclude them from BERTopic analyses, they are replaced with the placeholder 'DATE'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_list=[]\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "for i, text in enumerate(df.text):\n",
    "    doc = nlp(text)\n",
    "    # Replace named entities with their entity type\n",
    "    new_tokens = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\", \"DATE\"] and token.text != \"time\":\n",
    "            remove_list.append(token.text)\n",
    "            new_tokens.append(token.ent_type_)\n",
    "        else:\n",
    "            new_tokens.append(token.text)\n",
    "\n",
    "    # Join the new tokens into a single string\n",
    "    df.loc[i, \"text\"] = \" \".join(new_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Remove specific 2-3 word phrases containg time words </h3>\n",
    "\n",
    "Replace phrases such as 'first time', 'point in time', or 'long story' with 'PLACEHOLDER', as these phrases are describing points in time or idioms unrelated to time perception. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Note**: Whether to include sentinment-based idioms of time phrases could be debated ('best time', 'wonderful time')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 3-word phraes (e.g. \"all in time\")\n",
    "two_word_prefix = ['all in', 'all the', 'around about', 'around such', 'around the', 'at a', 'at about', 'at any', 'at such', 'at the', 'by the', 'for a', 'for any', 'for such', 'for the', 'in a long', 'it was', 'one more', 'pass the', 'point in', 'the only', 'time to', 'until any', 'until the', 'was a', 'was about', 'was in', 'was the', 'waste of', 'when in']\n",
    "\n",
    "def three_word_phrases(string):\n",
    "    for expr in two_word_prefix: \n",
    "        regex = r\"\\b({})\\s+time\\b\".format(expr)\n",
    "        string = re.sub(regex, r\"PLACEHOLDER PLACEHOLDER PLACEHOLDER\", string)\n",
    "    return string\n",
    "    \n",
    "df.text = df.text.loc[:].apply(three_word_phrases)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#for 2-word phrases with various seed words, it's easier to tokenize the data first\n",
    "def tokenize_words(string):\n",
    "    data = nltk.word_tokenize(string)  # tokenize string to words\n",
    "    return data\n",
    "\n",
    "df.text = df.text.loc[:].apply(tokenize_words)\n",
    "\n",
    "\n",
    "#remove 2-word phrases \n",
    "\n",
    "#e.g. \"first time\"\n",
    "preword_dict = {\"time\":  [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eigth\", \"nineth\", \"tenth\", \"whole\", \"this\", \"every\", \"each\", \"that\", \"next\", \"one\", \"good\", 'bad', \"same\", \"last\", \"hard\", \"great\", \"entire\", \"some\", \"current\", \"single\", \"my\", \"winter\", \"summer\", \"spring\", \"dinner\", \"wonderful\", \"night\", 'right', 'have', 'had', \"best\", 'awful', 'worst', \"free\", \"quality\", 'another', 'popular'],\n",
    "                \"short\": [\"in\", \"stopped\", 'running'],\n",
    "                'long':  [\"not\"],\n",
    "                'rate':  [\"to\", \"I\", \"would\", \"any\"],\n",
    "                'length':[\"in\"],\n",
    "                }\n",
    "#e.g. \"short story\"\n",
    "postword_dict = {\n",
    "                \"time\" : [],\n",
    "                \"short\": [\"story\", \"sentences\", 'phrase', 'version', 'versions'],\n",
    "                'long':  [\"story\", \"pants\", \"after\", \"gone\", \"before\", 'walk', 'walks', 'hair'],\n",
    "                'rate':  [],\n",
    "                'length':  [\"in\"]\n",
    "                }\n",
    "\n",
    "#replace with \"PLACEHOLDER\"\n",
    "for key in preword_dict:\n",
    "    for text in df.text[:]:\n",
    "        for i, word in enumerate(text[:-3]):\n",
    "            if key == word and text[i-1] in preword_dict[key]:\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "                text[i-1] = \"PLACEHOLDER\"\n",
    "            elif key == word and text[i+1] in postword_dict[key]:\n",
    "                text[i] = \"PLACEHOLDER\"\n",
    "                text[i+1] = \"PLACEHOLDER\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save\n",
    "df.to_pickle(\"processed_data_2.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ac408516564915e59f6571e8840a617524b2c5af7c094526d6726d37b65d83a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
