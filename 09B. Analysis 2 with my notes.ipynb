{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Analysis 2 (BERTTopic) </h1>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis involved using the (detailed) [BERTopic Documentation](https://maartengr.github.io/BERTopic/api/bertopic.html). Since it involved many manual steps that were specific to our data (e.g. removing certain topics by index), only the key steps are shown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Import data </h4>\n",
    "\n",
    "Remember your data is in this format:\n",
    "\n",
    "|index| url | substance    | classes   | center_word | doc | \n",
    "|----------------| ------------------------- | ------ | --------- | -------- | ------ |\n",
    "|5| https://erowid.org/experiences/... | 1p-lsd | Serotonergic psychedelics | time | I was talking around the ... |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import pickle   \n",
    "from bertopic import BERTopic\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import plotly.offline as pyo\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bertopic versions\n",
    "all C=15\n",
    "\n",
    "NEW\n",
    "1) bertopic_model_1: topic_model = BERTopic(seed_topic_list=seed_topic_list) - docs, n-gram, stop_words\n",
    "2) bertopic_model_2: topic_model = BERTopic(seed_topic_list=seed_topic_list) - docs2, n-gram, stop_words, calculate_probabilities=True\n",
    "\n",
    "\n",
    "For Antidepressants / antipsychotics, Deliriants, Psilocybin mushrooms (surprising for this one that neceesary) min topic size reduced to 4 -> more topics appeared also above count 4, then excluded above 4. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each substance class, do removal of topics manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_df1 = pd.DataFrame()\n",
    "\n",
    "remove_df1[\"Serotonergic psychedelics\"] = [0, 3, 5,  6, 8, 10, 11, 13, 14, 15, 16, 17, 18, 25, 26, 30, 31, 34, 43, 44, 46, 50, 51, 58, 60, 61, 63, 65, 68, 70, 73, 81, 86, 90, 91, 93, 94, 97, 99, 105, 108, 111, 112, 116, 118, 120, 122, 126, 130, 142]\n",
    "remove_df1[\"Dissociative psychedelics\"] = [0, 3, 4, 6, 8, 12, 13, 19, 20, 21, 22, 26, 28, 29, 31, 36, 43, 44, 47, 1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Entactogens\"] = [0, 1, 2, 4, 5, 7, 8, 9, 11, 14, 18, 20, 22, 24, 25, 26, 27, 36, 40, 44, 47, 50, 52, 53, 54, 55, 57, 61, 63, 64, 65,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Depressant sedatives\"] = [0, 1, 3, 6, 9, 10, 15, 16, 17, 20, 21, 22, 23, 30, 31, 32, 33, 35, 38, 41, 45, 46, 47, 51, 52, 53, 56, 57, 61, 62, 65, 74, 76, 79, 81, 82, 89, 90, 96, 97,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Stimulants\"] = [0, 1, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 21, 22, 23, 26, 27, 28, 30, 33, 37, 39, 45, 47, 50, 52, 54, 55, 56, 59,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Deliriants\"] = [0, 2, 3, 4, 5, 7, 8, 10, 14, 15, 23, 31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Antidepressants antipsychotics\"] = [2, 3, 4, 5, 6, 8, 9, 10, 11, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000]\n",
    "remove_df1[\"LSD\"] = [1, 3, 4, 5, 7, 10, 11, 19, 20, 21, 22, 28, 38, 41, 42,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"MDMA\"] = [0, 1, 2, 5, 6, 7, 12, 15, 17, 19, 21, 22, 28, 30, 32, 34, 37, 38, 40, 41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 54, 59,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 0]\n",
    "remove_df1[\"Cannabis spp\"] = [0, 1, 2, 7, 14, 17, 19, 21, 29, 30, 31, 35, 38, 39,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Salvia divinorum\"] = [0, 1, 3, 4, 6, 7, 9, 11, 15, 16, 20, 28, 34, 35, 37, 38, 40, 45, 46, 56,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"DMT\"] = [1, 3, 4, 6, 9, 12, 14, 15, 19, 20, 25, 31, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
    "\n",
    "psilocybin_mushrooms_remove1 = [0, 1, 3, 8, 12, 13, 14, 21, 22, 23, 24, 26, 31, 38, 52, 55, 57, 58, 66, 70, 72, 82, 85, 90, 95, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]\n",
    "\n",
    "\n",
    "\n",
    "depressant_sedatives_remove2 = [2, 4, 5, 8, 10, 19, 29, 31, 41, 45, 52, 55, 61, 66, 69, 73, 79, 82, 83, 87, 89, 91, 93, 94, 96, 101, 102, 104, 108, 110, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]\n",
    "deliriants_remove2 = [0, 1, 2, 9, 10, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
    "antidepressants_antipsychotic_remove2 = [1, 7, 8, 9, 10, 11, 12, 52]\n",
    "lsd_remove2 = [1, 2, 5, 14, 30, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]\n",
    "dmnt_remove2 = [2, 20, 21, 27, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get docs and topic model for particular class or substance\n",
    "substance_or_class = \"Serotonergic psychedelics\"\n",
    "\n",
    "topic_model = BERTopic.load(f\"BERTopic files/BERTopic models/bertopic_model_1_{substance_or_class}\")\n",
    "\n",
    "\n",
    "docs = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_docs_{substance_or_class}.pkl\")\n",
    "\n",
    "temp_remove_list1 = list(set(remove_df1[substance_or_class].tolist()))\n",
    "\n",
    "document_info = topic_model.get_document_info(docs)\n",
    "\n",
    "\n",
    "#get df for class/substance\n",
    "class_df  = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df_{substance_or_class}.pkl\")\n",
    "\n",
    "#merge df and document info, so doc, ntp-ftp-stp class and topic in one df for color map\n",
    "temp_df = pd.merge(class_df, document_info, left_on='docs', right_on='Document')\n",
    "\n",
    "\n",
    "#update df based of keep list\n",
    "df2 = temp_df[~temp_df.filter(items=['Topic']).isin(temp_remove_list1).any(axis=1)]\n",
    "df2.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fractal', 0.15781796218083613), ('fractals', 0.09967148237011583), ('infinity', 0.03936794261871484), ('endless', 0.029808580435880267), ('arrangements', 0.029772939986120232), ('mathematical', 0.028383255619488658), ('universes', 0.025683584556699914), ('symbol', 0.0250403827875864), ('exploded', 0.023966655871418916), ('thousands', 0.02156385888031063)]\n",
      "\n",
      "\n",
      "fractal . things , people , objects , all in a colorful light grid of eternity and infinity . this is not what i was expecting . not like this .\n",
      "-----------\n",
      "Seed: eternity\n",
      "Substance: dmt\n",
      "Class: Serotonergic psychedelics\n",
      "Url: https://erowid.org/experiences/exp.php?ID=90880\n",
      "-----------\n",
      "\n",
      "i once again encountered the fractal mind but PLACEHOLDER PLACEHOLDER it would explode off into infinity but i was still able to comprehend all of it at once . i realized\n",
      "-----------\n",
      "Seed: infinity\n",
      "Substance: lsd\n",
      "Class: Serotonergic psychedelics\n",
      "Url: https://erowid.org/experiences/exp.php?ID=18714\n",
      "-----------\n",
      "\n",
      "down into the fractal the PLACEHOLDER PLACEHOLDER i surfaced , i realized that the still slowing fractals represented universes in the sense of alternate universes , all a part of the\n",
      "-----------\n",
      "Seed: slowing\n",
      "Substance: dpt\n",
      "Class: Serotonergic psychedelics\n",
      "Url: https://erowid.org/experiences/exp.php?ID=17250\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get 3 most representative docs2 per topic\n",
    "topic=87 #change topic here\n",
    "\n",
    "\n",
    "list_of_3 = topic_model.get_representative_docs(topic=topic)\n",
    "print(topic_model.get_topic(topic))\n",
    "print()\n",
    "print()\n",
    "for doc in list_of_3:\n",
    "    print(doc)\n",
    "    i = df[df.docs == doc].index[0] \n",
    "    print(\"-----------\")\n",
    "    print(\"Seed: \" + str(df.loc[i, \"seed\"]))\n",
    "    print(\"Substance: \" + str(df.loc[i, \"substance\"]))\n",
    "    print(\"Class: \" + str(df.loc[i, \"classes\"]))\n",
    "    print(\"Url: \" + str(df.loc[i, \"url\"]))\n",
    "    print(\"-----------\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 12, -1, 193, 111, 119, 190, 172, 22, 99, 222, 211, 2, 62, 175]\n"
     ]
    }
   ],
   "source": [
    "#find similar topics based of keyword(s)\n",
    "similar_topics, similarity = topic_model.find_topics(\"slow\", top_n=15)\n",
    "print(similar_topics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Class topic modelling </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = []\n",
    "\n",
    "for topic in df3['Topic'].unique():\n",
    "    subset = df3[df3['Topic'] == topic]\n",
    "    print(f\"Topic: {topic}\")\n",
    "    normal_count = int(subset[subset['type'] == 'Neutral time perception words']['weight'].values)\n",
    "    slow_count = int(subset[subset['type'] == 'Slow time perception words']['weight'].values)\n",
    "    fast_count = int(subset[subset['type'] == 'Fast time perception words']['weight'].values)\n",
    "    print(f\"Normal count: {normal_count}, Slow count: {slow_count}, Fast count: {fast_count}\")\n",
    "    print(f\"x {int(slow_count/fast_count)}\")\n",
    "    x.append(int(slow_count/fast_count))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "for topic in enumerate(df3.Topic):\n",
    "    ftp = 0\n",
    "    ntp = 0\n",
    "    stp = 0\n",
    "\n",
    "    if df3.loc[i, \"type\"] == \"Fast time perception words\":\n",
    "        ftp = df3.loc[i, \"weight\"]\n",
    "    if df3.loc[i, \"type\"] == \"Normal time perception words\":\n",
    "        ntp = df3.loc[i, \"weight\"]\n",
    "    if df3.loc[i, \"type\"] == \"Slow time perception words\":\n",
    "        stp = df3.loc[i, \"weight\"]\n",
    "    sum_ = ftp + ntp + stp\n",
    "    print(f\"FTP: {(ftp/sum_)*100}%,  NTP: {(ntp/sum_)*100}%,  STP: {(stp/sum_)*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get substance class or type labels for each doc\n",
    "#substance = df2.substance.to_list()\n",
    "types = df2.type.to_list()\n",
    "\n",
    "\n",
    "\n",
    "#topics per class or type - https://maartengr.github.io/BERTopic/getting_started/topicsperclass/topicsperclass.html\n",
    "#df3 = topic_model.topics_per_class(docs2, classes=substances)\n",
    "df3 = topic_model.topics_per_class(df2.Document.to_list(), classes=types)\n",
    "\n",
    "\n",
    "\n",
    "#get frequency of substance class/documents of that class\n",
    "#freq_dict = dict(df2.substance.value_counts())\n",
    "freq_dict = dict(df2.type.value_counts())\n",
    "\n",
    "\n",
    "#the series 'class' is not about substance classes but has substances in it. The term class comes from the BerTopic documentation.\n",
    "for i, item in enumerate(df3.Frequency):\n",
    "    df3.loc[i, \"Frequency\"] = df3.loc[i, \"Frequency\"]/freq_dict[df3.loc[i, \"Class\"]]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Other visualisations </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#hierarchy things\n",
    "\n",
    "#get hierarhical documents\n",
    "hierarchical_topics = topic_model.hierarchical_topics(docs2)\n",
    "\n",
    "\n",
    "# Visualize these representations\n",
    "topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "\n",
    "\n",
    "#topic tree\n",
    "print(topic_model.get_topic_tree(hierarchical_topics, max_distance=None, tight_layout=False))\n",
    "\n",
    "\n",
    "#itnertopic distance map\n",
    "topic_model.visualize_topics()\n",
    "\n",
    "#bar charts\n",
    "topic_model.visualize_barchart(topics=XXX)\n",
    "\n",
    "#heat map\n",
    "topic_model.visualize_heatmap(topics=[3, 28])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Ereate Erowid qutoes </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Erowid qutoes (ignore)\n",
    "dictx = {}\n",
    "for i, item in enumerate(df.classes):\n",
    "    substance = df.loc[i, \"substance\"]\n",
    "    if item not in dictx:\n",
    "        dictx[item] = []\n",
    "    if substance not in dictx[item]:\n",
    "        dictx[item].append(substance)\n",
    "\n",
    "\n",
    "listx = []\n",
    "for i, item in enumerate(topic_info.CustomName):\n",
    "    if int(topic_info.loc[i, \"Topic\"]) not in remove_list:\n",
    "        listx.append(item)\n",
    "\n",
    "\n",
    "#for substances\n",
    "for item in dictx.values():\n",
    "    for i in item:\n",
    "        print(f\"<option value='{i}'>{i}</option>\")\n",
    "\n",
    "#for substances\n",
    "for i in dictx.keys():\n",
    "    print(f\"<option value='{i}'>{i}</option>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Visualize documents by topic colour and ntp-ftp-stp class colour </h3>\n",
    "\n",
    "The graph generated with is a modified version of the <code>visualize_documents</code> and <code>visualize__hierarhical_documents</code>, so that one can see how the topics modelled reflect normal, fast or slow time perception (ntp-ftp, stp). In colour mode 1 documents are coloured according to their topic, and in colour mode 2 documents are coloured white-red-blue according to their ntp-ftp-stp class (e.g. \"slower\" seed word -> stp).\n",
    "\n",
    "To use this function, it has to be added to the BERTopic documentation. Also, you need to pass a dictionary <code>color_map2</code> with documents as keys and RGB values as values, and a list topics indexed by doc for all the topics you want to model. \n",
    "\n",
    "<br>\n",
    "\n",
    "All the modifications of the BERTopic documentation have the comment \"#CHANGED HERE\". The most important line that was changed is this one:\n",
    "\n",
    " ```marker=dict(size=5, opacity=0.5, color = [color_map[doc] if doc is not None else [255, 255, 255] for doc in selection.doc]) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_visualize_documents(self,\n",
    "                                    docs: List[str],\n",
    "                                    topics: List[int] = None,\n",
    "                                    embeddings: np.ndarray = None,\n",
    "                                    reduced_embeddings: np.ndarray = None,\n",
    "                                    sample: float = None,\n",
    "                                    hide_annotations: bool = False,\n",
    "                                    hide_document_hover: bool = False,\n",
    "                                    custom_labels: bool = False,\n",
    "                                    title: str = \"<b>Documents and Topics</b>\",\n",
    "                                    width: int = 1200,\n",
    "                                    height: int = 750,\n",
    "                                    color_map2 = None, #CHANGED HERE\n",
    "                                    keep_list3 = None): #CHANGED HERE\n",
    "        \"\"\" Visualize documents and their topics in 2D\n",
    "\n",
    "        Arguments:\n",
    "            topic_model: A fitted BERTopic instance.\n",
    "            docs: The documents you used when calling either `fit` or `fit_transform`\n",
    "            topics: A selection of topics to visualize.\n",
    "                    Not to be confused with the topics that you get from `.fit_transform`.\n",
    "                    For example, if you want to visualize only topics 1 through 5:\n",
    "                    `topics = [1, 2, 3, 4, 5]`.\n",
    "            embeddings: The embeddings of all documents in `docs`.\n",
    "            reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
    "            sample: The percentage of documents in each topic that you would like to keep.\n",
    "                    Value can be between 0 and 1. Setting this value to, for example,\n",
    "                    0.1 (10% of documents in each topic) makes it easier to visualize\n",
    "                    millions of documents as a subset is chosen.\n",
    "            hide_annotations: Hide the names of the traces on top of each cluster.\n",
    "            hide_document_hover: Hide the content of the documents when hovering over\n",
    "                                specific points. Helps to speed up generation of visualization.\n",
    "            custom_labels: Whether to use custom topic labels that were defined using \n",
    "                        `topic_model.set_topic_labels`.\n",
    "            title: Title of the plot.\n",
    "            width: The width of the figure.\n",
    "            height: The height of the figure.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        To visualize the topics simply run:\n",
    "\n",
    "        ```python\n",
    "        topic_model.visualize_documents(docs)\n",
    "        ```\n",
    "\n",
    "        Do note that this re-calculates the embeddings and reduces them to 2D.\n",
    "        The advised and prefered pipeline for using this function is as follows:\n",
    "\n",
    "        ```python\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        from bertopic import BERTopic\n",
    "        from umap import UMAP\n",
    "\n",
    "        # Prepare embeddings\n",
    "        docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "        sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "        # Train BERTopic\n",
    "        topic_model = BERTopic().fit(docs, embeddings)\n",
    "\n",
    "        # Reduce dimensionality of embeddings, this step is optional\n",
    "        # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "        # Run the visualization with the original embeddings\n",
    "        topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "\n",
    "        # Or, if you have reduced the original embeddings already:\n",
    "        topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "        ```\n",
    "\n",
    "        Or if you want to save the resulting figure:\n",
    "\n",
    "        ```python\n",
    "        fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "        fig.write_html(\"path/to/file.html\")\n",
    "        ```\n",
    "\n",
    "        <iframe src=\"../../getting_started/visualization/documents.html\"\n",
    "        style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe>\n",
    "        \"\"\"\n",
    "        topic_per_doc = keep_list3 #CHANGED HERE\n",
    "\n",
    "        # Sample the data to optimize for visualization and dimensionality reduction\n",
    "        if sample is None or sample > 1:\n",
    "            sample = 1\n",
    "\n",
    "        indices = []\n",
    "        for topic in set(topic_per_doc):\n",
    "            s = np.where(np.array(topic_per_doc) == topic)[0]\n",
    "            size = len(s) if len(s) < 100 else int(len(s) * sample)\n",
    "            indices.extend(np.random.choice(s, size=size, replace=False))\n",
    "        indices = np.array(indices)\n",
    "\n",
    "        df = pd.DataFrame({\"topic\": np.array(topic_per_doc)[indices]})\n",
    "        df[\"doc\"] = [docs[index] for index in indices]\n",
    "        df[\"topic\"] = [topic_per_doc[index] for index in indices]\n",
    "\n",
    "        # Extract embeddings if not already done\n",
    "        if sample is None:\n",
    "            if embeddings is None and reduced_embeddings is None:\n",
    "                embeddings_to_reduce = self._extract_embeddings(df.doc.to_list(), method=\"document\")\n",
    "            else:\n",
    "                embeddings_to_reduce = embeddings\n",
    "        else:\n",
    "            if embeddings is not None:\n",
    "                embeddings_to_reduce = embeddings[indices]\n",
    "            elif embeddings is None and reduced_embeddings is None:\n",
    "                embeddings_to_reduce = self._extract_embeddings(df.doc.to_list(), method=\"document\")\n",
    "\n",
    "        # Reduce input embeddings\n",
    "        if reduced_embeddings is None:\n",
    "            umap_model = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit(embeddings_to_reduce)\n",
    "            embeddings_2d = umap_model.embedding_\n",
    "        elif sample is not None and reduced_embeddings is not None:\n",
    "            embeddings_2d = reduced_embeddings[indices]\n",
    "        elif sample is None and reduced_embeddings is not None:\n",
    "            embeddings_2d = reduced_embeddings\n",
    "\n",
    "        unique_topics = set(topic_per_doc)\n",
    "        if topics is None:\n",
    "            topics = unique_topics\n",
    "\n",
    "        # Combine data\n",
    "        df[\"x\"] = embeddings_2d[:, 0]\n",
    "        df[\"y\"] = embeddings_2d[:, 1]\n",
    "\n",
    "        # Prepare text and names\n",
    "        if self.custom_labels_ is not None and custom_labels:\n",
    "            names = [self.custom_labels_[topic + self._outliers] for topic in unique_topics]\n",
    "        else:\n",
    "            names = [f\"{topic}_\" + \"_\".join([word for word, value in self.get_topic(topic)][:3]) for topic in unique_topics]\n",
    "\n",
    "\n",
    "\n",
    "        # Outliers and non-selected topics\n",
    "        non_selected_topics = set(unique_topics).difference(topics)\n",
    "        if len(non_selected_topics) == 0:\n",
    "            non_selected_topics = [-1]\n",
    "\n",
    "        selection = df.loc[df.topic.isin(non_selected_topics), :]\n",
    "        selection[\"text\"] = \"\"\n",
    "        selection.loc[len(selection), :] = [None, None, selection.x.mean(), selection.y.mean(), \"Other documents\"]\n",
    "\n",
    "\n",
    "\n",
    "        all_traces = []\n",
    "        for level in range(2): #CHANGED FROM HERE\n",
    "            traces = []\n",
    "\n",
    "\n",
    "            if level == 0:\n",
    "\n",
    "                # Selected topics\n",
    "                for name, topic in zip(names, unique_topics):\n",
    "                    if topic in topics and topic != -1:\n",
    "                        selection = df.loc[df.topic == topic, :]\n",
    "                        selection[\"text\"] = \"\"\n",
    "\n",
    "                        if not hide_annotations:\n",
    "                            selection.loc[len(selection), :] = [None, None, selection.x.mean(), selection.y.mean(), name]\n",
    "\n",
    "                        traces.append(\n",
    "                            go.Scattergl(\n",
    "                                x=selection.x,\n",
    "                                y=selection.y,\n",
    "                                hovertext=selection.doc if not hide_document_hover else None,\n",
    "                                hoverinfo=\"text\",\n",
    "                                text=selection.text,\n",
    "                                mode='markers+text',\n",
    "                                name=name,\n",
    "                                textfont=dict(\n",
    "                                    size=12,\n",
    "                                ),\n",
    "                                marker=dict(size=5, opacity=0.5)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            elif level == 1: \n",
    "                # Selected topics\n",
    "                for name, topic in zip(names, unique_topics):\n",
    "                    if topic in topics and topic != -1:\n",
    "                        selection = df.loc[df.topic == topic, :]\n",
    "                        selection[\"text\"] = \"\"\n",
    "\n",
    "                        if not hide_annotations:\n",
    "                            selection.loc[len(selection), :] = [None, None, selection.x.mean(), selection.y.mean(), name]\n",
    "\n",
    "\n",
    "                        traces.append(\n",
    "                            go.Scattergl(\n",
    "                                x=selection.x,\n",
    "                                y=selection.y,\n",
    "                                hovertext=selection.doc if not hide_document_hover else None,\n",
    "                                hoverinfo=\"text\",\n",
    "                                text=selection.text,\n",
    "                                mode='markers+text',\n",
    "                                name=name,\n",
    "                                textfont=dict(size=12),\n",
    "                                marker=dict(size=5, opacity=0.5, color = [color_map2[doc] if doc is not None else [255, 255, 255] for doc in selection.doc]) #TO HERE\n",
    "                            )\n",
    "         \n",
    "                        )\n",
    "            all_traces.append(traces)\n",
    "\n",
    "\n",
    "\n",
    "        # Track and count traces\n",
    "        nr_traces_per_set = [len(traces) for traces in all_traces]\n",
    "        trace_indices = [(0, nr_traces_per_set[0])]\n",
    "        for index, nr_traces in enumerate(nr_traces_per_set[1:]):\n",
    "            start = trace_indices[index][1]\n",
    "            end = nr_traces + start\n",
    "            trace_indices.append((start, end))\n",
    "\n",
    "        # Visualization\n",
    "        fig = go.Figure()\n",
    "        for traces in all_traces:\n",
    "            for trace in traces:\n",
    "                fig.add_trace(trace)\n",
    "\n",
    "        for index in range(len(fig.data)):\n",
    "            if index >= nr_traces_per_set[0]:\n",
    "                fig.data[index].visible = False\n",
    "\n",
    "        # Create and add slider\n",
    "        steps = []\n",
    "        for index, indices in enumerate(trace_indices):\n",
    "            step = dict(\n",
    "                method=\"update\",\n",
    "                label=str(index),\n",
    "                args=[{\"visible\": [False] * len(fig.data)}]\n",
    "            )\n",
    "            for index in range(indices[1]-indices[0]):\n",
    "                step[\"args\"][0][\"visible\"][index+indices[0]] = True\n",
    "            steps.append(step)\n",
    "\n",
    "        sliders = [dict(\n",
    "            currentvalue={\"prefix\": \"Colour mode: \"},\n",
    "            pad={\"t\": 20},\n",
    "            steps=steps\n",
    "        )]\n",
    "        \n",
    "\n",
    "        # Add grid in a 'plus' shape\n",
    "        x_range = (df.x.min() - abs((df.x.min()) * .15), df.x.max() + abs((df.x.max()) * .15))\n",
    "        y_range = (df.y.min() - abs((df.y.min()) * .15), df.y.max() + abs((df.y.max()) * .15))\n",
    "        fig.add_shape(type=\"line\",\n",
    "                    x0=sum(x_range) / 2, y0=y_range[0], x1=sum(x_range) / 2, y1=y_range[1],\n",
    "                    line=dict(color=\"#CFD8DC\", width=2))\n",
    "        fig.add_shape(type=\"line\",\n",
    "                    x0=x_range[0], y0=sum(y_range) / 2, x1=x_range[1], y1=sum(y_range) / 2,\n",
    "                    line=dict(color=\"#9E9E9E\", width=2))\n",
    "        fig.add_annotation(x=x_range[0], y=sum(y_range) / 2, text=\"D1\", showarrow=False, yshift=10)\n",
    "        fig.add_annotation(y=y_range[1], x=sum(x_range) / 2, text=\"D2\", showarrow=False, xshift=10)\n",
    "\n",
    "        # Stylize layout\n",
    "        fig.update_layout(\n",
    "            sliders=sliders,\n",
    "            template=\"simple_white\",\n",
    "            title={\n",
    "                'text': f\"{title}\",\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(\n",
    "                    size=22,\n",
    "                    color=\"Black\")\n",
    "            },\n",
    "            width=width,\n",
    "            height=height\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(visible=False)\n",
    "        fig.update_yaxes(visible=False)\n",
    "        return fig "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_df1 = pd.DataFrame()\n",
    "\n",
    "remove_df1[\"Serotonergic psychedelics\"] = [0, 3, 5,  6, 8, 10, 11, 13, 14, 15, 16, 17, 18, 25, 26, 30, 31, 34, 43, 44, 46, 50, 51, 58, 60, 61, 63, 65, 68, 70, 73, 81, 86, 90, 91, 93, 94, 97, 99, 105, 108, 111, 112, 116, 118, 120, 122, 126, 130, 142]\n",
    "remove_df1[\"Dissociative psychedelics\"] = [0, 3, 4, 6, 8, 12, 13, 19, 20, 21, 22, 26, 28, 29, 31, 36, 43, 44, 47, 1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Entactogens\"] = [0, 1, 2, 4, 5, 7, 8, 9, 11, 14, 18, 20, 22, 24, 25, 26, 27, 36, 40, 44, 47, 50, 52, 53, 54, 55, 57, 61, 63, 64, 65,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Depressant sedatives\"] = [0, 1, 3, 6, 9, 10, 15, 16, 17, 20, 21, 22, 23, 30, 31, 32, 33, 35, 38, 41, 45, 46, 47, 51, 52, 53, 56, 57, 61, 62, 65, 74, 76, 79, 81, 82, 89, 90, 96, 97,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Stimulants\"] = [0, 1, 3, 4, 5, 6, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 21, 22, 23, 26, 27, 28, 30, 33, 37, 39, 45, 47, 50, 52, 54, 55, 56, 59,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Deliriants\"] = [0, 2, 3, 4, 5, 7, 8, 10, 14, 15, 23, 31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Antidepressants antipsychotics\"] = [2, 3, 4, 5, 6, 8, 9, 10, 11, 15, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000]\n",
    "remove_df1[\"LSD\"] = [1, 3, 4, 5, 7, 10, 11, 19, 20, 21, 22, 28, 38, 41, 42,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"MDMA\"] = [0, 1, 2, 5, 6, 7, 12, 15, 17, 19, 21, 22, 28, 30, 32, 34, 37, 38, 40, 41, 42, 44, 45, 46, 47, 48, 50, 51, 52, 54, 59,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 0]\n",
    "remove_df1[\"Cannabis spp\"] = [0, 1, 2, 7, 14, 17, 19, 21, 29, 30, 31, 35, 38, 39,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"Salvia divinorum\"] = [0, 1, 3, 4, 6, 7, 9, 11, 15, 16, 20, 28, 34, 35, 37, 38, 40, 45, 46, 56,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000,  1000, 1000]\n",
    "remove_df1[\"DMT\"] = [1, 3, 4, 6, 9, 12, 14, 15, 19, 20, 25, 31, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n",
    "\n",
    "psilocybin_mushrooms_remove1 = [0, 1, 3, 8, 12, 13, 14, 21, 22, 23, 24, 26, 31, 38, 52, 55, 57, 58, 66, 70, 72, 82, 85, 90, 95, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244]\n",
    "\n",
    "\n",
    "\n",
    "depressant_sedatives_remove2 = [2, 4, 5, 8, 10, 19, 29, 31, 41, 45, 52, 55, 61, 66, 69, 73, 79, 82, 83, 87, 89, 91, 93, 94, 96, 101, 102, 104, 108, 110, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253]\n",
    "deliriants_remove2 = [0, 1, 2, 9, 10, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52]\n",
    "antidepressants_antipsychotic_remove2 = [1, 7, 8, 9, 10, 11, 12, 52]\n",
    "lsd_remove2 = [1, 2, 5, 14, 30, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150]\n",
    "dmnt_remove2 = [2, 20, 21, 27, 32, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "substances_classes_list2 = [\"Serotonergic psychedelics\", \"Dissociative psychedelics\", \"Entactogens\", \"Stimulants\", \"Psilocybin mushrooms\", \"MDMA\", \"Cannabis spp\", \"Salvia divinorum\"]\n",
    "\n",
    "\n",
    "for substance_or_class in substances_classes_list2:\n",
    "        print(substance_or_class)\n",
    "\n",
    "\n",
    "\n",
    "        #remove list\n",
    "        \n",
    "        if substance_or_class == \"Psilocybin mushrooms\":\n",
    "                temp_remove_list1 = psilocybin_mushrooms_remove1\n",
    "        else:\n",
    "                temp_remove_list1 = list(set(remove_df1[substance_or_class].tolist()))\n",
    "\n",
    "\n",
    "        #get topic model\n",
    "        temp_topic_model = BERTopic.load(f\"BERTopic files/BERTopic models/bertopic_model_1_{substance_or_class}\")\n",
    "\n",
    "        #labels\n",
    "        labels = temp_topic_model.generate_topic_labels(nr_words=3, topic_prefix=True, word_length=15, separator=\" - \")\n",
    "        temp_topic_model.set_topic_labels(labels)\n",
    "\n",
    "        #get docs for document info\n",
    "        temp_docs = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_docs_{substance_or_class}.pkl\")\n",
    "\n",
    "        #get df\n",
    "        temp_df  = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df_{substance_or_class}.pkl\")\n",
    "\n",
    "        #get document info\n",
    "        temp_document_info = temp_topic_model.get_document_info(temp_docs)\n",
    "\n",
    "        #merge df and document info, so doc, ntp-ftp-stp class and topic in one df for color map\n",
    "        temp_df = pd.merge(temp_df, temp_document_info, left_on='docs', right_on='Document')\n",
    "\n",
    "\n",
    "        #update df based of keep list\n",
    "        temp_df2 = temp_df[~temp_df.filter(items=['Topic']).isin(temp_remove_list1).any(axis=1)]\n",
    "        temp_df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        #save updated df2 for future reference\n",
    "        temp_df2.to_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df2_{substance_or_class}.pkl\")\n",
    "\n",
    "        #get docs2 from this updated df\n",
    "        temp_docs2 = temp_df2.Document.tolist()\n",
    "\n",
    "\n",
    "        #keep list1 - all topics once\n",
    "        temp_keep_list1 = [topic for topic in range(-1, len(temp_topic_model.get_topic_info())-1) if topic not in temp_remove_list1]\n",
    "        #get keep list2 - all topics of docs remaining indexed by docs\n",
    "        temp_keep_list2 = [topic for topic in temp_topic_model.topics_ if topic not in temp_remove_list1]\n",
    "\n",
    "\n",
    "        #color map (I would not normally spell it as 'colour' but wanted to keep consistent with BERTopic API :D )\n",
    "        color_map1 = {'Neutral time perception words':[255, 255, 255], 'Fast time perception words':[195, 27, 52], \"Slow time perception words\":[48, 69, 186]}\n",
    "\n",
    "        #create color map to assign documents colour of their seed word\n",
    "        color_map2 = {}\n",
    "        for i, type in enumerate(temp_df2.type):\n",
    "                topic = temp_df2.loc[i, \"Topic\"]\n",
    "                if topic in temp_keep_list1:\n",
    "                        doc = temp_df2.loc[i, \"Document\"]\n",
    "                        color_map2[doc] = color_map1[type]\n",
    "\n",
    "        #put slash back for title of graph\n",
    "        if substance_or_class == \"Depressant sedatives\":\n",
    "                title = \"Depressant / sedatives\"\n",
    "        elif substance_or_class == \"Antidepressants antipsychotics\":\n",
    "                title = \"Antidepressants / antipsychotics\"\n",
    "        else:\n",
    "                title = substance_or_class\n",
    "\n",
    "\n",
    "        ####\n",
    "        temp_topic_model2 = BERTopic.load(f\"BERTopic files/BERTopic models/bertopic_model_2_{substance_or_class}\") #CHANGED\n",
    "\n",
    "\n",
    "        #run modified viszualize documents function (see modification below)\n",
    "        temp_fig = temp_topic_model2.modified_visualize_documents(docs=temp_docs2, topics=temp_keep_list1, custom_labels=True, title=title, color_map2=color_map2, keep_list3=temp_keep_list2)\n",
    "\n",
    "\n",
    "        #save plotly file\n",
    "        pyo.plot(temp_fig, filename=f'BERTopic files/BERTopic plots/BERTopic_plot_{substance_or_class}.html')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#DONE WITH MIN TOPIC SIZE 4 (AS WEL AS PSILOCYBIN MUSHROOMS ABOVE)\n",
    "substances_classes_list = [\"Deliriants\", \"Depressant sedatives\", \"Antidepressants antipsychotics\", \"LSD\", \"DMT\"]\n",
    "\n",
    "\n",
    "for substance_or_class in substances_classes_list:\n",
    "        print(substance_or_class)\n",
    "\n",
    "        #remove list\n",
    "\n",
    "        temp_remove_list1 = list(set(remove_df1[substance_or_class].tolist()))\n",
    "\n",
    "        \n",
    "        if substance_or_class == \"Deliriants\":\n",
    "                temp_remove_list2 = deliriants_remove2\n",
    "        elif substance_or_class == \"Depressant sedatives\":\n",
    "                temp_remove_list2 = depressant_sedatives_remove2\n",
    "        elif substance_or_class == \"Antidepressants antipsychotics\":\n",
    "                temp_remove_list2 = antidepressants_antipsychotic_remove2\n",
    "        elif substance_or_class == \"LSD\":\n",
    "                temp_remove_list2 = lsd_remove2\n",
    "        elif substance_or_class == \"DMT\":\n",
    "                temp_remove_list2 = dmnt_remove2\n",
    "\n",
    "        #get docs for document info\n",
    "        temp_docs = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_docs_{substance_or_class}.pkl\")\n",
    "\n",
    "        #get topic model\n",
    "        temp_topic_model0 = BERTopic.load(f\"BERTopic files/BERTopic models/bertopic_model_1_{substance_or_class}\")\n",
    "\n",
    "        temp_document_info0 = temp_topic_model0.get_document_info(temp_docs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #update df based of keep list\n",
    "        temp_document_info0 = temp_document_info0[~temp_document_info0.filter(items=['Topic']).isin(temp_remove_list1).any(axis=1)]\n",
    "        temp_document_info0.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "        temp_docs2 = temp_document_info0.Document.tolist()\n",
    "\n",
    "\n",
    "        temp_topic_model = BERTopic.load(f\"BERTopic files/BERTopic models/bertopic_model_2_{substance_or_class}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #get df\n",
    "        temp_df2  = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df_{substance_or_class}.pkl\")\n",
    "        #update df based of keep list\n",
    "        temp_df2 = temp_df2[~temp_df2.filter(items=['Topic']).isin(temp_remove_list1).any(axis=1)]\n",
    "        temp_df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        #get document info\n",
    "        temp_document_info = temp_topic_model.get_document_info(temp_docs2)\n",
    "\n",
    "        #labels\n",
    "        labels = temp_topic_model.generate_topic_labels(nr_words=3, topic_prefix=True, word_length=15, separator=\" - \")\n",
    "        temp_topic_model.set_topic_labels(labels)\n",
    "\n",
    "\n",
    "        #merge df and document info, so doc, ntp-ftp-stp class and topic in one df for color map\n",
    "        temp_df2 = pd.merge(temp_df2, temp_document_info, left_on='docs', right_on='Document')\n",
    "\n",
    "\n",
    "        #update df based of keep list\n",
    "        temp_df2 = temp_df2[~temp_df2.filter(items=['Topic']).isin(temp_remove_list2).any(axis=1)]\n",
    "        temp_df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "        #save updated df2 for future reference\n",
    "        temp_df2.to_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df2_{substance_or_class}.pkl\")\n",
    "\n",
    "        #get docs2 from this updated df\n",
    "        temp_docs3 = temp_df2.Document.tolist()\n",
    "\n",
    "\n",
    "        #keep list1 - all topics once\n",
    "        temp_keep_list1 = [topic for topic in range(-1, len(temp_topic_model.get_topic_info())-1) if topic not in temp_remove_list2]\n",
    "        #get keep list2 - all topics of docs remaining indexed by docs\n",
    "        temp_keep_list2 = [topic for topic in temp_topic_model.topics_ if topic not in temp_remove_list2]\n",
    "\n",
    "\n",
    "        #color map (I would not normally spell it as 'colour' but wanted to keep consistent with BERTopic API :D )\n",
    "        color_map1 = {'Neutral time perception words':[255, 255, 255], 'Fast time perception words':[195, 27, 52], \"Slow time perception words\":[48, 69, 186]}\n",
    "\n",
    "        #create color map to assign documents colour of their seed word\n",
    "        color_map2 = {}\n",
    "        for i, type in enumerate(temp_df2.type):\n",
    "                topic = temp_df2.loc[i, \"Topic\"]\n",
    "                if topic in temp_keep_list1:\n",
    "                        doc = temp_df2.loc[i, \"Document\"]\n",
    "                        color_map2[doc] = color_map1[type]\n",
    "\n",
    "        #put slash back for title of graph\n",
    "        if substance_or_class == \"Depressant sedatives\":\n",
    "                title = \"Depressant / sedatives\"\n",
    "        elif substance_or_class == \"Antidepressants antipsychotics\":\n",
    "                title = \"Antidepressants / antipsychotics\"\n",
    "        else:\n",
    "                title = substance_or_class\n",
    "\n",
    "\n",
    "\n",
    "        #run modified viszualize documents function (see modification below)\n",
    "        #temp_fig = temp_topic_model.modified_visualize_documents(docs=temp_docs3, topics=temp_keep_list1, custom_labels=True, title=title, color_map2=color_map2, keep_list3=temp_keep_list2)\n",
    "\n",
    "\n",
    "        #save plotly file\n",
    "        #pyo.plot(temp_fig, filename=f'BERTopic files/BERTopic plots/BERTopic_plot_{substance_or_class}.html')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erowid Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV file for Erowid Quotes finder tool\n",
    "\n",
    "#for urls with no placeholders - find quote on the Erowid page\n",
    "def prepare_url(s, url):\n",
    "    s = s.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" i \", \" I \")\n",
    "    words = s.split()  # split the string into a list of words\n",
    "    first_three_words = ' '.join(words[:3])  # join the first three words with a space\n",
    "    last_three_words = ' '.join(words[-3:])  # join the last three words with a space\n",
    "    updated_url = url + \"#:~:text=\" + first_three_words.replace(\" \", \"%20\") + \",\" + last_three_words.replace(\" \", \"%20\")\n",
    "    return (updated_url)\n",
    "\n",
    "\n",
    "\n",
    "substances_classes_list_lowercase_subst = [\"Serotonergic psychedelics\", \"Dissociative psychedelics\", \"Entactogens\", \"Deliriants\", \"Depressant sedatives\", \"Stimulants\", \"Antidepressants antipsychotics\", \"lsd\", \"psilocybin mushrooms\", \"dmt\", \"mdma\", \"cannabis spp\", \"salvia divinorum\"]\n",
    "\n",
    "\n",
    "#create \n",
    "df4 = pd.DataFrame(columns=[\"Topic Nr\", \"Topic\", \"Substance\", \"Class\", \"Document\"])\n",
    "\n",
    "\n",
    "for substance_or_class in substances_classes_list_lowercase_subst:\n",
    "    print(substance_or_class)\n",
    "\n",
    "    #get temp df\n",
    "    temp_df2  = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df2_{substance_or_class}.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "    #for substances \"LSD\", \"Psilocybin mushrooms\", \"MDMA\", \"Cannabis spp\", \"Salvia divinorum\" use own topics (not class topics)\n",
    "    large_substances = [\"lsd\", \"dmt\", \"psilocybin mushrooms\", \"mdma\", \"cannabis spp\", \"salvia divinorum\"]\n",
    "    if substance_or_class.lower() not in large_substances:\n",
    "        temp_df2 = temp_df2[~temp_df2.filter(items=['substance']).isin(large_substances).any(axis=1)]\n",
    "\n",
    "    #remove outlier rows\n",
    "    #temp_df2 = temp_df2[~temp_df2.Topic == -1]\n",
    "\n",
    "    #sort df2 by highest probabilities\n",
    "    temp_df2 = temp_df2.sort_values(by=[\"Probability\"], ascending=False)\n",
    "    temp_df2.reset_index(drop=True, inplace=True) \n",
    "\n",
    "    #set custom labels for temp_df\n",
    "    labels = temp_topic_model.generate_topic_labels(nr_words=3, topic_prefix=True, word_length=15, separator=\" - \")\n",
    "    temp_topic_model.set_topic_labels(labels)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    substance_topics_combos = []\n",
    " \n",
    "    for i, topic_nr in enumerate(temp_df2.Topic):\n",
    "        if topic_nr != -1:\n",
    "            substance = temp_df2.loc[i, \"substance\"]\n",
    "            substance_topics_combo = str(topic_nr) + substance\n",
    "            substance_topics_combos.append(substance_topics_combo)\n",
    "            if substance_topics_combos.count(substance_topics_combo) <= 10: \n",
    "                class_ = temp_df2.loc[i, \"classes\"]\n",
    "                url = temp_df2.loc[i, \"url\"]\n",
    "                doc = temp_df2.loc[i, \"Document\"]\n",
    "                prob = round(temp_df2.loc[i, \"Probability\"], 3)\n",
    "                #get topic labels from topic_info df\n",
    "                if  len(str(topic_nr)) == 1: \n",
    "                    topic = \"00\" + temp_df2.loc[i, \"CustomName\"]\n",
    "                elif len(str(topic_nr)) == 2: \n",
    "                    topic = \"0\" + temp_df2.loc[i, \"CustomName\"]\n",
    "                else:\n",
    "                    topic = temp_df2.loc[i, \"CustomName\"]\n",
    "                #change url so it leads to the quote on the\n",
    "                if any(substring in doc for substring in [\"miranda\", \"megan\", \"matt\", \"alexa\"]): #exclude names not removed by Spacy in pre-processing\n",
    "                    pass \n",
    "                elif any(substring in doc for substring in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\"]):\n",
    "                    doc = \"...\" + doc + f\"...- TBS:{prob}  (NO URL)\" \n",
    "                else:\n",
    "                    url = prepare_url(doc, url)    \n",
    "                    doc = \"...\" + doc + f\"... - TBS:{prob}  (<a href={url}>URL</a>)\"       \n",
    "\n",
    "                df4.loc[len(df4.index)] = [topic_nr, topic, substance, class_, doc]\n",
    "\n",
    "#save\n",
    "df4.to_csv(\"Representative Quotes Per Topic-Substance.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare classes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_list = [\"Serotonergic psychedelics\", \"Dissociative psychedelics\", \"Entactogens\", \"Deliriants\", \"Depressant / sedatives\", \"Stimulants\", \"Oneirogens\", \"Antidepressants / antipsychotics\", \"Other\"]\n",
    "\n",
    "#save df and docs for each class separetely\n",
    "for class_ in classes_list:\n",
    "        temp_df = df[df.classes == class_]\n",
    "        if class_ == \"Depressant / sedatives\":\n",
    "                class_ = \"Depressant sedatives\"\n",
    "        elif class_ == \"Antidepressants / antipsychotics\":\n",
    "                class_ = \"Antidepressants antipsychotics\"\n",
    "        temp_df.to_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df_{class_}.pkl\")\n",
    "        temp_docs = [doc for doc in temp_df.docs]\n",
    "        with open(f\"BERTopic files/BERTopic docs/BERTopic_docs_{class_}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(temp_docs, f)\n",
    "\n",
    "\n",
    "#Capitalize for graph title asthetics \n",
    "substances_list = [\"LSD\", \"Psilocybin mushrooms\", \"MDMA\", \"Cannabis spp.\", \"Salvia divinorum\", \"DMT\"]\n",
    "\n",
    "#save df and docs for each large substance separetely\n",
    "for substance in substances_list:\n",
    "        temp_df = df[df.substance == substance.lower()]\n",
    "        temp_df.to_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df_{substance}.pkl\")\n",
    "        temp_docs = [doc for doc in temp_df.docs]\n",
    "        with open(f\"BERTopic files/BERTopic docs/BERTopic_docs_{substance}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(temp_docs, f)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#create topic model for class / substance\n",
    "vectorizer_model= CountVectorizer(stop_words=\"english\") \n",
    "\n",
    "#classes \n",
    "for class_ in classes_list:\n",
    "    if class_ == \"Depressant / sedatives\":\n",
    "            class_ = \"Depressant sedatives\"\n",
    "    elif class_ == \"Antidepressants / antipsychotics\":\n",
    "            class_ = \"Antidepressants antipsychotics\"\n",
    "\n",
    "    temp_topic_model = BERTopic(seed_topic_list=seed_topic_list, n_gram_range=(1,3), vectorizer_model=vectorizer_model, calculate_probabilities=False)\n",
    "    temp_docs = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_docs_{class_}.pkl\")\n",
    "    topics, probs = temp_topic_model.fit_transform(temp_docs)\n",
    "    temp_topic_model.save(f\"BERTopic files/BERTopic models/bertopic_model_1_{class_}\")\n",
    "\n",
    "\n",
    "#substances\n",
    "for substance in substances_list:\n",
    "    temp_topic_model = BERTopic(seed_topic_list=seed_topic_list, n_gram_range=(1,3), vectorizer_model=vectorizer_model, calculate_probabilities=False)\n",
    "    temp_docs = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_docs_{substance}.pkl\")\n",
    "    topics, probs = temp_topic_model.fit_transform(temp_docs)\n",
    "    temp_topic_model.save(f\"BERTopic files/BERTopic models/bertopic_model_1_{substance}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ac408516564915e59f6571e8840a617524b2c5af7c094526d6726d37b65d83a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
