{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Analysis 2 (BERTTopic) </h1>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis involved using BERTopic. See detailed documentation [here](https://maartengr.github.io/BERTopic/api/bertopic.html). Since it involved many manual steps that were specific to our data (e.g. removing certain topics by index), only the 2 key steps are shown"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Visualize documents by topic colour and ntp-ftp-stp class colour </h3>\n",
    "\n",
    "The graph generated by this cell is a modified version of the <code>visualize_documents</code> and <code>visualize__hierarhical_documents</code>, so that one can see how the topics modelled reflect normal, fast or slow time perception (ntp-ftp, stp). In colour mode 1 documents are coloured according to their topic, and in colour mode 2 documents are coloured white-red-blue according to their ntp-ftp-stp class (e.g. \"slower\" seed word -> stp).\n",
    "\n",
    "To use this function, it has to be added to the BERTopic documentation. Also, you need to pass a dictionary <code>color_map2</code> with documents as keys and RGB values as values, and a list topics indexed by doc for all the topics you want to model. \n",
    "\n",
    "<br>\n",
    "\n",
    "All the modifications of the BERTopic documentation have the comment \"#CHANGED HERE\". The most important line that was changed is this one:\n",
    "\n",
    " ```marker=dict(size=5, opacity=0.5, color = [color_map[doc] if doc is not None else [255, 255, 255] for doc in selection.doc]) ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_visualize_documents(self,\n",
    "                                    docs: List[str],\n",
    "                                    topics: List[int] = None,\n",
    "                                    embeddings: np.ndarray = None,\n",
    "                                    reduced_embeddings: np.ndarray = None,\n",
    "                                    sample: float = None,\n",
    "                                    hide_annotations: bool = False,\n",
    "                                    hide_document_hover: bool = False,\n",
    "                                    custom_labels: bool = False,\n",
    "                                    title: str = \"<b>Documents and Topics</b>\",\n",
    "                                    width: int = 1200,\n",
    "                                    height: int = 750,\n",
    "                                    color_map2 = None, #CHANGED HERE\n",
    "                                    keep_list3 = None): #CHANGED HERE\n",
    "        \"\"\" Visualize documents and their topics in 2D\n",
    "\n",
    "        Arguments:\n",
    "            topic_model: A fitted BERTopic instance.\n",
    "            docs: The documents you used when calling either `fit` or `fit_transform`\n",
    "            topics: A selection of topics to visualize.\n",
    "                    Not to be confused with the topics that you get from `.fit_transform`.\n",
    "                    For example, if you want to visualize only topics 1 through 5:\n",
    "                    `topics = [1, 2, 3, 4, 5]`.\n",
    "            embeddings: The embeddings of all documents in `docs`.\n",
    "            reduced_embeddings: The 2D reduced embeddings of all documents in `docs`.\n",
    "            sample: The percentage of documents in each topic that you would like to keep.\n",
    "                    Value can be between 0 and 1. Setting this value to, for example,\n",
    "                    0.1 (10% of documents in each topic) makes it easier to visualize\n",
    "                    millions of documents as a subset is chosen.\n",
    "            hide_annotations: Hide the names of the traces on top of each cluster.\n",
    "            hide_document_hover: Hide the content of the documents when hovering over\n",
    "                                specific points. Helps to speed up generation of visualization.\n",
    "            custom_labels: Whether to use custom topic labels that were defined using \n",
    "                        `topic_model.set_topic_labels`.\n",
    "            title: Title of the plot.\n",
    "            width: The width of the figure.\n",
    "            height: The height of the figure.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        To visualize the topics simply run:\n",
    "\n",
    "        ```python\n",
    "        topic_model.visualize_documents(docs)\n",
    "        ```\n",
    "\n",
    "        Do note that this re-calculates the embeddings and reduces them to 2D.\n",
    "        The advised and prefered pipeline for using this function is as follows:\n",
    "\n",
    "        ```python\n",
    "        from sklearn.datasets import fetch_20newsgroups\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        from bertopic import BERTopic\n",
    "        from umap import UMAP\n",
    "\n",
    "        # Prepare embeddings\n",
    "        docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "        sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "        embeddings = sentence_model.encode(docs, show_progress_bar=False)\n",
    "\n",
    "        # Train BERTopic\n",
    "        topic_model = BERTopic().fit(docs, embeddings)\n",
    "\n",
    "        # Reduce dimensionality of embeddings, this step is optional\n",
    "        # reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "\n",
    "        # Run the visualization with the original embeddings\n",
    "        topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "\n",
    "        # Or, if you have reduced the original embeddings already:\n",
    "        topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "        ```\n",
    "\n",
    "        Or if you want to save the resulting figure:\n",
    "\n",
    "        ```python\n",
    "        fig = topic_model.visualize_documents(docs, reduced_embeddings=reduced_embeddings)\n",
    "        fig.write_html(\"path/to/file.html\")\n",
    "        ```\n",
    "\n",
    "        <iframe src=\"../../getting_started/visualization/documents.html\"\n",
    "        style=\"width:1000px; height: 800px; border: 0px;\"\"></iframe>\n",
    "        \"\"\"\n",
    "        topic_per_doc = keep_list3 #CHANGED HERE\n",
    "\n",
    "        # Sample the data to optimize for visualization and dimensionality reduction\n",
    "        if sample is None or sample > 1:\n",
    "            sample = 1\n",
    "\n",
    "        indices = []\n",
    "        for topic in set(topic_per_doc):\n",
    "            s = np.where(np.array(topic_per_doc) == topic)[0]\n",
    "            size = len(s) if len(s) < 100 else int(len(s) * sample)\n",
    "            indices.extend(np.random.choice(s, size=size, replace=False))\n",
    "        indices = np.array(indices)\n",
    "\n",
    "        df = pd.DataFrame({\"topic\": np.array(topic_per_doc)[indices]})\n",
    "        df[\"doc\"] = [docs[index] for index in indices]\n",
    "        df[\"topic\"] = [topic_per_doc[index] for index in indices]\n",
    "\n",
    "        # Extract embeddings if not already done\n",
    "        if sample is None:\n",
    "            if embeddings is None and reduced_embeddings is None:\n",
    "                embeddings_to_reduce = self._extract_embeddings(df.doc.to_list(), method=\"document\")\n",
    "            else:\n",
    "                embeddings_to_reduce = embeddings\n",
    "        else:\n",
    "            if embeddings is not None:\n",
    "                embeddings_to_reduce = embeddings[indices]\n",
    "            elif embeddings is None and reduced_embeddings is None:\n",
    "                embeddings_to_reduce = self._extract_embeddings(df.doc.to_list(), method=\"document\")\n",
    "\n",
    "        # Reduce input embeddings\n",
    "        if reduced_embeddings is None:\n",
    "            umap_model = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit(embeddings_to_reduce)\n",
    "            embeddings_2d = umap_model.embedding_\n",
    "        elif sample is not None and reduced_embeddings is not None:\n",
    "            embeddings_2d = reduced_embeddings[indices]\n",
    "        elif sample is None and reduced_embeddings is not None:\n",
    "            embeddings_2d = reduced_embeddings\n",
    "\n",
    "        unique_topics = set(topic_per_doc)\n",
    "        if topics is None:\n",
    "            topics = unique_topics\n",
    "\n",
    "        # Combine data\n",
    "        df[\"x\"] = embeddings_2d[:, 0]\n",
    "        df[\"y\"] = embeddings_2d[:, 1]\n",
    "\n",
    "        # Prepare text and names\n",
    "        if self.custom_labels_ is not None and custom_labels:\n",
    "            names = [self.custom_labels_[topic + self._outliers] for topic in unique_topics]\n",
    "        else:\n",
    "            names = [f\"{topic}_\" + \"_\".join([word for word, value in self.get_topic(topic)][:3]) for topic in unique_topics]\n",
    "\n",
    "\n",
    "\n",
    "        # Outliers and non-selected topics\n",
    "        non_selected_topics = set(unique_topics).difference(topics)\n",
    "        if len(non_selected_topics) == 0:\n",
    "            non_selected_topics = [-1]\n",
    "\n",
    "        selection = df.loc[df.topic.isin(non_selected_topics), :]\n",
    "        selection[\"text\"] = \"\"\n",
    "        selection.loc[len(selection), :] = [None, None, selection.x.mean(), selection.y.mean(), \"Other documents\"]\n",
    "\n",
    "\n",
    "\n",
    "        all_traces = []\n",
    "        for level in range(2): #CHANGED FROM HERE\n",
    "            traces = []\n",
    "\n",
    "\n",
    "            if level == 0:\n",
    "\n",
    "                # Selected topics\n",
    "                for name, topic in zip(names, unique_topics):\n",
    "                    if topic in topics and topic != -1:\n",
    "                        selection = df.loc[df.topic == topic, :]\n",
    "                        selection[\"text\"] = \"\"\n",
    "\n",
    "                        if not hide_annotations:\n",
    "                            selection.loc[len(selection), :] = [None, None, selection.x.mean(), selection.y.mean(), name]\n",
    "\n",
    "                        traces.append(\n",
    "                            go.Scattergl(\n",
    "                                x=selection.x,\n",
    "                                y=selection.y,\n",
    "                                hovertext=selection.doc if not hide_document_hover else None,\n",
    "                                hoverinfo=\"text\",\n",
    "                                text=selection.text,\n",
    "                                mode='markers+text',\n",
    "                                name=name,\n",
    "                                textfont=dict(\n",
    "                                    size=12,\n",
    "                                ),\n",
    "                                marker=dict(size=5, opacity=0.5)\n",
    "                            )\n",
    "                        )\n",
    "\n",
    "            elif level == 1: \n",
    "                # Selected topics\n",
    "                for name, topic in zip(names, unique_topics):\n",
    "                    if topic in topics and topic != -1:\n",
    "                        selection = df.loc[df.topic == topic, :]\n",
    "                        selection[\"text\"] = \"\"\n",
    "\n",
    "                        if not hide_annotations:\n",
    "                            selection.loc[len(selection), :] = [None, None, selection.x.mean(), selection.y.mean(), name]\n",
    "\n",
    "\n",
    "                        traces.append(\n",
    "                            go.Scattergl(\n",
    "                                x=selection.x,\n",
    "                                y=selection.y,\n",
    "                                hovertext=selection.doc if not hide_document_hover else None,\n",
    "                                hoverinfo=\"text\",\n",
    "                                text=selection.text,\n",
    "                                mode='markers+text',\n",
    "                                name=name,\n",
    "                                textfont=dict(size=12),\n",
    "                                marker=dict(size=5, opacity=0.5, color = [color_map2[doc] if doc is not None else [255, 255, 255] for doc in selection.doc]) #TO HERE\n",
    "                            )\n",
    "         \n",
    "                        )\n",
    "            all_traces.append(traces)\n",
    "\n",
    "\n",
    "\n",
    "        # Track and count traces\n",
    "        nr_traces_per_set = [len(traces) for traces in all_traces]\n",
    "        trace_indices = [(0, nr_traces_per_set[0])]\n",
    "        for index, nr_traces in enumerate(nr_traces_per_set[1:]):\n",
    "            start = trace_indices[index][1]\n",
    "            end = nr_traces + start\n",
    "            trace_indices.append((start, end))\n",
    "\n",
    "        # Visualization\n",
    "        fig = go.Figure()\n",
    "        for traces in all_traces:\n",
    "            for trace in traces:\n",
    "                fig.add_trace(trace)\n",
    "\n",
    "        for index in range(len(fig.data)):\n",
    "            if index >= nr_traces_per_set[0]:\n",
    "                fig.data[index].visible = False\n",
    "\n",
    "        # Create and add slider\n",
    "        steps = []\n",
    "        for index, indices in enumerate(trace_indices):\n",
    "            step = dict(\n",
    "                method=\"update\",\n",
    "                label=str(index),\n",
    "                args=[{\"visible\": [False] * len(fig.data)}]\n",
    "            )\n",
    "            for index in range(indices[1]-indices[0]):\n",
    "                step[\"args\"][0][\"visible\"][index+indices[0]] = True\n",
    "            steps.append(step)\n",
    "\n",
    "        sliders = [dict(\n",
    "            currentvalue={\"prefix\": \"Colour mode: \"},\n",
    "            pad={\"t\": 20},\n",
    "            steps=steps\n",
    "        )]\n",
    "        \n",
    "\n",
    "        # Add grid in a 'plus' shape\n",
    "        x_range = (df.x.min() - abs((df.x.min()) * .15), df.x.max() + abs((df.x.max()) * .15))\n",
    "        y_range = (df.y.min() - abs((df.y.min()) * .15), df.y.max() + abs((df.y.max()) * .15))\n",
    "        fig.add_shape(type=\"line\",\n",
    "                    x0=sum(x_range) / 2, y0=y_range[0], x1=sum(x_range) / 2, y1=y_range[1],\n",
    "                    line=dict(color=\"#CFD8DC\", width=2))\n",
    "        fig.add_shape(type=\"line\",\n",
    "                    x0=x_range[0], y0=sum(y_range) / 2, x1=x_range[1], y1=sum(y_range) / 2,\n",
    "                    line=dict(color=\"#9E9E9E\", width=2))\n",
    "        fig.add_annotation(x=x_range[0], y=sum(y_range) / 2, text=\"D1\", showarrow=False, yshift=10)\n",
    "        fig.add_annotation(y=y_range[1], x=sum(x_range) / 2, text=\"D2\", showarrow=False, xshift=10)\n",
    "\n",
    "        # Stylize layout\n",
    "        fig.update_layout(\n",
    "            sliders=sliders,\n",
    "            template=\"simple_white\",\n",
    "            title={\n",
    "                'text': f\"{title}\",\n",
    "                'x': 0.5,\n",
    "                'xanchor': 'center',\n",
    "                'yanchor': 'top',\n",
    "                'font': dict(\n",
    "                    size=22,\n",
    "                    color=\"Black\")\n",
    "            },\n",
    "            width=width,\n",
    "            height=height\n",
    "        )\n",
    "\n",
    "        fig.update_xaxes(visible=False)\n",
    "        fig.update_yaxes(visible=False)\n",
    "        return fig "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Erowid Quotes </h4>\n",
    "\n",
    "The data behind [Erowid Quotes](https://akseli-ilmanen.github.io/BSc-Dissertation/), was created int his cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CSV file for Erowid Quotes finder tool\n",
    "\n",
    "#for urls with no placeholders - find quote on the Erowid page\n",
    "def prepare_url(s, url):\n",
    "    s = s.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" i \", \" I \")\n",
    "    words = s.split()  # split the string into a list of words\n",
    "    first_three_words = ' '.join(words[:3])  # join the first three words with a space\n",
    "    last_three_words = ' '.join(words[-3:])  # join the last three words with a space\n",
    "    updated_url = url + \"#:~:text=\" + first_three_words.replace(\" \", \"%20\") + \",\" + last_three_words.replace(\" \", \"%20\")\n",
    "    return (updated_url)\n",
    "\n",
    "\n",
    "\n",
    "substances_classes_list_lowercase_subst = [\"Serotonergic psychedelics\", \"Dissociative psychedelics\", \"Entactogens\", \"Deliriants\", \"Depressant sedatives\", \"Stimulants\", \"Antidepressants antipsychotics\", \"lsd\", \"psilocybin mushrooms\", \"dmt\", \"mdma\", \"cannabis spp\", \"salvia divinorum\"]\n",
    "\n",
    "\n",
    "#create \n",
    "df4 = pd.DataFrame(columns=[\"Topic Nr\", \"Topic\", \"Substance\", \"Class\", \"Document\"])\n",
    "\n",
    "\n",
    "for substance_or_class in substances_classes_list_lowercase_subst:\n",
    "    print(substance_or_class)\n",
    "\n",
    "    #get temp df\n",
    "    temp_df2  = pd.read_pickle(f\"BERTopic files/BERTopic docs/BERTopic_df2_{substance_or_class}.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "    #for substances \"LSD\", \"Psilocybin mushrooms\", \"MDMA\", \"Cannabis spp\", \"Salvia divinorum\" use own topics (not class topics)\n",
    "    large_substances = [\"lsd\", \"dmt\", \"psilocybin mushrooms\", \"mdma\", \"cannabis spp\", \"salvia divinorum\"]\n",
    "    if substance_or_class.lower() not in large_substances:\n",
    "        temp_df2 = temp_df2[~temp_df2.filter(items=['substance']).isin(large_substances).any(axis=1)]\n",
    "\n",
    "    #remove outlier rows\n",
    "    #temp_df2 = temp_df2[~temp_df2.Topic == -1]\n",
    "\n",
    "    #sort df2 by highest probabilities\n",
    "    temp_df2 = temp_df2.sort_values(by=[\"Probability\"], ascending=False)\n",
    "    temp_df2.reset_index(drop=True, inplace=True) \n",
    "\n",
    "    #set custom labels for temp_df\n",
    "    labels = temp_topic_model.generate_topic_labels(nr_words=3, topic_prefix=True, word_length=15, separator=\" - \")\n",
    "    temp_topic_model.set_topic_labels(labels)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    substance_topics_combos = []\n",
    " \n",
    "    for i, topic_nr in enumerate(temp_df2.Topic):\n",
    "        if topic_nr != -1:\n",
    "            substance = temp_df2.loc[i, \"substance\"]\n",
    "            substance_topics_combo = str(topic_nr) + substance\n",
    "            substance_topics_combos.append(substance_topics_combo)\n",
    "            if substance_topics_combos.count(substance_topics_combo) <= 10: \n",
    "                class_ = temp_df2.loc[i, \"classes\"]\n",
    "                url = temp_df2.loc[i, \"url\"]\n",
    "                doc = temp_df2.loc[i, \"Document\"]\n",
    "                prob = round(temp_df2.loc[i, \"Probability\"], 3)\n",
    "                #get topic labels from topic_info df\n",
    "                if  len(str(topic_nr)) == 1: \n",
    "                    topic = \"00\" + temp_df2.loc[i, \"CustomName\"]\n",
    "                elif len(str(topic_nr)) == 2: \n",
    "                    topic = \"0\" + temp_df2.loc[i, \"CustomName\"]\n",
    "                else:\n",
    "                    topic = temp_df2.loc[i, \"CustomName\"]\n",
    "                #change url so it leads to the quote on the\n",
    "                if any(substring in doc for substring in [\"miranda\", \"megan\", \"matt\", \"alexa\"]): #exclude names not removed by Spacy in pre-processing\n",
    "                    pass \n",
    "                elif any(substring in doc for substring in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\"]):\n",
    "                    doc = \"...\" + doc + f\"...- TBS:{prob}  (NO URL)\" \n",
    "                else:\n",
    "                    url = prepare_url(doc, url)    \n",
    "                    doc = \"...\" + doc + f\"... - TBS:{prob}  (<a href={url}>URL</a>)\"       \n",
    "\n",
    "                df4.loc[len(df4.index)] = [topic_nr, topic, substance, class_, doc]\n",
    "\n",
    "#save\n",
    "df4.to_csv(\"Representative Quotes Per Topic-Substance.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ac408516564915e59f6571e8840a617524b2c5af7c094526d6726d37b65d83a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
