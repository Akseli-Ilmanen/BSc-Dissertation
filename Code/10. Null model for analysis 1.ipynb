{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43b06059",
   "metadata": {},
   "source": [
    "<h1> 10. Null model for analysis 1 </h1>\n",
    "\n",
    "Run analysis 1 for networks generated from random seed words. Very similar steps as in 7. Analysis 1 (Co-occurence networks).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00349197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "#functions\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from community import community_louvain\n",
    "\n",
    "\n",
    "\n",
    "#time_words\n",
    "ntp_words = ['time', 'period', 'periods', 'duration', 'clock', 'temporal', 'spacetime', 'timespan', 'timespans', 'timeline', 'timelines', 'elapse', 'elapsed', 'length', 'timewise', 'velocity', 'pace', 'rate', 'tempo', 'pass', 'passing', 'passed']\n",
    "ftp_words = ['quick','quicker', 'quickly', 'quickest', 'fast', 'faster', 'fastest', 'fastened', 'rapid','rapidly', 'short', 'shorter', 'shortly', 'shortest','speedy', 'speedy','speeded', 'speedier', 'hurry', 'hurried', 'swift', 'swifter', 'swiftly', 'haste', 'hasty', 'brisk', 'turbo', 'accelerate', 'acceleration', 'accelerated', 'accelerating']\n",
    "stp_words = ['slow', 'slower', 'slowly', 'slows', 'slowed', 'slowest', 'slowing', 'slowdown', 'long', 'looong', 'longer', 'longer', 'longest', 'steady', 'deceleration', 'decelerate', 'decelerating', 'decelerated', 'dilatory', 'dilation', 'infinity', 'eternity', 'lengthy', 'prolonged', 'protracted', 'extended', 'unending', 'endless']\n",
    "time_words = sorted(ntp_words + ftp_words + stp_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9d25f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_for_class(df2, class_): #\"class_\" can refer both class or substance with many reports\n",
    "\n",
    "    #exclude words of random list from networks\n",
    "    random_list = list(df2.seed.unique()) # remove random list \n",
    "    df2 = df2[~df2.filter(items=['source', 'target']).isin(random_list).any(axis=1)]\n",
    "\n",
    "\n",
    "    df2.insert(4, 'weight', 1)\n",
    "\n",
    "    if class_ == \"all\":\n",
    "        #remove non class rows\n",
    "        grouped = df2.groupby([\"classes\", \"source\", \"target\"], as_index=False)\n",
    "        \n",
    "        \n",
    "    elif class_ in [\"Serotonergic psychedelics\", \"Dissociative psychedelics\", \"Entactogens\", \"Deliriants\", \"Depressant / sedatives\", \"Stimulants\", \"Antidepressants / antipsychotics\"]:\n",
    "        #remove non class rows\n",
    "        df2 = df2[df2['classes'] == class_]\n",
    "        grouped = df2.groupby([\"source\", \"target\"], as_index=False)\n",
    "\n",
    "\n",
    "    elif class_ in [\"LSD\", \"Psilocybin mushrooms\", \"DMT\", \"MDMA\", \"Cannabis spp.\", \"Salvia divinorum\"]:\n",
    "        #remove non substance rows\n",
    "        df2 = df2[df2['substance'] == class_.lower()]\n",
    "        grouped = df2.groupby([\"source\", \"target\"], as_index=False)\n",
    "    \n",
    "\n",
    "    df2 = grouped[[\"weight\"]].agg({'weight': np.sum}) \n",
    "\n",
    "\n",
    "    #formatting\n",
    "    df2.sort_values(by=[\"weight\"], ascending=False, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)  \n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59018042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_words(df2):\n",
    "\n",
    "\n",
    "    #exclude self-loops (source = target)\n",
    "    df2 = df2[df2['source'] != df2['target']]\n",
    "\n",
    "    #remove non_seed_time_words + ... - \n",
    "    frequent_non_seed_time_words = [\"second\", \"seconds\", \"minute\", \"minutes\", \"hour\", \"hours\", \"day\", \"days\", \"week\", \"weeks\", \"weekend\", \"weekends\", \"month\", \"months\", \"year\", \"years\", \"times\", \"spend\", \"spent\", \"spending\", \"timestamp\", \"timestamps\"]\n",
    "\n",
    "    df2 = df2[~df2.filter(items=['source', 'target']).isin(frequent_non_seed_time_words).any(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93212cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tfidf(tfidf_df, class_, df2, gamma):  \n",
    "\n",
    "    if class_ in [\"LSD\", \"Psilocybin mushrooms\", \"DMT\", \"MDMA\", \"Cannabis spp.\", \"Salvia divinorum\"]:\n",
    "        class_ = class_.lower()\n",
    "    \n",
    "    #sort values by highest to lowest tfidf scores for a class\n",
    "    tfidf_df.sort_values(by=class_, ascending=False, inplace=True)\n",
    "    tfidf_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    #select top delta tfidf words and filter all edges where both words not in top tfidf\n",
    "    top_tfidf = tfidf_df[\"word\"].to_list()[:gamma]\n",
    "    df2 = df2[df2['source'].isin(top_tfidf) & df2['target'].isin(top_tfidf)]\n",
    "\n",
    "\n",
    "    #formatting\n",
    "    df2.sort_values(by=[\"weight\"], ascending=False, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac13585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterby_rw(df2, delta):\n",
    "    \n",
    "    #create temporary graph G1 to calculate strength and degree for each node\n",
    "    G1 = nx.from_pandas_edgelist(df2, edge_attr=\"weight\")\n",
    "    strength_dict = dict(G1.degree(weight=\"weight\"))\n",
    "    degree_dict = dict(G1.degree())\n",
    "\n",
    "    df2[\"prob_source\"] = 0\n",
    "    df2[\"prob_target\"] = 0\n",
    "\n",
    "    #iterate through all rows and calculate prob of null model for each edge\n",
    "    for i, weight in enumerate(df2.weight):\n",
    "        source = df2.loc[i, \"source\"]\n",
    "        target = df2.loc[i, \"target\"]\n",
    "\n",
    "        df2.loc[i, \"prob_source\"] = (1 - (weight / strength_dict[source]))**(degree_dict[source]-1)\n",
    "        df2.loc[i, \"prob_target\"] = (1 - (weight / strength_dict[target]))**(degree_dict[target]-1)\n",
    "\n",
    "    \n",
    "    #delta threshold for null model -> filter all edges where the prob of either node for null model is above delta\n",
    "    df2 = df2[((df2[\"prob_source\"] < delta) & (df2[\"prob_target\"] < delta))]\n",
    "    df2.drop([\"prob_source\", \"prob_target\"], axis=1, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    #formating\n",
    "    df2.sort_values(by=[\"weight\"], ascending=False, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    G2 = nx.from_pandas_edgelist(df2, edge_attr=\"weight\")\n",
    "\n",
    "\n",
    "    return df2, G2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31f65191",
   "metadata": {},
   "source": [
    "<h4> Modularity scores from random networks </h4>\n",
    "\n",
    "Repeat steps as in analysis 1. Instead of creating GEFX file, we calculate the Louvian modularity for each random network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68951336",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data and tfidf scores\n",
    "df2 = pd.read_pickle(\"timecorpus_C=4.pkl\")\n",
    "tfidf_df = pd.read_pickle(\"tfidf_df_C=4.pkl\")\n",
    "tfidf_df_SUB = pd.read_pickle(\"tfidf_df_C=4_SUBSTANCES.pkl\")\n",
    "tfidf_df_merged = pd.merge(tfidf_df, tfidf_df_SUB, on='word')\n",
    "\n",
    "modularity_dict = {}\n",
    "\n",
    "for j in range(15):\n",
    "    print(j)\n",
    "    df2 = pd.read_pickle(f\"Random corpus\\_randomcorpus_C=4_j={j}.pkl\")\n",
    "    def run_class(df2, class_, tfidf_df, gamma, delta):\n",
    "        df2 = aggregate_for_class (df2, class_)\n",
    "        df2 = exclude_words(df2)\n",
    "        df2 = filter_tfidf(tfidf_df, class_, df2, gamma)\n",
    "        df2, G2 = filterby_rw(df2, delta)\n",
    "        return df2, G2\n",
    "\n",
    "    #Choose class_, select gamma, delta with same values for each class as you have \n",
    "    #all Serotonergic psychedelics, Dissociative psychedelics, Entactogens, Deliriants, Depressant / sedatives, Stimulants, Antidepressants / antipsychotics, LSD, Psilocybin mushrooms, DMT, MDMA, Cannabis spp., Salvia divinorum\n",
    "    df2, G2  = run_class(df2=df2, class_=\"Serotonergic psychedelics\", tfidf_df=tfidf_df_merged, gamma=5000, delta=0.03)\n",
    "\n",
    "    partition = nx.community.louvain_communities(G2, weight='weight')\n",
    "\n",
    "    # Compute the modularity score\n",
    "    modularity = nx.algorithms.community.modularity(G2, partition)\n",
    "    modularity_dict[j] = [modularity, len(df2), partition]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "69e8bec3",
   "metadata": {},
   "source": [
    "<h4> Modularity by violin plot & index  </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fb5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Get list of modularity scores\n",
    "modularity_scores = [modularity_dict[value][0] for value in modularity_dict]\n",
    "\n",
    "\n",
    "# Modularity score when done with time perception seed words \n",
    "specific_score = 0.7 #\n",
    "\n",
    "#add to modularity scores\n",
    "modularity_scores.append(specific_score)\n",
    "\n",
    "\n",
    "# Create a figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "\n",
    "# Create the violin plot on the left subplot\n",
    "ax1.violinplot(modularity_scores, showmeans=True)\n",
    "\n",
    "# Add a title and axis labels\n",
    "ax1.set_title('Modularity Scores')\n",
    "ax1.set_xlabel('Distribution')\n",
    "ax1.set_ylabel('Score')\n",
    "\n",
    "# Create the dot plot on the right subplot\n",
    "positions = np.arange(len(modularity_scores))\n",
    "ax2.scatter(positions, modularity_scores, color='blue', alpha=0.5, label='Random seed words')\n",
    "\n",
    "# Add the specific score as a red dot\n",
    "ax2.scatter(positions[modularity_scores.index(specific_score)], specific_score, color='red', label='Time perception seed words')\n",
    "\n",
    "# Add an arrow with a label pointing at the specific score\n",
    "ax2.annotate('Specific score', xy=(positions[modularity_scores.index(specific_score)], specific_score), xytext=(positions[modularity_scores.index(specific_score)]+3, specific_score+0.1), arrowprops=dict(facecolor='black', arrowstyle='->'))\n",
    "\n",
    "# Add a legend\n",
    "ax2.legend()\n",
    "\n",
    "# Add a title and axis labels\n",
    "ax2.set_title('Modularity Scores with Specific Score')\n",
    "ax2.set_xlabel('Index')\n",
    "ax2.set_ylabel('Score')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ec5d87d0c9d602239099759a2ff0cea7d54e644ca45d44fb6e382449c6081fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
