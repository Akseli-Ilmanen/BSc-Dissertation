{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43b06059",
   "metadata": {},
   "source": [
    "<h1> Analysis 1 (Co-occurence networks) </h1>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad1eea28",
   "metadata": {},
   "source": [
    "Takes Time corpus data and tf-idf scores. Exclude words, filters nodes & edges and creates Gephi file of co-occurrence networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00349197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "#functions\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "#time_words\n",
    "ntp_words = ['time', 'period', 'periods', 'duration', 'clock', 'temporal', 'spacetime', 'timespan', 'timespans', 'timeline', 'timelines', 'elapse', 'elapsed', 'length', 'timewise', 'velocity', 'pace', 'rate', 'tempo', 'pass', 'passing', 'passed']\n",
    "ftp_words = ['quick','quicker', 'quickly', 'quickest', 'fast', 'faster', 'fastest', 'fastened', 'rapid','rapidly', 'short', 'shorter', 'shortly', 'shortest','speedy', 'speedy','speeded', 'speedier', 'hurry', 'hurried', 'swift', 'swifter', 'swiftly', 'haste', 'hasty', 'brisk', 'turbo', 'accelerate', 'acceleration', 'accelerated', 'accelerating']\n",
    "stp_words = ['slow', 'slower', 'slowly', 'slows', 'slowed', 'slowest', 'slowing', 'slowdown', 'long', 'looong', 'longer', 'longer', 'longest', 'steady', 'deceleration', 'decelerate', 'decelerating', 'decelerated', 'dilatory', 'dilation', 'infinity', 'eternity', 'lengthy', 'prolonged', 'protracted', 'extended', 'unending', 'endless']\n",
    "time_words = sorted(ntp_words + ftp_words + stp_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "629db649",
   "metadata": {},
   "source": [
    "<h4> Function: Aggregate for class </h4>\n",
    "\n",
    "Remember, Time corpus is in this format: \n",
    "\n",
    "|substance| classes | seed   | source    | tagert   | weight | colourbias | \n",
    "|----------------| ------------------------- | ------ | --------- | -------- | ------ | ---------- |\n",
    "|LSD| Serotonergic psychedelics | slowly | periphery | abstract | 1      | -0.1       |\n",
    "\n",
    "\n",
    "This function removes all the rows from non-selected class. And the aggregates the weight and colourbias for word pair duplicate (e.g. if source node is 'slowly' and target node is 'periphery' twice).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e9d25f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_for_class(df2, class_): #\"class_\" can refer both class or substance with many reports\n",
    "\n",
    "\n",
    "    df2.insert(4, 'weight', 1)\n",
    "\n",
    "    if class_ == \"all\":\n",
    "        #remove non class rows\n",
    "        grouped = df2.groupby([\"classes\", \"source\", \"target\"], as_index=False)\n",
    "        \n",
    "        \n",
    "    elif class_ in [\"Serotonergic psychedelics\", \"Dissociative psychedelics\", \"Entactogens\", \"Deliriants\", \"Depressant / sedatives\", \"Stimulants\", \"Antidepressants / antipsychotics\"]:\n",
    "        #remove non class rows\n",
    "        df2 = df2[df2['classes'] == class_]\n",
    "        grouped = df2.groupby([\"source\", \"target\"], as_index=False)\n",
    "\n",
    "\n",
    "    elif class_ in [\"LSD\", \"Psilocybin mushrooms\", \"DMT\", \"MDMA\", \"Cannabis spp.\", \"Salvia divinorum\"]:\n",
    "        #remove non substance rows\n",
    "        df2 = df2[df2['substance'] == class_.lower()]\n",
    "        grouped = df2.groupby([\"source\", \"target\"], as_index=False)\n",
    "    \n",
    "\n",
    "    df2 = grouped[[\"weight\"]].agg({'weight': np.sum}) \n",
    "    df2 = grouped[[\"weight\", \"colourbias\"]].agg({'weight': np.sum, 'colourbias': np.sum})\n",
    "\n",
    "\n",
    "\n",
    "    #formatting\n",
    "    df2.sort_values(by=[\"weight\"], ascending=False, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)  \n",
    "\n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dace3fd8",
   "metadata": {},
   "source": [
    "<h4> Function: Exclude words </h4>\n",
    "\n",
    "Exclude undesired words. In my case, self-loopsm, time words, frequent time words about the 'time of the day' were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "59018042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_words(df2):\n",
    "\n",
    "\n",
    "\n",
    "    #exclude self-loops (source = target)\n",
    "    df2 = df2[df2['source'] != df2['target']]\n",
    "\n",
    "\n",
    "    #remove non_seed_time_words of time of the day + time words - FOR NEXT RUN COULD INCLUDE 'frequent_non_seed_time_words' AGAIN\n",
    "    frequent_non_seed_time_words = [\"second\", \"seconds\", \"minute\", \"minutes\", \"hour\", \"hours\", \"day\", \"days\", \"week\", \"weeks\", \"weekend\", \"weekends\", \"month\", \"months\", \"year\", \"years\", \"times\", \"spend\", \"spent\", \"spending\", \"timestamp\", \"timestamps\"]\n",
    "\n",
    "    remove_list = frequent_non_seed_time_words + time_words \n",
    "    df2 = df2[~df2.filter(items=['source', 'target']).isin(remove_list).any(axis=1)]\n",
    "\n",
    "\n",
    "\n",
    "    #Not implemeneted. Could be used to colour nodes on network with 1-5 score of concreteness vs abstraction. Dataset for this available at http://crr.ugent.be/archives/1330\n",
    "    \"\"\"\n",
    "    #concretenesss\n",
    "    #get concreteness score as dictionary from Concreteness_ratings_Brysbaert_et_al_BRM\n",
    "    #exclude words not in Concreteness_ratings corpus\n",
    "    df_temp = pd.read_excel(\"Concreteness_ratings_Brysbaert_et_al_BRM.xlsx\")\n",
    "    concreteness_dict = pd.Series(df_temp[\"Conc.M\"].values,index=df_temp.Word).to_dict()\n",
    "    df2 = df2[df2[\"source\"].isin(list(concreteness_dict)) & df2[\"target\"].isin(list(concreteness_dict))] \n",
    "    \"\"\"\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b883a923",
   "metadata": {},
   "source": [
    "<h4> Function: Filter by tf-idf </h4>\n",
    "\n",
    "Only keep rows, where both source, target node are in top tf-idf list. How many values in top tf-idf, should depend on which class you are creating graph for. Parameter 'gamma' sets how many. (Larger dataset -> Larger gamma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "93212cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_tfidf(tfidf_df, class_, df2, gamma):  \n",
    "\n",
    "    #lowercase align\n",
    "    if class_ in [\"LSD\", \"Psilocybin mushrooms\", \"DMT\", \"MDMA\", \"Cannabis spp.\", \"Salvia divinorum\"]:\n",
    "        class_ = class_.lower()\n",
    "    \n",
    "    #sort values by highest to lowest tfidf scores for a class\n",
    "    tfidf_df.sort_values(by=class_, ascending=False, inplace=True)\n",
    "    tfidf_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    #select top delta tfidf words and filter all edges where both words not in top tfidf\n",
    "    top_tfidf = tfidf_df[\"word\"].to_list()[:gamma]\n",
    "    df2 = df2[df2['source'].isin(top_tfidf) & df2['target'].isin(top_tfidf)] # '&' means both nodes in row have to be in top_idf\n",
    "\n",
    "\n",
    "    #formatting\n",
    "    df2.sort_values(by=[\"weight\"], ascending=False, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "978be1a0",
   "metadata": {},
   "source": [
    "<h4> Function: Filter by relative weight size </h4>\n",
    "\n",
    "Filter all edges that do \"not carry a disproportionate fraction of a node's strength\". \n",
    "\n",
    "$$p_{ij} = (1 - \\frac{w_{ij}}{s_{i}})^{k_{i} - 1}$$\n",
    "\n",
    "$w_{ij}$ is the weight of an edge. $k_{i}$ and $s_{i}$ are the degegree and strength of a node<sub>i</sub>. The strength is a weighted version of degree by multiplying the sum of all the weights of edges to/from that node. If $p_{ij}$ is above a set threshold, the edge is excluded.\n",
    "\n",
    "Steps: \n",
    "- Create temporary graph with network\n",
    "- Calculate  $p_{ij}$\n",
    "- Exclude all rows where $p_{ij} > delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac13585c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterby_rw(df2, delta):\n",
    "    \n",
    "    #create temporary graph G1 (NetworkX) to calculate strength and degree for each node\n",
    "    G1 = nx.from_pandas_edgelist(df2, edge_attr=\"weight\")\n",
    "    strength_dict = dict(G1.degree(weight=\"weight\"))\n",
    "    degree_dict = dict(G1.degree())\n",
    "\n",
    "    df2[\"prob_source\"] = 0\n",
    "    df2[\"prob_target\"] = 0\n",
    "\n",
    "    #iterate through all rows and calculate prob of null model for each edge\n",
    "    for i, weight in enumerate(df2.weight):\n",
    "        source = df2.loc[i, \"source\"]\n",
    "        target = df2.loc[i, \"target\"]\n",
    "\n",
    "        df2.loc[i, \"prob_source\"] = (1 - (weight / strength_dict[source]))**(degree_dict[source]-1)\n",
    "        df2.loc[i, \"prob_target\"] = (1 - (weight / strength_dict[target]))**(degree_dict[target]-1)\n",
    "\n",
    "    \n",
    "    #delta threshold for null model -> filter all edges where the prob of either node for null model is above delta\n",
    "    df2 = df2[((df2[\"prob_source\"] < delta) & (df2[\"prob_target\"] < delta))]\n",
    "    df2.drop([\"prob_source\", \"prob_target\"], axis=1, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    #formating\n",
    "    df2.sort_values(by=[\"weight\"], ascending=False, inplace=True)\n",
    "    df2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e0e2d50",
   "metadata": {},
   "source": [
    "<h4> Function: Create GEFX file for Gephi </h4>\n",
    "\n",
    "Gephi is a software, you can plot the co-occurrence network in. \n",
    "\n",
    "For nodes and edges, create a 'normal' and 'relative' colourbias dictionary. For latter, include 'ceiling' The motivation for creating a relative colourbias (divided by weight) and with a ceiling is explained by me in this [YouTube video](https://youtu.be/U1zzyvW_WjM?t=146).\n",
    "\n",
    "Prepare\n",
    "- Create folders for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42012b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GEFX_with_attributes(df2, class_, gamma, delta):\n",
    "\n",
    "\n",
    "    #create relative colourbias by dividing through weight\n",
    "    df2['relative colourbias with ceiling'] = 0\n",
    "    df2['relative colourbias with ceiling'] = df2.apply(lambda row: row['colourbias'] / row['weight'], axis=1)\n",
    "\n",
    "\n",
    "    #nx_from_pandas_edgelist only works for 'A-B', or 'B-A' edges but does not sum them as one edge. The code below allows us to treat 'A-B' and 'B-A' as the same edge, and combine their weights.\n",
    "\n",
    "    # Create a copy of the DataFrame with reversed edges\n",
    "    df2_rev = df2.rename(columns={'source': 'target', 'target': 'source'})\n",
    "\n",
    "    # Concatenate the original DataFrame with the reversed DataFrame\n",
    "    df2_combined = pd.concat([df2, df2_rev])\n",
    "\n",
    "    # Group the combined DataFrame by the two nodes and sum the weights\n",
    "    df2 = df2_combined.groupby(['source', 'target']).agg({'weight': 'sum', 'colourbias': 'sum', 'relative colourbias with ceiling': 'sum'}).reset_index()\n",
    "\n",
    "\n",
    "    #colour with smaller absolute max value, sets ceiling for other colour - important for visualisation \n",
    "    max_value = df2['relative colourbias with ceiling'].max()\n",
    "    min_value = df2['relative colourbias with ceiling'].min()\n",
    "    ceiling_value_edges = min(max_value, abs(min_value))  \n",
    "    print(ceiling_value_edges)\n",
    "\n",
    "    #make sure that the most positive and most negative edge colourbias are the same value (at the ceiling)\n",
    "    for i, item in enumerate(df2['relative colourbias with ceiling']):\n",
    "        if abs(item) > ceiling_value_edges:\n",
    "            df2.loc[i, 'relative colourbias with ceiling'] = (item/abs(item))* ceiling_value_edges\n",
    "\n",
    "        \n",
    "\n",
    "    # Create a co-occurrence network from the DataFrame\n",
    "    G2 = nx.from_pandas_edgelist(df2, 'source', 'target', edge_attr=['weight', 'colourbias', 'relative colourbias with ceiling'])\n",
    "\n",
    "    \n",
    "\n",
    "    # Colourbias dictionary for clustering algorithm\n",
    "    node_colour_bias_attr_dict1 = {}\n",
    "    for col in ['source', 'target']:\n",
    "        for i, item in enumerate(df2[col].unique()):\n",
    "            # Get the rows corresponding to the node\n",
    "            node_rows = df2[df2[col] == item]\n",
    "            # Calculate the total colourbias for the node based on the new values\n",
    "            total_colourbias = node_rows['colourbias'].sum()\n",
    "            # Update the dictionary with the new value\n",
    "            node_colour_bias_attr_dict1[item] = total_colourbias\n",
    "\n",
    "\n",
    "    \n",
    "    #create relative colourbias by dividing through word frequency in Time Corpus\n",
    "    freq_dict = {}\n",
    "    for col in ['source', 'target']:\n",
    "        for i, item in enumerate(df2[col]):\n",
    "            freq_dict[item] = freq_dict.setdefault(item, 0) + df2.loc[i, \"weight\"]\n",
    "\n",
    "\n",
    "\n",
    "    node_colour_bias_attr_dict2 = node_colour_bias_attr_dict1.copy()\n",
    "    for item in node_colour_bias_attr_dict2:\n",
    "        node_colour_bias_attr_dict2[item] = node_colour_bias_attr_dict2[item]/freq_dict[item]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #colour with smaller absolute max value, sets ceiling for other colour - important for visualisation \n",
    "    max_value = max(node_colour_bias_attr_dict2.items(), key=lambda x: x[1])[1]\n",
    "    min_value = min(node_colour_bias_attr_dict2.items(), key=lambda x: x[1])[1]\n",
    "    ceiling_value_nodes = min(max_value, abs(min_value))\n",
    "    \n",
    "\n",
    "    #make sure that the most positive and most negative node colourbias are the same value (at the ceiling)\n",
    "    for item in node_colour_bias_attr_dict2:\n",
    "        if abs(node_colour_bias_attr_dict2[item]) > ceiling_value_nodes:\n",
    "            node_colour_bias_attr_dict2[item] = (node_colour_bias_attr_dict2[item]/abs(node_colour_bias_attr_dict2[item]))* ceiling_value_nodes\n",
    "\n",
    "\n",
    "\n",
    "    #set node colour & concreteness attributes\n",
    "    nx.set_node_attributes(G2, node_colour_bias_attr_dict1, name=\"absolute colour_bias\")\n",
    "    nx.set_node_attributes(G2, node_colour_bias_attr_dict2, name=\"relative colour_bias with ceiling\")\n",
    "\n",
    "\n",
    "\n",
    "    #Not implemeneted. Could be used to colour nodes on network with 1-5 score of concreteness vs abstraction.\n",
    "    \"\"\"\n",
    "    #remove all concreteness attributes, for which there are no graph nodes\n",
    "    concreteness_attr_dict = {}\n",
    "    for item in np.unique(df2[['source', 'target']].values):\n",
    "        concreteness_attr_dict[item] = concreteness_dict[item]\n",
    "\n",
    "    nx.set_node_attributes(G2, concreteness_attr_dict, name=\"concreteness 1-5\")\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # \"/\" does not possible in folders, files\n",
    "    if class_ == \"Antidepressants / antipsychotics\":\n",
    "        class_ = \"Antidepressants antipsychotics\"\n",
    "\n",
    "    elif class_ == \"Depressant / sedatives\":\n",
    "        class_ = \"Depressant sedatives\"\n",
    "\n",
    "    if class_ == \"Cannabis spp.\":\n",
    "        class_ = \"Cannabis spp\"\n",
    "    \n",
    "    \n",
    "    return df2, nx.write_gexf(G2, f\"Gephi/{class_}/{class_}_gamma={gamma}_delta={delta}-RANDOM.gexf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83a9ab71",
   "metadata": {},
   "source": [
    "<h4> Run functions </h4>\n",
    "\n",
    "Using Time corpus (df2) and tf-idf scores, run all the functins above. \n",
    "\n",
    "Here you can select: \n",
    "- class_\n",
    "- gamma (Threshold for tf-idf filtering)\n",
    "- delta (Significance threshold for relative weight filtering)\n",
    "\n",
    "\n",
    "Available classes:\n",
    "- 'all', 'Serotonergic psychedelics', 'Dissociative psychedelics', 'Entactogens', 'Deliriants', 'Depressant / sedatives', 'Stimulants', 'Antidepressants / antipsychotics', 'LSD', 'Psilocybin mushrooms', 'DMT', 'MDMA', 'Cannabis spp.', 'Salvia divinorum'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data and tfidf scores\n",
    "df2 = pd.read_pickle(\"timecorpus_C=4.pkl\")\n",
    "tfidf_df = pd.read_pickle(\"tfidf_df_C=4.pkl\")\n",
    "tfidf_df_SUB = pd.read_pickle(\"tfidf_df_C=4_SUBSTANCES.pkl\")\n",
    "tfidf_df_merged = pd.merge(tfidf_df, tfidf_df_SUB, on='word')\n",
    "\n",
    "\n",
    "\n",
    "def run_class(df2, class_, tfidf_df, gamma, delta):\n",
    "    df2 = aggregate_for_class (df2, class_)\n",
    "    df2 = exclude_words(df2)\n",
    "    df2 = filter_tfidf(tfidf_df, class_, df2, gamma)\n",
    "    df2 = filterby_rw(df2, delta)\n",
    "    df2 = get_GEFX_with_attributes(df2, class_, gamma, delta)\n",
    "    return df2, G2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2, G2  = run_class(df2=df2, class_=\"Serotonergic psychedelics\", tfidf_df=tfidf_df_merged, gamma=5000, delta=0.03)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbde6f34",
   "metadata": {},
   "source": [
    "<h4> Next steps </h4>\n",
    "\n",
    "Install [Gephi](https://gephi.org/) and replicate my workflow from this [YouTube video](https://youtu.be/U1zzyvW_WjM?t=146)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ec5d87d0c9d602239099759a2ff0cea7d54e644ca45d44fb6e382449c6081fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
