{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> 11. Other analyses </h1>\n",
    "\n",
    "These are a set of smaller additional analyses, that provide additional information but are not necessary for the co-occurence network or BERTopic analyses.\n",
    "\n",
    "<br>\n",
    "\n",
    "Varibales\n",
    "\n",
    "- df = Dataframe from pre-processing 3 ( title | substance | classes | url | text )\n",
    "- df2 = Dataframe with number of reports per substance\n",
    "- df4 = Dataframe with number of reports per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"processed_data_3.pkl\")\n",
    "\n",
    "#corpus list\n",
    "corpus_list = []\n",
    "for text in df.text:\n",
    "    corpus_list += text\n",
    "\n",
    "corpus_list = list(filter(lambda a: a not in [\"PERSON\", \"ORG\", \"GPE\", \"LOC\", \"DATE\", \"PLACEHOLDER\"], corpus_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Sample size (substances) </h4>\n",
    "\n",
    "Find the number of reports for each substance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Dict with number of reports per substance\n",
    "size_dict = dict(df.substance.value_counts())\n",
    "\n",
    "#Create df\n",
    "df2  = pd.DataFrame(columns = [\"substance\", \"classes\",\"sample size\"])\n",
    "\n",
    "\n",
    "substance_list = []\n",
    "\n",
    "#Create dataframe with substance, corresponding class and number of reports\n",
    "for i, substance in enumerate(df.substance):\n",
    "    if substance not in substance_list:\n",
    "        df2.loc[i, \"substance\"] = substance\n",
    "        df2.loc[i, \"classes\"] = df.loc[i, \"classes\"]\n",
    "        df2.loc[i, \"sample size\"] = size_dict[substance]\n",
    "        substance_list.append(substance)\n",
    "\n",
    "#save\n",
    "df2.to_csv('substances_sample_sizes.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Sample sizes (classes) </h4>\n",
    "\n",
    "Find the number of reports for each class. Map as pie chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.groupby(\"classes\")[\"sample size\"].sum()\n",
    "\n",
    "#pie chart\n",
    "plt.pie(df3.values, labels=df3.index, autopct=lambda pct: f\"{round(pct/100.*df3.sum()):,d} ({pct:.1f}%)\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Seed time perception words </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntp_words = ['time', 'period', 'periods', 'duration', 'clock', 'temporal', 'spacetime', 'timespan', 'timespans', 'timeline', 'timelines', 'elapse', 'elapsed', 'length', 'timewise', 'velocity', 'pace', 'rate', 'tempo', 'pass', 'passing', 'passed']\n",
    "ftp_words = ['quick','quicker', 'quickly', 'quickest', 'fast', 'faster', 'fastest', 'fastened', 'rapid','rapidly', 'short', 'shorter', 'shortly', 'shortest','speedy', 'speedy','speeded', 'speedier', 'hurry', 'hurried', 'swift', 'swifter', 'swiftly', 'haste', 'hasty', 'brisk', 'turbo', 'accelerate', 'acceleration', 'accelerated', 'accelerating']\n",
    "stp_words = ['slow', 'slower', 'slowly', 'slows', 'slowed', 'slowest', 'slowing', 'slowdown', 'long', 'looong', 'longer', 'longer', 'longest', 'steady', 'deceleration', 'decelerate', 'decelerating', 'decelerated', 'dilatory', 'dilation', 'infinity', 'eternity', 'lengthy', 'prolonged', 'protracted', 'extended', 'unending', 'endless']\n",
    "time_words = sorted(ntp_words + ftp_words + stp_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Balancing fast-slow seed words </h4>\n",
    "\n",
    "- Ftp-stp words were 'balanced' by considering their relative frequency in the common English language. The SUBTLEX-UK is a 201.3 million words corpus generated from subtitles in BBC broadcasts. (Available [here](http://crr.ugent.be/archives/1423)).\n",
    "\n",
    "- The code below gets the frequencies of all ftp-stp words in SUBTLEX-UK. Adjust ftp-stp words, so they have an equal cummulative frequency in the SUBTLEX-UK corpus.\n",
    "\n",
    "- df4 compares the relative frequencies of specific time words in both corpora\n",
    "\n",
    "\n",
    "<h6> van Heuven, W.J.B., Mandera, P., Keuleers, E. and Brysbaert, M., 2014. Subtlex-UK: A New and Improved Word Frequency Database for British English. Quarterly Journal of Experimental Psychology [Online], 67(6), pp.1176â€“1190. Available from: https://doi.org/10.1080/17470218.2013.850521. </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBTLEX-UK corpus\n",
    "df4 = pd.read_excel(\"SUBTLEX-UK.xlsx\")\n",
    "\n",
    "#renaming\n",
    "df4.rename(columns = {'FreqCount':'freq', 'Spelling':\"word\"}, inplace = True)\n",
    "\n",
    "#remove all columns but word \n",
    "df4.drop(df4.columns.difference(['word', 'freq']), 1, inplace=True)\n",
    "#sort values\n",
    "df4.sort_values('freq', ascending=False, inplace=True)\n",
    "df4.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#remove words from SUBTLEX not in Erowid corpus\n",
    "#df5 for later Zipfs law analysis \n",
    "df5 = df4[df4['word'].isin(set(corpus_list))]\n",
    "\n",
    "#remove words from SUBTLEX not in time words\n",
    "df4 = df4[df4['word'].isin(time_words)]\n",
    "\n",
    "\n",
    "#frequency of ftp-stp words in SUBTLEX \n",
    "df_fast = df4[~df4.filter(items=['word']).isin(ntp_words + stp_words).any(1)] \n",
    "print(\"SUBTLEX-UK FTP sum is\")\n",
    "print(df_fast[\"freq\"].sum())\n",
    "\n",
    "df_slow = df4[~df4.filter(items=['word']).isin(ntp_words + ftp_words).any(1)] \n",
    "print(\"SUBTLEX-UK STP sum is\")\n",
    "print(df_slow[\"freq\"].sum())\n",
    "\n",
    "\n",
    "#relative word frequency (=Erowid/SUBTLEX) dataframe\n",
    "\n",
    "df4[\"Erowid_freq\"] = 0\n",
    "df4[\"Relative_freq\"] = 0\n",
    "\n",
    "for i, word in enumerate(df4.word):\n",
    "    df4.loc[i, \"Erowid_freq\"] = corpus_list.count(word)\n",
    "    df4.loc[i, \"Relative_freq\"] = corpus_list.count(word)/df4.loc[i, \"freq\"]\n",
    "\n",
    "df4 = df4.sort_values(by='Relative_freq', ascending=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Get context window function </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get context window function for time_words\n",
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        if center_word not in time_words:\n",
    "            i += 1\n",
    "            pass\n",
    "        else:\n",
    "            context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "            yield context_words, center_word\n",
    "            i += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> Modified get context window (CW) function </h4>\n",
    "\n",
    "See 7. Get Time corpus 2 (For BERTopic).ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified get context window function for time_words\n",
    "def modified_get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        if center_word not in time_words:\n",
    "            i += 1\n",
    "            pass\n",
    "        else:\n",
    "            context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "            for word in time_words:\n",
    "                if word in context_words:\n",
    "                    i += C\n",
    "                    pass\n",
    "            yield context_words, center_word\n",
    "            i += 1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Zipf's law - data preparation </hr3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erowid corpus frequency dict\n",
    "counter = Counter(corpus_list)\n",
    "corpus_dict = dict(Counter({k: c for k, c in counter.items()}))\n",
    "\n",
    "\n",
    "#time corpus (C=4) \n",
    "time_corpus_list_C4 = []\n",
    "for context_words, center_word in get_windows(corpus_list, C=4):\n",
    "    time_corpus_list_C4 += context_words\n",
    "\n",
    "#Time corpus frequency dict (C=4)\n",
    "counter_C4 = Counter(time_corpus_list_C4)\n",
    "time_corpus_dict_C4 = dict(Counter({k: c for k, c in counter_C4.items()}))\n",
    "\n",
    "\n",
    "#tf idf frequency dict (C=4)\n",
    "tfidf_df_C4 = pd.read_pickle(\"tfidf_df_C=4.pkl\")\n",
    "\n",
    "#remove rows with values 0, as these are words not in the time corpus\n",
    "tfidf_df_C4 = tfidf_df_C4[tfidf_df_C4[\"all\"] != 0]\n",
    "#remove rows with time seed words\n",
    "tfidf_df_C4 = tfidf_df_C4[~tfidf_df_C4.filter(items=['word']).isin(time_words).any(axis=1)]\n",
    "\n",
    "#sort by tf-idf\n",
    "tfidf_df_C4.sort_values(by=[\"all\"], ascending=False, inplace=True)\n",
    "tfidf_df_C4.reset_index(drop=True, inplace=True)\n",
    "tfidf_list_C4 = tfidf_df_C4[\"all\"].to_list()\n",
    "\n",
    "#tf_idf_values go from negative values to positives values, normalize them from 0 to 1 for graph\n",
    "min_value = min(tfidf_list_C4)\n",
    "max_value = max(tfidf_list_C4)\n",
    "normalized_tfidf_list_C4 = [(x - min_value) / (max_value - min_value) for x in tfidf_list_C4]\n",
    "\n",
    "\n",
    "\n",
    "#time corpus (C=15)\n",
    "time_corpus_list_C15 = []\n",
    "for context_words, center_word in get_windows(corpus_list, C=15):\n",
    "        time_corpus_list_C15 += context_words\n",
    "\n",
    "#Time corpus frequency dict (C=15)\n",
    "counter_C15 = Counter(time_corpus_list_C15)\n",
    "time_corpus_dict_C15 = dict(Counter({k: c for k, c in counter_C15.items()}))\n",
    "\n",
    "#tf idf frequency dict (C=15)\n",
    "tfidf_df_C15 = pd.read_pickle(\"tfidf_df_C=15.pkl\")\n",
    "\n",
    "#remove rows with values 0, as these are words not in the time corpus\n",
    "tfidf_df_C15 = tfidf_df_C15[tfidf_df_C15[\"all\"] != 0]\n",
    "#remove rows with time seed words\n",
    "tfidf_df_C15 = tfidf_df_C15[~tfidf_df_C15.filter(items=['word']).isin(time_words).any(axis=1)]\n",
    "\n",
    "#sort by tf-idf\n",
    "tfidf_df_C15.sort_values(by=[\"all\"], ascending=False, inplace=True)\n",
    "tfidf_df_C15.reset_index(drop=True, inplace=True)\n",
    "tfidf_list_C15 = tfidf_df_C15[\"all\"].to_list()\n",
    "\n",
    "#tf_idf_values go from negative values to positives values, normalize them from 0 to 1 for graph\n",
    "min_value = min(tfidf_list_C15)\n",
    "max_value = max(tfidf_list_C15)\n",
    "normalized_tfidf_list_C15 = [(x - min_value) / (max_value - min_value) for x in tfidf_list_C15]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Zipf's law - word frequency </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted dictionaries from large to small freq\n",
    "sorted_dict_corpus = dict(OrderedDict(sorted(corpus_dict.items(), key = lambda x: x[1],reverse = True)))\n",
    "sorted_dict_C4 = dict(OrderedDict(sorted(time_corpus_dict_C4.items(), key = lambda x: x[1],reverse = True)))\n",
    "sorted_dict_C15 = dict(OrderedDict(sorted(time_corpus_dict_C15.items(), key = lambda x: x[1],reverse = True)))\n",
    "\n",
    "#freq and rank of each word in list format\n",
    "freqs_subtlex = df5.freq.tolist()[:12703]\n",
    "ranks_subtlex = np.arange(1, len(freqs_subtlex)+1)\n",
    "\n",
    "\n",
    "freqs_corpus = list(sorted_dict_corpus.values())[:12703] #since they are at slightly varying lengths, cap all at 12650\n",
    "ranks_corpus = np.arange(1, len(freqs_corpus)+1)\n",
    "\n",
    "freqs_C4 = list(sorted_dict_C4.values())[:12703]\n",
    "ranks_C4 = np.arange(1, len(freqs_C4)+1)\n",
    "\n",
    "freqs_C15 = list(sorted_dict_C15.values())[:12703]\n",
    "ranks_C15 = np.arange(1, len(freqs_C15)+1)\n",
    "\n",
    "\n",
    "\n",
    "#plot 3 lines on loglog curve\n",
    "plt.loglog(ranks_subtlex, freqs_subtlex, label='SUBTLEX corpus')\n",
    "plt.loglog(ranks_corpus, freqs_corpus, label='Erowid corpus')\n",
    "plt.loglog(ranks_C15, freqs_C4, label='Time corpus (C=4)')\n",
    "plt.loglog(ranks_C15, freqs_C15, label='Time corpus (C=15)')\n",
    "\n",
    "\n",
    "\n",
    "# Set up the tick formatter\n",
    "def log_tick_formatter(val, pos=None):\n",
    "    \"\"\"\n",
    "    Convert a log tick value to a plain tick value.\n",
    "    \"\"\"\n",
    "    if val < 1:\n",
    "        return '{:.3g}'.format(val)\n",
    "    else:\n",
    "        return int(val)\n",
    "\n",
    "\n",
    "# Format the x-axis and y-axis tick labels\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "\n",
    "\n",
    "# annotate with arrows for words\n",
    "zero_value = 0\n",
    "\n",
    "#subtlex dictionary\n",
    "subtlex_dict = dict(zip(df5[\"word\"], df5[\"freq\"]))\n",
    "\n",
    "#blue (SUBTLEX corpus) \n",
    "plt.annotate('\"experience\"', xy=(list(subtlex_dict).index(\"experience\"), subtlex_dict[\"experience\"]), xytext=(zero_value+30, zero_value+400000),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(48/255, 129/255, 185/255)))     \n",
    "\n",
    "plt.annotate('\"heart\"', xy=(list(subtlex_dict).index(\"heart\"), subtlex_dict[\"heart\"]), xytext=(zero_value+18, zero_value+200),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(48/255, 129/255, 185/255)))\n",
    "\n",
    "plt.annotate('\"continuum\"', xy=(list(subtlex_dict).index(\"continuum\"), subtlex_dict[\"continuum\"]), xytext=(zero_value+300, zero_value+10),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(48/255, 129/255, 185/255)))\n",
    "\n",
    "\n",
    "#orange (Erowid coprus)\n",
    "plt.annotate('          ', xy=(list(sorted_dict_corpus).index(\"experience\"), sorted_dict_corpus[\"experience\"]), xytext=(zero_value+30, zero_value+400000),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(252/255, 142/255, 42/255)))     \n",
    "\n",
    "plt.annotate('     ', xy=(list(sorted_dict_corpus).index(\"heart\"), sorted_dict_corpus[\"heart\"]), xytext=(zero_value+25, zero_value+200),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(252/255, 142/255, 42/255)))\n",
    "\n",
    "plt.annotate('         ', xy=(list(sorted_dict_corpus).index(\"continuum\"), sorted_dict_corpus[\"continuum\"]), xytext=(zero_value+600, zero_value+10),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(252/255, 142/255, 42/255)))\n",
    "\n",
    "\n",
    "\n",
    "#green (C=4)\n",
    "plt.annotate('          ', xy=(list(sorted_dict_C4).index(\"experience\"), sorted_dict_C4[\"experience\"]), xytext=(zero_value+30, zero_value+400000),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(61/255, 168/255, 61/255)))\n",
    "\n",
    "plt.annotate('     ', xy=(list(sorted_dict_C4).index(\"heart\"), sorted_dict_C4[\"heart\"]), xytext=(zero_value+20, zero_value+200),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(61/255, 168/255, 61/255)))\n",
    "\n",
    "plt.annotate('         ', xy=(list(sorted_dict_C4).index(\"continuum\"), sorted_dict_C4[\"continuum\"]), xytext=(zero_value+300, zero_value+10),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(61/255, 168/255, 61/255)))\n",
    "\n",
    "\n",
    "#red (C=15)\n",
    "plt.annotate('          ', xy=(list(sorted_dict_C15).index(\"experience\"), sorted_dict_C15[\"experience\"]), xytext=(zero_value+30, zero_value+400000),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(218/255, 60/255, 61/255)))\n",
    "      \n",
    "plt.annotate('     ', xy=(list(sorted_dict_C15).index(\"heart\"), sorted_dict_C15[\"heart\"]), xytext=(zero_value+20, zero_value+200),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(218/255, 60/255, 61/255)))\n",
    "\n",
    "plt.annotate('         ', xy=(list(sorted_dict_C15).index(\"continuum\"), sorted_dict_C15[\"continuum\"]), xytext=(zero_value+300, zero_value+10),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(218/255, 60/255, 61/255)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Zipf\\'s law (Word frequency)'); plt.xlabel('Word rank'); plt.ylabel('Word frequency'); plt.legend(); plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Zipf's law - cummulative distribution </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pct_subtlex = [100*x / sum(freqs_subtlex) for x in freqs_subtlex]\n",
    "\n",
    "pct_corpus = [100*x / sum(freqs_corpus) for x in freqs_corpus]\n",
    "\n",
    "pct_C4 = [100*x / sum(freqs_C4) for x in freqs_C4]\n",
    "\n",
    "pct_C15 = [100*x / sum(freqs_C15) for x in freqs_C15]\n",
    "\n",
    "\n",
    "# Plot all dictionaries on the same graph\n",
    "cum_freqs_subtlex = np.cumsum(pct_subtlex)\n",
    "cum_freqs_corpus = np.cumsum(pct_corpus)\n",
    "cum_freqs_C4 = np.cumsum(pct_C4)\n",
    "cum_freqs_C15 = np.cumsum(pct_C15)\n",
    "\n",
    "#plot 3 lines on loglog curve\n",
    "plt.plot(ranks_subtlex, cum_freqs_subtlex, label='SUBTLEX corpus')\n",
    "plt.plot(ranks_corpus, cum_freqs_corpus, label='Erowid corpus')\n",
    "plt.plot(ranks_C4, cum_freqs_C4, label='Time corpus (C=4)')\n",
    "plt.plot(ranks_C15, cum_freqs_C15, label='Time corpus (C=15)')\n",
    "\n",
    "\n",
    "# Format the y-axis as a percentage\n",
    "def percent_tick_formatter(val, pos=None):\n",
    "    \"\"\"\n",
    "    Convert a numeric tick value to a percentage tick value.\n",
    "    \"\"\"\n",
    "    return '{:.0f}%'.format(val)\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(percent_tick_formatter))\n",
    "\n",
    "# Format the x-axis and y-axis tick labels\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "plt.gca().yaxis.set_tick_params(which='minor', labelleft=False)  # Remove minor tick labels\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "\n",
    "plt.title('Zipf\\'s law (Cumulative percentage of corpus)'); plt.xlabel('Word rank'); plt.ylabel('Cumulative percentage of corpus')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Zipf's law - tf-idf </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep\n",
    "index_exper_C4 = tfidf_df_C4[\"word\"].to_list().index(\"experience\")\n",
    "index_heart_C4 = tfidf_df_C4[\"word\"].to_list().index(\"heart\")\n",
    "index_continuum_C4 = tfidf_df_C4[\"word\"].to_list().index(\"continuum\")\n",
    "\n",
    "index_exper_C15 = tfidf_df_C15[\"word\"].to_list().index(\"experience\")\n",
    "index_heart_C15 = tfidf_df_C15[\"word\"].to_list().index(\"heart\")\n",
    "index_continuum_C15 = tfidf_df_C15[\"word\"].to_list().index(\"continuum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq and rank of each word in list format\n",
    "freqs_C4 = normalized_tfidf_list_C4\n",
    "ranks_C4 = np.arange(1, len(freqs_C4)+1)\n",
    "\n",
    "freqs_C15 = normalized_tfidf_list_C15\n",
    "ranks_C15 = np.arange(1, len(freqs_C15)+1)\n",
    "\n",
    "\n",
    "# Create the figure and first axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "#plot Erowid corpus on the left axis\n",
    "ax.loglog(ranks_corpus, freqs_corpus, label='Erowid corpus', color='orange')\n",
    "\n",
    "\n",
    "# Set up the tick formatter\n",
    "def log_tick_formatter(val, pos=None):\n",
    "    \"\"\"\n",
    "    Convert a log tick value to a plain tick value.\n",
    "    \"\"\"\n",
    "    if val < 1:\n",
    "        return '{:.3g}'.format(val)\n",
    "    else:\n",
    "        return int(val)\n",
    "\n",
    "\n",
    "def log_tick_formatter_right(val, pos=None):\n",
    "    \"\"\"\n",
    "    Convert a log tick value to a plain tick value for the second y-axis.\n",
    "    \"\"\"\n",
    "    return '{:.3g}'.format(val)\n",
    "\n",
    "\n",
    "# Format the x-axis and y-axis tick labels\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "plt.gca().yaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "# Set the font size of the x-axis and y-axis tick labels\n",
    "ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "# Create a second y-axis on the right side\n",
    "ax2 = ax.twinx()\n",
    "\n",
    "# Plot the lines for the Time corpus (C=4) and Time corpus (C=15) on the second y-axis\n",
    "ax2.semilogx(ranks_C4, freqs_C4, label='Time corpus (C=4)', color='green')\n",
    "ax2.semilogx(ranks_C15, freqs_C15, label='Time corpus (C=15)', color='red')\n",
    "\n",
    "# Set up the tick formatter for the second y-axis\n",
    "ax2.yaxis.set_major_formatter(FuncFormatter(log_tick_formatter_right))\n",
    "ax.xaxis.set_major_formatter(FuncFormatter(log_tick_formatter))\n",
    "\n",
    "\n",
    "# Set the font size of the y-axis tick labels for the second y-axis\n",
    "ax2.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "#axes labels\n",
    "ax.set_xlabel('Word rank')\n",
    "ax.set_ylabel('Word frequency')\n",
    "ax2.set_ylabel('Normalized tf-idf value')\n",
    "\n",
    "# Add a legend to the plot\n",
    "handles1, labels1 = ax.get_legend_handles_labels()\n",
    "handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "\n",
    "\n",
    "fig.legend(handles1, labels1, loc='lower left', fontsize=10)\n",
    "fig.legend(handles2, labels2, loc='lower right', fontsize=10)\n",
    "\n",
    "\n",
    "\n",
    "# annotate with arrows for words\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#orange Erowid corpus\n",
    "ax.annotate('\"experience\"', xy=(list(sorted_dict_corpus).index(\"experience\"), sorted_dict_corpus[\"experience\"]), xytext=(zero_value+2, zero_value+100),\n",
    "             arrowprops=dict(facecolor='red', arrowstyle='->', color=(252/255, 142/255, 42/255)),\n",
    "             annotation_clip=False)\n",
    "\n",
    "ax.annotate('\"heart\"', xy=(list(sorted_dict_corpus).index(\"heart\"), sorted_dict_corpus[\"heart\"]), xytext=(zero_value+150, zero_value+35000),\n",
    "             arrowprops=dict(facecolor='red', arrowstyle='->', color=(252/255, 142/255, 42/255)),\n",
    "             annotation_clip=False)\n",
    "\n",
    "ax.annotate('\"continuum\"', xy=(list(sorted_dict_corpus).index(\"continuum\"), sorted_dict_corpus[\"continuum\"]), xytext=(zero_value+60, zero_value+500),\n",
    "             arrowprops=dict(facecolor='red', arrowstyle='->', color=(252/255, 142/255, 42/255)),\n",
    "             annotation_clip=False)\n",
    "\n",
    "\n",
    "#indices (C=4)\n",
    "index_exper_C4 = tfidf_df_C4[\"word\"].to_list().index(\"experience\")\n",
    "index_heart_C4 = tfidf_df_C4[\"word\"].to_list().index(\"heart\")\n",
    "index_continuum_C4 = tfidf_df_C4[\"word\"].to_list().index(\"continuum\")\n",
    "\n",
    "#green (C=4)\n",
    "ax2.annotate('          ', xy=(index_exper_C4, freqs_C4[index_exper_C4]), xytext=(zero_value+2, zero_value+0.23),\n",
    "             arrowprops=dict(facecolor='green', arrowstyle='->', color=(61/255, 168/255, 61/255)))\n",
    "\n",
    "ax2.annotate('     ', xy=(index_heart_C4, freqs_C4[index_heart_C4]), xytext=(zero_value+190, zero_value+0.9),\n",
    "             arrowprops=dict(facecolor='green', arrowstyle='->', color=(61/255, 168/255, 61/255)))\n",
    "\n",
    "ax2.annotate('         ', xy=(index_continuum_C4, freqs_C4[index_continuum_C4]), xytext=(zero_value+60, zero_value+0.43),\n",
    "             arrowprops=dict(facecolor='green', arrowstyle='->', color=(61/255, 168/255, 61/255)))\n",
    "\n",
    "\n",
    "#indices (C=)\n",
    "index_exper_C15 = tfidf_df_C15[\"word\"].to_list().index(\"experience\")\n",
    "index_heart_C15 = tfidf_df_C15[\"word\"].to_list().index(\"heart\")\n",
    "index_continuum_C15 = tfidf_df_C15[\"word\"].to_list().index(\"continuum\")\n",
    "\n",
    "\n",
    "#red (C=15)\n",
    "ax2.annotate('          ', xy=(index_exper_C15, freqs_C15[index_exper_C15]), xytext=(zero_value+2, zero_value+0.22),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(218/255, 60/255, 61/255)))\n",
    "      \n",
    "ax2.annotate('     ', xy=(index_heart_C15, freqs_C15[index_heart_C15]), xytext=(zero_value+200, zero_value+0.9),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(218/255, 60/255, 61/255)))\n",
    "\n",
    "ax2.annotate('         ', xy=(index_continuum_C15, freqs_C15[index_continuum_C15]), xytext=(zero_value+60, zero_value+0.43),\n",
    "             arrowprops=dict(facecolor='blue', arrowstyle='->', color=(218/255, 60/255, 61/255)))\n",
    "\n",
    "\n",
    "\n",
    "plt.title('Word frequency vs tf-idf')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Wordclouds - word frequency vs tf-idf </h3>\n",
    "\n",
    "Create 4 word clouds, to compare 4x4: two classes as word freq vs tf-idf.  \n",
    "\n",
    "Download as csv file and use at https://wordart.com/create, it looks nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove from wordclouds\n",
    "\n",
    "#temp remove list\n",
    "remove_list = [\"zyprexa\", \"tryptamine\", \"vaporiser\", \"redose\", \"peter\", \"psychedelic\", \"stramonium\", \"shannon\", \"pod\", \"atropine\", \"deliriants\", \"nightshade\", \"sceletium\", \"suboxone\", \"subutex\", \"zyprexa\", \"citalopram\", \"zoloft\", \"fluoxetine\", \"benzo\", \"cbd\", \"lotus\", \"rayanne\", \"opiates\", \"hashish\", \"alcoholic\", \"analgesia\", \"vodka\", \"apnea\", \"analgesic\", \"tylenol\", \"cigar\", \"antihistamine\", \"benedryl\", \"vaped\", \"trazodone\", \"tweaker\", \"mah\", \"desoxyn\", \"crank\", \"booster\", \"meph\", \"canker\" , \"entactogenic\", \"xtc\", \"albert\", \"antihistamine\", \"capsule\", \"butane\", \"sinicuichi\", \"ferris\", \"cory\", \"bromo\", \"bodyload\", \"tryptamine\", \"phenethylamines\", \"zeta\", \"glauca\", \"stims\", \"vivarin\", \"gourd\", \"yerba\", \"caffine\", \"cigs\", \"modalert\", \"cigar\", \"nutmegs\", \"tachycardia\", \"adderal\", \"bzp\", \"espresso\", \"hookah\", \"piperazine\", \"phentermine\", \"nut\", \"ciggarette\", \"betel\"]\n",
    "\n",
    "#remove non_seed_time_words + ... - later done by pre-processing\n",
    "master_remove_list = remove_list + time_words + [\"second\", \"seconds\", \"minute\", \"minutes\", \"hour\", \"hours\", \"day\", \"days\", \"week\", \"weeks\", \"weekend\", \"weekends\", \"month\", \"months\", \"year\", \"years\", \"times\", \"spend\", \"spent\", \"spending\", \"timestamp\", \"timestamps\"]\n",
    "\n",
    "\n",
    "#generate and save word freq word clouds for two types of classes\n",
    "df_Serot = df[df.filter(items=['classes']).isin(['Serotonergic psychedelics']).any(1)]\n",
    "df_Stimul = df[df.filter(items=['classes']).isin(['Stimulants']).any(1)]\n",
    "\n",
    "\n",
    "Serot_corpus_list = []\n",
    "for text in df_Serot.text:\n",
    "    Serot_corpus_list += text\n",
    "\n",
    "Stimul_corpus_list = []\n",
    "for text in df_Stimul.text:\n",
    "    Stimul_corpus_list += text\n",
    "\n",
    "Serot_time_corpus_list = []\n",
    "for context_words, center_word in get_windows(Serot_corpus_list, C=4):\n",
    "    Serot_time_corpus_list += context_words\n",
    "\n",
    "Stimul__time_corpus_list = []\n",
    "for context_words, center_word in get_windows(Stimul_corpus_list, C=4):\n",
    "    Stimul__time_corpus_list += context_words\n",
    "\n",
    "Serot_counter = Counter(Serot_time_corpus_list)\n",
    "wfreq_df_Serot = pd.DataFrame.from_dict(dict(Serot_counter), orient='index', columns=['frequency'])\n",
    "\n",
    "\n",
    "Stimul_counter = Counter(Stimul__time_corpus_list)\n",
    "wfreq_df_Stimul = pd.DataFrame.from_dict(dict(Stimul_counter), orient='index', columns=['frequency'])\n",
    "\n",
    "\n",
    "wfreq_df_Serot = wfreq_df_Serot[~wfreq_df_Serot.filter(items=['']).isin(master_remove_list).any(axis=1)]\n",
    "wfreq_df_Serot.to_csv(\"wordcloud_wfreq_Serot.csv\")\n",
    "\n",
    "wfreq_df_Stimul = wfreq_df_Stimul[~wfreq_df_Stimul.filter(items=['']).isin(master_remove_list).any(axis=1)]\n",
    "wfreq_df_Stimul.to_csv(\"wordcloud_wfreq_Stimul.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#generate and save tf idf word clouds\n",
    "tfidf_df_Serot = tfidf_df_C4.loc[:, ['word', 'Serotonergic psychedelics']]\n",
    "tfidf_df_Serot = tfidf_df_Serot[~tfidf_df_Serot.filter(items=['word']).isin(remove_list).any(axis=1)]\n",
    "tfidf_df_Serot.to_csv(\"wordcloud_tfidf_Serot.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tfidf_df_Stimul = tfidf_df_C4.loc[:, ['word', 'Stimulants']]\n",
    "tfidf_df_Stimul = tfidf_df_Stimul[~tfidf_df_Stimul.filter(items=['word']).isin(remove_list).any(axis=1)]\n",
    "tfidf_df_Stimul.to_csv(\"wordcloud_tfidf_Stimul.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4ac408516564915e59f6571e8840a617524b2c5af7c094526d6726d37b65d83a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
