{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f69531",
   "metadata": {},
   "source": [
    "<h1> Pre-Processing 2 </h1>\n",
    "\n",
    "\n",
    "Currently, there are 21,548 reports with a total of 20,361,959 words. \n",
    "\n",
    "Pre-processing 2 involves:\n",
    "- Tokenization \n",
    "- Lemmatization\n",
    "- Removing different types of words (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04f0fc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3486b5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data after pre-processing 1\n",
    "df = pd.read_excel(\"complete_processed_data.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0acdf",
   "metadata": {},
   "source": [
    "<h3> Tokenization </h3>\n",
    "\n",
    "Tokenize words and remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0945f342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_words(string):\n",
    "    data = re.sub(r'[,!?;-]+', '.', string)\n",
    "    data = nltk.word_tokenize(data)  # tokenize string to words\n",
    "    data = [ ch.lower() for ch in data\n",
    "             if ch.isalpha()\n",
    "             or ch == '.' \n",
    "           ]\n",
    "    return data\n",
    "\n",
    "df.text = df.text.loc[:].apply(tokenize_words)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27da89c7",
   "metadata": {},
   "source": [
    "<h4> Time words </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bbb6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ntp_words = ['time', 'period', 'periods', 'duration', 'clock', 'temporal', 'spacetime', 'timespan', 'timespans', 'timeline', 'timelines', 'elapse', 'elapsed', 'length', 'timewise', 'velocity', 'pace', 'rate', 'tempo', 'pass', 'passing', 'passed']\n",
    "ftp_words = ['quick','quicker', 'quickly', 'quickest', 'fast', 'faster', 'fastest', 'fastened', 'rapid','rapidly', 'short', 'shorter', 'shortly', 'shortest','speedy', 'speedy','speeded', 'speedier', 'hurry', 'hurried', 'swift', 'swifter', 'swiftly', 'haste', 'hasty', 'brisk', 'turbo', 'accelerate', 'acceleration', 'accelerated', 'accelerating']\n",
    "stp_words = ['slow', 'slower', 'slowly', 'slows', 'slowed', 'slowest', 'slowing', 'slowdown', 'long', 'looong', 'longer', 'longer', 'longest', 'steady', 'deceleration', 'decelerate', 'decelerating', 'decelerated', 'dilatory', 'dilation', 'infinity', 'eternity', 'lengthy', 'prolonged', 'protracted', 'extended', 'uneding', 'endless']\n",
    "time_words = sorted(ntp_words + ftp_words + stp_words)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ac0acdf",
   "metadata": {},
   "source": [
    "<h3> Remove specific phrases </h3>\n",
    "\n",
    "Removes phrases such as 'first time', 'point in time', or 'long story', as these phrases are describing points in time or idioms unrelated to time perception. Whether to include sentinment-based idioms for time phrases could be debated ('best time', 'wonderful time')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb0995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#non seed time words - will be removed (two steps down) shown here\n",
    "#dominate the pattern \n",
    "non_seed_time_words = [\"second\", \"seconds\", \"minute\", \"minutes\", \"hour\", \"hours\", \"day\", \"days\", \"week\", \"weeks\", \"weekend\", \"weekends\", \"month\", \"months\", \"year\", \"years\", \"times\", \"spent\"]\n",
    "\n",
    "#two word phrases\n",
    "preword_dict = {\"time\":  [\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\", \"eigth\", \"nineth\", \"tenth\", \"whole\", \"this\", \"every\", \"each\", \"that\", \"next\", \"one\", \"good\", 'bad', \"same\", \"last\", \"hard\", \"great\", \"entire\", \"some\", \"current\", \"single\", \"my\", \"winter\", \"summer\", \"spring\", \"dinner\", \"wonderful\", \"night\", 'right', 'have', 'had', \"best\", 'awful', 'worst', \"free\", \"quality\", 'another', 'popular'],\n",
    "                \"short\": [\"in\", \"stopped\", 'running'],\n",
    "                'long':  [\"not\"],\n",
    "                'rate':  [\"to\", \"I\", \"would\", \"any\"],\n",
    "                'length':[\"in\"],\n",
    "                'period':  [\"week\", \"year\"] \n",
    "                }\n",
    "\n",
    "postword_dict = {\n",
    "                \"time\" : [],\n",
    "                \"short\": [\"story\", \"sentences\", 'phrase', 'version', 'versions'],\n",
    "                'long':  [\"story\", \"pants\", \"after\", \"gone\", \"before\", 'walk', 'walks'],\n",
    "                'rate':  [],\n",
    "                'period':  [\"to\", \"I\", \"would\", \"any\"],\n",
    "                'length':  [\"in\"]\n",
    "                }\n",
    "\n",
    "#three_word_phrases combinations pre time\n",
    "one_word_pre_time = [\"in\", \"the\", \"such\", \"a\", \"only\", \"was\", 'my', 'about', 'any']\n",
    "two_words_pre_time = [\"point\", \"by\", \"around\", \"at\", \"for\", \"until\", \"the\", \"it\", \"when\", 'was', 'pass', 'all']\n",
    "\n",
    "\n",
    "\n",
    "for key in preword_dict:\n",
    "    for text in df.text:\n",
    "        for i, word in enumerate(text[:-3]):\n",
    "            if key == word and text[i-1] in preword_dict[key]:\n",
    "                text[i] = \"placeholder\"\n",
    "            elif key == word and text[i+1] in postword_dict[key]:\n",
    "                text[i] = \"placeholder\" #removed later - useful for indexing\n",
    "            elif word == \"time\" and text[i-1] in one_word_pre_time and text[i-2] in two_words_pre_time:\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"time\" and text[i-1] == \"from\" and text[i+1] == \"to\" and text[i-3] == \"time\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"time\" and text[i-3] == \"in\" and text[i-2] == \"a\" and text[i-3] == \"long\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"time\" and text[i+1] == \"to\" and text[i+2] == \"time\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"time\" and text[i-2] == \"time\" and text[i-1] == \"to\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"time\" and text[i-2] == \"one\" and text[i-1] == \"more\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"time\" and text[i-2] == \"waste\" and text[i-1] == \"of\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"long\" and text[i-1] == \"as\" and text[i+1] == \"as\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"quickly\" and text[i-1] == \"as\" and text[i+1] == \"as\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"fast\" and text[i+1] == \"and\" and text[i+2] == \"furious\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"period\" and text[i+1] == \"of\" and text[i+2] == \"years\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"period\" and text[i+1] == \"of\" and text[i+2] == \"my\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"rate\" and text[i-2] == \"at\" and text[i-1] == \"this\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"length\" and text[i+1] == \"of\" and text[i+2] == \"this\":\n",
    "                text[i] = \"placeholder\"\n",
    "            elif word == \"fast\" and text[i+1] == \"as\" and text[i+2] == \"possible\":\n",
    "                text[i] = \"placeholder\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ac0acdf",
   "metadata": {},
   "source": [
    "<h3> Lemmatization </h3>\n",
    "\n",
    "- For example: 'months' -> 'month' or 'running' -> 'run'\n",
    "\n",
    "- Don't do for time words. E.g., it's useful to keep 'time' and 'times' separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed89e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus_list = []\n",
    "for text in df.text:\n",
    "    corpus_list += text\n",
    "\n",
    "corpus_tagged_tup = nltk.pos_tag(set(corpus_list))\n",
    "corpus_tagged = {}\n",
    "for tup in corpus_tagged_tup:\n",
    "    x, y = tup \n",
    "    corpus_tagged[x] = y\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for j, text in enumerate(df.text):\n",
    "    for i, word in enumerate(text):\n",
    "        if corpus_tagged[word].startswith(('J', 'V', 'N', 'R')):\n",
    "            lemmatized_word = lemmatizer.lemmatize(word, pos = get_wordnet_pos(corpus_tagged[word]))\n",
    "            if  lemmatized_word not in time_words:\n",
    "                df.loc[j, \"text\"][i] = lemmatized_word\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fff6dd0",
   "metadata": {},
   "source": [
    "<h3> Remove various types of words </h3>\n",
    "\n",
    "The following will remove:\n",
    "- NLTK stop words (see: https://www.geeksforgeeks.org/removing-stop-words-nltk-python/)\n",
    "- Numbers 1-100 as words\n",
    "- Names of substances\n",
    "- Time\n",
    "- Words containing only 1 or 2 letters\n",
    "- Rare words (occuring less than 10 times in the corpus)\n",
    "- Unimportant common words in the Erowid identified by Ballentine (2022)\n",
    "- Unimportant common words identified by myself\n",
    "\n",
    "<h6> Ballentine, G., Friedman, S. F., & Bzdok, D. (2022). Trips and neurotransmitters: Discovering principled patterns across 6850 hallucinogenic experiences. In Sci Adv (Vol. 8, Issue 11, p. eabl6989). https://doi.org/10.1126/sciadv.abl6989 </h6>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove words with 1-2letters, nltk stopwords, words occuring less than 7 times in the entire corpus, words recommended by Ballentine (2022), and common words subjectively identified as not relevant \n",
    "\n",
    "\n",
    "#stop words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#rare words\n",
    "counter = Counter(corpus_list)\n",
    "rare_words = Counter({k: c for k, c in counter.items() if c < 10})\n",
    "rare_words = [k for k in rare_words.keys()]\n",
    "\n",
    "#numbers as words\n",
    "numbers_as_words = [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\", \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\", \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"twenty-one\", \"twenty-two\", \"twenty-three\", \"twenty-four\", \"twenty-five\", \"twenty-six\", \"twenty-seven\", \"twenty-eight\", \"twenty-nine\", \"thirty\", \"thirty-one\", \"thirty-two\", \"thirty-three\", \"thirty-four\", \"thirty-five\", \"thirty-six\", \"thirty-seven\", \"thirty-eight\", \"thirty-nine\", \"forty\", \"forty-one\", \"forty-two\", \"forty-three\", \"forty-four\", \"forty-five\", \"forty-six\", \"forty-seven\", \"forty-eight\", \"forty-nine\", \"fourty\", \"fourty-one\", \"forty-two\", \"fourty-three\", \"fourty-four\", \"fourty-five\", \"fourty-six\", \"fourty-seven\", \"fourty-eight\", \"fourty-nine\", \"fifty\", \"fifty-one\", \"fifty-two\", \"fifty-three\", \"fifty-four\", \"fifty-five\", \"fifty-six\", \"fifty-seven\", \"fifty-eight\", \"fifty-nine\", \"sixty\", \"sixty-one\", \"sixty-two\", \"sixty-three\", \"sixty-four\", \"sixty-five\", \"sixty-six\", \"sixty-seven\", \"sixty-eight\", \"sixty-nine\", \"seventy\", \"seventy-one\", \"seventy-two\", \"seventy-three\", \"seventy-four\", \"seventy-five\", \"seventy-six\", \"seventy-seven\", \"seventy-eight\", \"seventy-nine\", \"eighty\", \"eighty-one\", \"eighty-two\", \"eighty-three\", \"eighty-four\", \"eighty-five\", \"eighty-six\", \"eighty-seven\", \"eighty-eight\", \"eighty-nine\", \"ninety\", \"ninety-one\", \"ninety-two\", \"ninety-three\", \"ninety-four\", \"ninety-five\", \"ninety-six\", \"ninety-seven\", \"ninety-eight\", \"ninety-nine\", \"one hundred\", \"hundred\"]\n",
    "\n",
    "#substances and classes\n",
    "substances = [ \"lsd\",\"psilocybin mushrooms\",\"morning glory\",\"2c-i\",\"5-meo-dmt\",\"dmt\",\"25i-nbome\",\"argyreia nervosa\",\"2c-e\",\"5-meo-dipt\",\"amt\",\"2c-t-7\",\"2c-b\",\"echinopsis pachanoi\",\"dpt\",\"5-meo-dalt\",\"5-meo-mipt\",\"2c-c\",\"4-aco-dmt\",\"psychotria viridis\",\"dom\",\"tma-2\",\"doc\",\"2c-p\",\"2c-t-21\",\"4-ho-mipt\",\"2c-d\",\"4-ho-dipt\",\"1p-lsd\",\"dob-dragonfly\",\"4-acetoxy-dipt\",\"dob\",\"4-aco-det\",\"doi\",\"iboga\",\"ibogaine\",\"al-lad\",\"anadenanthera peregrina\",\"2c-t-2\",\"ayahuasca\",\"5-meo-amt\",\"mimosahuasca\",\"echinopsis peruviana\",\"anadenanthera colubrina\",\"lophophora williamsii\",\"4-acetoxy-mipt\",\"mescaline\",\"dipt\",\"2c-t-4\",\"dxm\",\"amanita muscaria\",\"methoxetamine\",\"3-meo-pcp\",\"ketamine\",\"nitrous oxide\",\"pcp\",\"mdma\",\"mda\",\"6-apb\",\"butylone\",\"ethylone\",\"mephedrone\",\"methylone\",\"mdai\",\"mbdb\",\"iap\",\"datura spp.\",\"brugmansia spp.\",\"scopolamine\",\"dimenhydrinate\",\"atropa belladona\",\"cannabis spp.\",\"diphenhydramine\",\"heroine\",\"zolpidem\",\"piper methysticum\",\"jwh-018\",\"hydrocodone\",\"fentanyl\",\"alprazolam\",\"melatonin\",\"alcohol (hard)\",\"sceletium tortuosum\",\"leonotis leonurus\",\"turnera diffusa\",\"morphine\",\"1,4-butaneidol\",\"cyclobenzaprine\",\"clonazepam\",\"opium\",\"piracetam\",\"lorazepam\",\"passiflora spp.\",\"alcohol (beer-wine)\",\"triazolam\",\"mitragyna speciosa\",\"tramadol\",\"synthetic cannabis\",\"oxycodone\",\"gabapentin\",\"papaver somniferum\",\"methadone\",\"alcohol\",\"codeine\",\"cannabinoid receptor agonists\",\"buprenorphine\",\"ether\",\"valeriana officinalis\",\"nymphaea caerulea\",\"gbl\",\"hash\",\"lactuca virosa\",\"hydromorphone\",\"carisoprodol\",\"zopiclone\",\"diazepam\",\"etizolam\",\"barbiturates\",\"meperidine\",\"cocaine\",\"myristica spp.\",\"caffeine\",\"mdpv\",\"methylphenidate\",\"ethylphenidate\",\"ilex paraguariensis\",\"ephedrine\",\"dmae\",\"coffea spp.\",\"betel nut\",\"2-aminoindan\",\"amphetamine\",\"metamphetamine\",\"modafinil\",\"nicotiana tabacum\",\"crack\",\"4-fluoroamphetamine\",\"adrafinil\",\"substituted piperazines\",\"tfmpp\",\"benzylpiperazine\",\"atomoxetine\",\"propylhexedrine\",\"calea zacatechichi\",\"silene undulata\",\"paroxetine\",\"bupropion\",\"trazadone\",\"sertraline\",\"olanzapine\",\"venlafaxine\",\"quetiapine\",\"mirtazapine\",\"amitriptyline\",\"hypericum perforatum\",\"salvia divinorum\",\"yohimbe\",\"acorus calamus\",\"nepeta cataria\",\"heimia salicifolia\",\"5-htp\",\n",
    "               'ritalin', 'changa', 'cigarette', 'speed', 'dextroamphetamine', 'klonopin', 'mdxx', 'opiate', \"psychedelics\", 'coke', 'yadda', 'vapor', 'shang', 'shrooms', 'lsd', 'psilocybin', 'mushroom', 'mushrooms', 'mushroom - p. cubensis', 'magic mushrooms', 'magic mushrooms (sclerotia)', \"'mushrooms'\", 'mushrooms  - p. cubensis', 'mushrooms - c. cyanescens', 'mushrooms - p cubensis', 'mushrooms - p. arcana', 'mushrooms - p. atlantis (sclerotia)', 'mushrooms - p. azurescens', 'mushrooms - p. azurescens?', 'mushrooms - p. baeocystis', 'mushrooms - p. cubenesis', 'mushrooms - p. cubenesis (amazonian strain)', 'mushrooms - p. cubensis', 'mushrooms - p. cubensis (albino penis envy)', 'mushrooms - p. cubensis (amazon strain)', 'mushrooms - p. cubensis (cambodian)', \"mushrooms - p. cubensis ('cambodian')\", 'mushrooms - p. cubensis (dried)', 'mushrooms - p. cubensis (ecuadorean)', 'mushrooms - p. cubensis (extract)', 'mushrooms - p. cubensis (fresh)', 'mushrooms - p. cubensis (golden teacher)', 'mushrooms - p. cubensis (golden teachers)', 'mushrooms - p. cubensis (in chocolate)', 'mushrooms - p. cubensis (mexican)', 'mushrooms - p. cubensis (mycelium)', 'mushrooms - p. cubensis (penis envy)', 'mushrooms - p. cubensis (pes amazonia)', 'mushrooms - p. cubensis (sclerotia)', 'mushrooms - p. cubensis (smoked)', 'mushrooms - p. cubensis (tasmanian strain)', 'mushrooms - p. cyanescens', 'mushrooms - p. cyanescens (smoked)', 'mushrooms - p. galindoi', 'mushrooms - p. mexicana', 'mushrooms - p. mexicana (fresh)', 'mushrooms - p. mexicana (sclerotia)', 'mushrooms - p. mexicana (truffles)', 'mushrooms - p. ovoideocystidiata', 'mushrooms - p. semilanceata', 'mushrooms - p. subaeruginosa', 'mushrooms - p. subbalteatus or p. papilionaceus', 'mushrooms - p. tampanensis', 'mushrooms - p. tampanensis (sclerotia)', 'mushrooms - p. tampanensis (truffles)', 'mushrooms - p. weilii', 'mushrooms - p. zapotecorum', 'mushrooms - panaeolus cyanescens', 'mushrooms - panaeolus subbalteatus', 'mushrooms - psilocybe cyanescens', 'mushrooms (blue vein)', 'mushrooms (dried)', 'mushrooms (edible)', 'mushrooms (extract)', 'mushrooms (fresh)', \"mushrooms ('gold caps')\", 'mushrooms (golden caps)', 'mushrooms (hawaiian)', 'mushrooms (in chocolate)', 'mushrooms (magic mushrooms)', 'mushrooms (magic)', 'mushrooms (mexican)', 'mushrooms (p. cubensis - mexican strain)', 'mushrooms (p. cubensis smoked)', 'mushrooms (p. cubensis)', 'mushrooms (p. cyanescens)', 'mushrooms (p. pelliculosa)', 'mushrooms (sclerotia)', 'mushrooms (smoked)', 'mushrooms (thai)', 'mushrooms- p. cubensis', 'mushroooms', 'cannabis', 'morning glorys', ' morning glory', 'mdma (ecstasy)', 'methamphetamine', 'ecstasy', 'meth', 'kratom', 'nutmeg', 'datura', 'amphetamines', 'cacti - t. pachanoi', 'kava', 'kava kava', 'absinthe', 'alcohol - hard', 'alcohol (rum)', 'alcohol (whiskey)', 'alcohol (mead)', 'alcohol (vodka)', 'alcohol - (liquor)', 'alcohol - (wine)', ' alcohol - hard', 'absinthe  (homemade)', 'absinthe (czech)', 'absinthe (homemade)', 'beer', 'wine', 'alcohol - beer', 'alcohol - wine', 'alcohol (mead)', 'alcohol - (wine)', 'alcohol - (wine)', 'alcohol - (wine)', '4-methylmethcathinone', '4-methylmethcathinone (mephedrone)', 'ambien', 'zolpidem (ambien)', ' zolpidem (ambien)', ' h.b. woodrose', 'h.b. woodrose seeds', 'woodrose', 'h.b. woodrose', 'h. b. woodrose', 'hawaiian baby woodrose seeds', 'hawaiian baby woodrose seeds', 'hawaiian baby woodrose seeds', 'h.b. woodrose (hbw)', 'catnip', 'cacti - t. peruvianus', '4-methylmethcathinone', 'brugmansia', 'brugmansia (tree datura)', 'brugmansia sanguinea', 'brugmansia suaveolens', 'damiana', 'tobacco', 'tobacco - cigarettes', 'tramadol (ultram)', 'blue lotus', 'dimenhydrinate (dramamine)', 'dramamine (dimenhydrinate)', 'dramamine (dimenhydrinate)', 'poppies - opium', 'poppies', 'poppies', 'oxycodone (oxycontin)', 'oxycontin', 'roxicodone', 'peyote', 'cannabis - hash', 'calamus', 'yerba mate', \"st. john's wort\", \"st. john's wort\", 'bromo-dragonfly', 'coffee', 'caffeine (coffee)', 'caffeine (coffee)', 'caffeine (coffee)', 'caffeine (coffee)', 'caffeine (coffee)', 'quetiapine (seroquel)', 'seroquel', 'gabapentin (neurontin)', 'neurontin', 'modafinil (provigil)', 'provigil', ' venlafaxine', 'venlafaxine (effexor)', 'effexor', 'paroxetine (paxil)', 'paxil', 'bupropion (wellbutrin)', 'wellbutrin', '4-Acetoxy-DET', 'diphenhydramine (benadryl)', 'benadryl', 'triazolam (halcion)', 'halcion', 'coca', 'valerian', 'piperazines - bzp', 'piperazines', 'piperazines - mcpp', 'piperazines - mcpp', 'piperazines - mcpp', 'silene capensis', 'passion flower', 'mimosa tenuiflora', 'dmt (extracted from m. tenuiflora)', 'dmt (extracted from m. tenuiflora)', 'belladonna', '1-4-butanediol', '1,4 butanediol', 'mipt', 'det', \"salvia\"\t,\"divinorum\", \"glory\"\n",
    "               \"psychedelic\", \"entactogen\", \"entactogens\", \"deliriant\", \"depressant\", \"sedative\", \"stimulant\", \"oneirogens\", \"antidepressant\", \"antipsychotics\"\n",
    "\n",
    "             ]\n",
    "\n",
    "#words from Ballentine (2022)\n",
    "ballentine_common_words = ['the', 'this', 'that', 'not', 'and', 'have', 'there', 'all', 'then', 'what', 'but', 'would', 'for', 'with', 'will', 'was', 'thing',\n",
    "                'get', 'could', 'from', 'more', 'etc', 'who', 'out', 'another', 'like', 'too', 'while', 'about', 'more', 'less', 'way', 'on',\n",
    "                'she', 'her', 'him', 'his', 'our', \"i'm\", 'i’m', 'are', 'can’t', \"i'd\", 'i’d',  'ich', 'der', 'das',\n",
    "                \"didn't\", \"don't\", \"dont\", \"i've\", \"it's\", \"wasn’t\", \"can't\", \"wouldn't\", \"couldn't\", \"couldn´t\", \"won't\", \"i'll\",\n",
    "                'them', 'were', 'they', 'through', 'back', 'being', 'only', 'also',\n",
    "                'went', 'some', 'again', 'into', 'after', 'around', 'down', 'just', 'very', 'things', 'when', 'over', 'other', 'before',\n",
    "                'because', 'which', 'took', 'than', 'before', 'still', 'didn’t',\n",
    "                'it’s', 'i’ve', 'didnt', 'didn´t', 'couldnt', 'couldn’t', 'their',\n",
    "                'don’t', \"that's\", 'won’t', 'und', 'che', 'que',\n",
    "                'μg/kg', 'mgs', \"mg's\", 'hcl', 'indole',\n",
    "                'pill', 'pills', 'pipe', 'smoke', 'smokes', 'smoked', 'smoking', 'blotter', 'tab', 'tabs', 'line', 'lines', 'dose', 'doses', 'dosage', 'hit', 'hits', 'bowl',\n",
    "                'trip', 'trips', 'tripping', 'tripped', 'trippy', 'k.hole', 'k-hole', 'khole',\n",
    "                'roll', 'rolls', 'rolling', 'rolled',\n",
    "                'das', '1999', '1/2', 'ten', 'substance', 'load', 'cherek', '5:00', '2001', '300', 'you', 'josh',\n",
    "            #    'you', 'seconds', 'months', 'days', 'weeks', 'years', 'second', 'month', 'day', 'week', 'year', 'hour', 'hours',\n",
    "\n",
    "                'powder', 'crystals', 'vaporized', 'vaporize',  'roll', 'rolling', 'rolled', 'nasal', 'bong', 'foil', 'root', 'bark', 'cannabis', 'toke', 'heroin'\n",
    "                'inject', 'injection', 'trip', 'pill', 'pills', 'injecting', 'insufflation', 'trips', 'tripping', 'tripped', 'trippy', 'pipe', 'rectal',\n",
    "                'snort', 'smoked', 'snorting', 'snorted', 'insufflated', 'injected', 'blotter', 'tab', 'oral', 'orally', 'weed', 'exstasy',\n",
    "                # 'body', 'experience', 'time', 'felt', 'feel', 'life', 'been', 'feeling', 'first', 'really', 'load', 'compound', 'effects',\n",
    "                'hole', 'bump', 'bumps', 'drunk', 'clubbing', 'boyfriend', 'husband', 'wife',\n",
    "                'syringe', 'needle', 'hospital',\n",
    "                'vial', 'bag',\n",
    "                'inject', 'drugs',\n",
    "                'vials', 'caps', 'bottle', 'robo', 'robitussin', 'syrup', 'vicks', 'coricidin', 'cough', 'freebase', 'compound', 'bottles', 'brand', 'tussin', 'cpm', 'maleate',\n",
    "                'chlorpheniramine', 'delsym', 'robotussin', 'joe', 'dex', 'dxm', 'prozac', '8oz', 'joint', 'pot',\n",
    "\n",
    "                'rave', 'raves', 'club', 'night', 'party', 'friend', 'car', '2000', 'boyfriends', 'girlfriend', 'girlfriends', 'rollin',\n",
    "\n",
    "                'die', 'nicht', 'mit', 'mir', 'darla', 'sich', 'mich', 'ist', 'ein', 'war', 'den',\n",
    "                'noch', 'een', 'auch', 'dass', 'hatte', 'auf', 'von', 'meine', 'als', 'eine',\n",
    "                'einen', 'alal', 'sie', 'het', 'dem', 'aus', 'mark', 'aber', 'nach', 'marijuana',\n",
    "                'des', 'approx', 'wavy', 'john', 'burnt', 'wie', 'chris'\n",
    "                \n",
    "                \n",
    "               ]\n",
    "#my common words\n",
    "my_common_words = [#adverbs\n",
    "                   'now', 'well', 'first', \"second\", \"third\", \"whole\", \"this\", \"every\", \"next\", \"one\", \"same\", \"however\", \"even\", \"something\", \"got\", \"probably\", \"without\", \"thereafter\", \"within\", \"aprox\", \"actually\", \"certain\", \"somehow\", \"rather\", \"least\", \"whatever\", \"whatsoever\", 'wenn', 'whoever', 'whose', 'whenever', 'whichever', 'really', 'never', 'bit', 'ever', 'told', 'later', 'almost', 'away', 'quite', 'pretty', 'completely', 'always', 'finally', 'extremely', 'yet', 'maybe', 'soon', 'far', 'side', 'else', 'definitely', 'close', 'slightly', 'eventually', 'usually', 'already', 'together', 'ago', 'along', 'alone', 'sometimes', 'immediately', 'simply', 'somewhat', 'totally', 'instead', 'perhaps', 'exactly', 'anyway', 'often', 'especially', 'normally', 'absolutely', 'mostly', 'truly', 'anymore', 'nearly', 'fully', 'fairly', 'incredibly', 'easily', 'barely', \n",
    "                   'recently', 'somewhere', 'basically', 'literally', 'constantly', 'certainly', 'forward', 'twice', 'store', 'clearly', \n",
    "                   'possibly', 'particularly', 'everywhere', 'generally', 'apparently', 'entirely', 'corner', 'instantly', 'approximately', 'alot', 'previously', 'perfectly', 'apart', 'obviously', 'relatively', 'occasionally', 'honestly', 'highly', 'roughly', 'directly', 'becomes', 'gradually', 'shortly', 'seriously', 'rapidly', 'indeed', 'hangover', 'ahead', 'otherwise', 'hardly', 'thus', 'aside', 'strongly', 'luckily', 'anywhere', 'nowhere', 'properly', 'smooth', 'intensely', 'greatly', 'chose', 'heavily', 'mainly', 'potentially', 'seemingly', 'loop', 'naturally', 'currently', 'whatsoever', 'therefore', 'merely', 'mildly', 'hopefully', 'initially', 'regularly', 'sometime', 'uncontrollably', 'frequently', 'surprisingly', 'carefully', 'silly', 'strangely', 'significantly', 'thoroughly', 'hallway', 'simultaneously', 'necessarily', 'increasingly', \n",
    "                   'rarely', 'definately', 'ignore', 'sore', 'lovely', 'lightly', 'violently', 'weather', 'thankfully', 'repeatedly', 'halfway', 'surely', 'specifically', 'differently', 'vaguely', 'amazingly', 'tingly', 'ether', 'desperately', 'settle', 'typically', 'fortunately', 'ultimately', 'essentially', 'practically', 'heavenly', 'distinctly', 'oddly', 'lately', 'suicide', 'quietly', 'terribly', 'strongest', 'nicely', 'partially', 'originally', 'infinitely', 'safely', 'nonetheless', 'overly', 'afterward', 'randomly', 'ugly', 'farther', 'driver', 'altogether', 'purely', 'equally', 'immensely', 'horribly', 'recreationally',  'subside', 'severely', 'supposedly', 'closely', 'noticeably', 'pleasantly', 'vividly', 'lonely', 'correctly', 'consciously', 'trippin', 'promptly', 'comfortably', 'virtually', 'considerably', 'loudly', 'partly', \n",
    "                   'unusually', 'largely', 'accurately', 'anytime', 'plate', 'profoundly', 'accidentally', 'dramatically', 'profusely', 'meanwhile', 'favourite', 'primarily', 'grind', 'importantly', 'frantically', 'successfully', 'wildly', 'freely', 'insanely', 'moderately', 'interestingly', 'commonly', 'readily', 'smoothly', 'downward', 'unbelievably', 'upside', 'whoever', 'tightly', 'effectively', 'softly', 'overwhelmingly', 'ridiculously', 'silently', 'november', 'drastically', 'reasonably', 'genuinely', 'peanut', 'consistently', 'elsewhere', 'brightly', 'precisely', 'firmly', 'tore', 'ally', 'subtly','remotely', 'sang', 'remarkably', 'similarly', 'upward', 'weekly', 'completly', 'finely', 'smack', 'intently', 'jelly', 'newly', 'belong', 'tremendously', 'responsibly', 'temple', 'exceptionally', 'automatically', 'solely', \n",
    "                   'uncertain', 'wonderfully', 'poorly', 'actively', 'aimlessly', 'presumably', 'avid', 'extensively', 'truely', 'furthermore', 'anything', 'nothing', 'half', 'part', 'sure', 'done', 'enough', 'sort', 'including', \"couple\", \"most\"\n",
    "                    #modal verbs\n",
    "                    'might', 'may', 'must', 'shall', 'ought',\n",
    "                    #cordinating conjunction\n",
    "                    'either', 'neither', 'plus', 'minus',\n",
    "                    #Preposition or subordinating conjunction\n",
    "                    'though', 'since', 'outside', 'love', 'inside', 'although', 'begin', 'behind', 'across', 'except', 'upon', 'throughout', 'onto', 'beyond', 'near', 'whether', 'despite', 'background', 'laugh', 'unlike', 'per', 'awhile', 'super', 'toward',\n",
    "                    #Adjective, comparative\n",
    "                    'higher', 'lower', 'smaller', 'larger', 'bigger', 'greater', 'lot'\n",
    "                    #Neutral adjectives\n",
    "                    'little', 'last', 'many', 'large', 'small', 'able', 'light', 'hard', 'right', 'left', 'several', 'great', 'lot', \"high\", \n",
    "                    #Neutral verbs\n",
    "                    \"let\", \"decide\", 'begin', 'seem', 'seems', 'come', 'want', 'know', 'become', 'sit', 'keep', 'happen', 'turn', 'close', 'give', 'call', 'end', 'go', 'get', 'take', 'try', 'come', 'talk', 'sit', 'walk', 'move', 'use', 'start', 'say', 'star', 'follow', 'following', 'stand', 'know', 'become', 'hold', 'leave', 'hear', 'bring', 'understand', 'giving', 'turn', 'know', 'make', 'find', 'put', 'tell', 'notice', 'do', 'show', \"let\", \"stay\"   \n",
    "                    #Neutral nouns\n",
    "                    \"amount\", \"point\" \n",
    "                    #Names\n",
    "                    \"chad\", \"dave\", \"nick\", 'jacob', \"chalya\", \"gunner\", 'samantha'\n",
    "                    \"placeholder\"\n",
    "                    ]\n",
    "\n",
    "\n",
    "remove_words_set = set(rare_words + stop_words + ballentine_common_words +  my_common_words + substances + numbers_as_words + non_seed_time_words)\n",
    "\n",
    "\n",
    "def remove_words(text):\n",
    "    text = [w for w in text if w not in remove_words_set] \n",
    "    #remove 1-2 letter words\n",
    "    text = [' '.join([w for w in i.split(' ') if len(w) >= 3]) for i in text]\n",
    "    while(\"\" in text):\n",
    "        text.remove(\"\")\n",
    "    return text\n",
    "    \n",
    "df.text = df.text.loc[:].apply(remove_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d91ea",
   "metadata": {},
   "source": [
    "Get length of corpus after removal of types of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff471e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=0\n",
    "for i in range(len(df)):\n",
    "    x = len(df.text[i])\n",
    "    y += x\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0acdf",
   "metadata": {},
   "source": [
    "<h3> Save data </h3>\n",
    "\n",
    "The dataframe structure will look very similar to at the end of pre-processing 1. The only difference is the content of the 'text' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988076b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"clean_processed_data.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4ac408516564915e59f6571e8840a617524b2c5af7c094526d6726d37b65d83a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
